{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for adding and updating DBNascent values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from . import dbutils (in script)\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = \"Aho2019displacement\"\n",
    "#paper_id = \"Aeby2020decapping\"\n",
    "#paper_id = \"Danko2018dynamic\"\n",
    "#paper_id = \"Estaras2015smad\"\n",
    "#paper_id = \"Duttke2015human\"\n",
    "#paper_id = \"Zhu2018rna\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load dborm.py\n",
    "# orm.py --- ORM for DBNascent\n",
    "#\n",
    "# Filename: orm.py\n",
    "# Description: ORM for DBNascent\n",
    "# Authors: Lynn Sanford <lynn.sanford@colorado.edu> and Zach Maas\n",
    "# Created: Mon Jun 10 13:11:55 2019 (-0600)\n",
    "# URL:\n",
    "#\n",
    "\n",
    "# Commentary:\n",
    "#\n",
    "# This file contains code for an ORM to interface with the Dowell\n",
    "# Lab's Nascent Database.\n",
    "#\n",
    "\n",
    "# Code:\n",
    "\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "# Base class for our ORM\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "# MAIN TABLES\n",
    "class organismInfo(Base):\n",
    "    __tablename__ = \"organismInfo\"\n",
    "#    metadata = MetaData()\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    genome_build = sql.Column(sql.String(length=50))\n",
    "    genome_bases = sql.Column(sql.BigInteger)\n",
    "\n",
    "\n",
    "class searchEq(Base):\n",
    "    __tablename__ = \"searchEq\"\n",
    "#    metadata = MetaData()\n",
    "    search_term = sql.Column(\n",
    "        sql.String(length=250), primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    db_term = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "class exptMetadata(Base):\n",
    "    __tablename__ = \"exptMetadata\"\n",
    "#    metadata = MetaData()\n",
    "    expt_id = sql.Column(sql.Integer,\n",
    "                         primary_key=True,\n",
    "                         index=True,\n",
    "                         unique=True)\n",
    "    srp = sql.Column(sql.String(length=50))\n",
    "    protocol = sql.Column(sql.String(length=50))\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), sql.ForeignKey(\"organismInfo.organism\")\n",
    "    )\n",
    "    library = sql.Column(sql.String(length=50))\n",
    "    spikein = sql.Column(sql.String(length=127))\n",
    "    paper_id = sql.Column(sql.String(length=127),\n",
    "                          index=True)\n",
    "    published = sql.Column(sql.Boolean)\n",
    "    year = sql.Column(sql.Integer)\n",
    "    first_author = sql.Column(sql.String(length=127))\n",
    "    last_author = sql.Column(sql.String(length=127))\n",
    "    doi = sql.Column(sql.String(length=300))\n",
    "    curator1 = sql.Column(sql.String(length=50))\n",
    "    curator2 = sql.Column(sql.String(length=50))\n",
    "    other_sr_data = sql.Column(sql.Boolean)\n",
    "    atac_seq = sql.Column(sql.Boolean)\n",
    "    rna_seq = sql.Column(sql.Boolean)\n",
    "    chip_seq = sql.Column(sql.Boolean)\n",
    "    three_dim_seq = sql.Column(sql.Boolean)\n",
    "    other_seq = sql.Column(sql.Boolean)\n",
    "    paper_qc_score = sql.Column(sql.Float)\n",
    "    paper_data_score = sql.Column(sql.Float)\n",
    "\n",
    "\n",
    "class sampleID(Base):\n",
    "    __tablename__ = \"sampleID\"\n",
    "#    metadata = MetaData()\n",
    "    srr = sql.Column(sql.String(length=50),\n",
    "                     primary_key=True,\n",
    "                     index=True,\n",
    "                     unique=True)\n",
    "    sample_name = sql.Column(sql.String(length=50),\n",
    "                             index=True)\n",
    "    sample_id = sql.Column(sql.Integer,\n",
    "                           index=True)\n",
    "\n",
    "\n",
    "class geneticInfo(Base):\n",
    "    __tablename__ = \"geneticInfo\"\n",
    "#    metadata = MetaData()\n",
    "    genetic_id = sql.Column(sql.Integer,\n",
    "                            primary_key=True,\n",
    "                            index=True,\n",
    "                            unique=True)\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), sql.ForeignKey(\"organismInfo.organism\")\n",
    "    )\n",
    "    sample_type = sql.Column(sql.String(length=127))\n",
    "    cell_type = sql.Column(sql.String(length=127))\n",
    "    clone_individual = sql.Column(sql.String(length=127))\n",
    "    strain = sql.Column(sql.String(length=127))\n",
    "    genotype = sql.Column(sql.String(length=127))\n",
    "    construct = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "class conditionInfo(Base):\n",
    "    __tablename__ = \"conditionInfo\"\n",
    "#    metadata = MetaData()\n",
    "    condition_id = sql.Column(sql.Integer,\n",
    "                              primary_key=True,\n",
    "                              index=True,\n",
    "                              unique=True)\n",
    "    condition_type = sql.Column(sql.String(length=127))\n",
    "    treatment = sql.Column(sql.String(length=127))\n",
    "    conc_intens = sql.Column(sql.String(length=50))\n",
    "    start_time = sql.Column(sql.Integer)\n",
    "    end_time = sql.Column(sql.Integer)\n",
    "    time_unit = sql.Column(sql.String(length=50))\n",
    "    duration = sql.Column(sql.Integer)\n",
    "    duration_unit = sql.Column(sql.String(length=50))\n",
    "\n",
    "\n",
    "exptCondition = sql.Table(\n",
    "    \"exptCondition\",\n",
    "    Base.metadata,\n",
    "    sql.Column(\"sample_id\",\n",
    "               sql.Integer,\n",
    "               sql.ForeignKey(\"sampleID.sample_id\")),\n",
    "    sql.Column(\"condition_id\",\n",
    "               sql.Integer,\n",
    "               sql.ForeignKey(\"conditionInfo.condition_id\")),\n",
    ")\n",
    "\n",
    "\n",
    "class linkIDs(Base):\n",
    "    __tablename__ = \"linkIDs\"\n",
    "#    metadata = MetaData()\n",
    "    sample_id = sql.Column(\n",
    "        sql.Integer,\n",
    "        sql.ForeignKey(\"sampleID.sample_id\"),\n",
    "        primary_key=True,\n",
    "        index=True,\n",
    "        unique=True,\n",
    "    )\n",
    "    genetic_id = sql.Column(sql.Integer,\n",
    "                            sql.ForeignKey(\"geneticInfo.genetic_id\"))\n",
    "    expt_id = sql.Column(sql.Integer,\n",
    "                         sql.ForeignKey(\"exptMetadata.expt_id\"))\n",
    "    sample_name = sql.Column(sql.String(length=127),\n",
    "                         sql.ForeignKey(\"sampleID.sample_name\"))\n",
    "    paper_id = sql.Column(sql.String(length=127),\n",
    "                         sql.ForeignKey(\"exptMetadata.paper_id\"))\n",
    "\n",
    "\n",
    "class sampleAccum(Base):\n",
    "    __tablename__ = \"sampleAccum\"\n",
    "#    metadata = MetaData()\n",
    "    sample_id = sql.Column(\n",
    "        sql.Integer,\n",
    "        sql.ForeignKey(\"sampleID.sample_id\"),\n",
    "        primary_key=True,\n",
    "        index=True,\n",
    "        unique=True,\n",
    "    )\n",
    "    replicate = sql.Column(sql.String(length=50))\n",
    "    single_paired = sql.Column(sql.String(length=50))\n",
    "    rcomp = sql.Column(sql.Boolean)\n",
    "    expt_unusable = sql.Column(sql.Boolean)\n",
    "    timecourse = sql.Column(sql.Boolean)\n",
    "    baseline_control_expt = sql.Column(sql.String(length=50))\n",
    "    notes = sql.Column(sql.String(length=300))\n",
    "    raw_read_depth = sql.Column(sql.Integer)\n",
    "    trim_read_depth = sql.Column(sql.Integer)\n",
    "    raw_read_length = sql.Column(sql.Integer)\n",
    "    duplication_picard = sql.Column(sql.Float)\n",
    "    single_map = sql.Column(sql.Integer)\n",
    "    multi_map = sql.Column(sql.Integer)\n",
    "    map_prop = sql.Column(sql.Float)\n",
    "    rseqc_tags = sql.Column(sql.Integer)\n",
    "    rseqc_cds = sql.Column(sql.Integer)\n",
    "    rseqc_five_utr = sql.Column(sql.Integer)\n",
    "    rseqc_three_utr = sql.Column(sql.Integer)\n",
    "    rseqc_intron = sql.Column(sql.Integer)\n",
    "    cds_rpk = sql.Column(sql.Float)\n",
    "    intron_rpk = sql.Column(sql.Float)\n",
    "    exint_ratio = sql.Column(sql.Float)\n",
    "    distinct_tenmillion_prop = sql.Column(sql.Float)\n",
    "    genome_prop_cov = sql.Column(sql.Float)\n",
    "    avg_fold_cov = sql.Column(sql.Float)\n",
    "    samp_qc_score = sql.Column(sql.Integer)\n",
    "    samp_data_score = sql.Column(sql.Integer)\n",
    "\n",
    "\n",
    "class nascentflowMetadata(Base):\n",
    "    __tablename__ = \"nascentflowMetadata\"\n",
    "#    metadata = MetaData()\n",
    "    nascentflow_id = sql.Column(\n",
    "        sql.Integer, primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    nascentflow_version = sql.Column(sql.String(length=127))\n",
    "    pipeline_revision_hash = sql.Column(sql.String(length=127))\n",
    "    pipeline_hash = sql.Column(sql.String(length=127))\n",
    "    nascentflow_date = sql.Column(sql.Date)\n",
    "    nascentflow_redo_date = sql.Column(sql.Date)\n",
    "    nextflow_version = sql.Column(sql.String(length=127))\n",
    "    fastqc_version = sql.Column(sql.String(length=127))\n",
    "    bbmap_version = sql.Column(sql.String(length=127))\n",
    "    hisat2_version = sql.Column(sql.String(length=127))\n",
    "    samtools_version = sql.Column(sql.String(length=127))\n",
    "    sratools_version = sql.Column(sql.String(length=127))\n",
    "    preseq_version = sql.Column(sql.String(length=127))\n",
    "    preseq_date = sql.Column(sql.Date)\n",
    "    rseqc_version = sql.Column(sql.String(length=127))\n",
    "    rseqc_date = sql.Column(sql.Date)\n",
    "    java_version = sql.Column(sql.String(length=127))\n",
    "    picard_gc_version = sql.Column(sql.String(length=127))\n",
    "    picard_dups_version = sql.Column(sql.String(length=127))\n",
    "    picard_date = sql.Column(sql.Date)\n",
    "    bedtools_version = sql.Column(sql.String(length=127))\n",
    "    igvtools_version = sql.Column(sql.String(length=127))\n",
    "    seqkit_version = sql.Column(sql.String(length=127))\n",
    "    mpich_version = sql.Column(sql.String(length=127))\n",
    "    gcc_version = sql.Column(sql.String(length=127))\n",
    "    python_version = sql.Column(sql.String(length=127))\n",
    "    numpy_version = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "exptNascentflow = sql.Table(\n",
    "    \"exptNascentflow\",\n",
    "    Base.metadata,\n",
    "    sql.Column(\"sample_id\",\n",
    "               sql.Integer,\n",
    "               sql.ForeignKey(\"sampleID.sample_id\")),\n",
    "    sql.Column(\"nascentflow_id\",\n",
    "               sql.Integer,\n",
    "               sql.ForeignKey(\"nascentflowMetadata.nascentflow_id\")),\n",
    ")\n",
    "\n",
    "\n",
    "class bidirflowMetadata(Base):\n",
    "    __tablename__ = \"bidirflowMetadata\"\n",
    "#    metadata = MetaData()\n",
    "    bidirflow_id = sql.Column(\n",
    "        sql.Integer, primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    bidirflow_version = sql.Column(sql.String(length=127))\n",
    "    pipeline_revision_hash = sql.Column(sql.String(length=127))\n",
    "    pipeline_hash = sql.Column(sql.String(length=127))\n",
    "    bidirflow_date = sql.Column(sql.Date)\n",
    "    nextflow_version = sql.Column(sql.String(length=127))\n",
    "    samtools_version = sql.Column(sql.String(length=127))\n",
    "    bedtools_version = sql.Column(sql.String(length=127))\n",
    "    mpich_version = sql.Column(sql.String(length=127))\n",
    "    openmpi_version = sql.Column(sql.String(length=127))\n",
    "    gcc_version = sql.Column(sql.String(length=127))\n",
    "    r_version = sql.Column(sql.String(length=127))\n",
    "    rsubread_version = sql.Column(sql.String(length=127))\n",
    "    boost_version = sql.Column(sql.String(length=127))\n",
    "    fstitch_version = sql.Column(sql.String(length=127))\n",
    "    tfit_version = sql.Column(sql.String(length=127))\n",
    "    dreg_version = sql.Column(sql.String(length=127))\n",
    "    dreg_date = sql.Column(sql.Date)\n",
    "    tfit_date = sql.Column(sql.Date)\n",
    "    fcgene_date = sql.Column(sql.Date)\n",
    "\n",
    "\n",
    "exptBidirflow = sql.Table(\n",
    "    \"exptBidirflow\",\n",
    "    Base.metadata,\n",
    "    sql.Column(\"sample_id\",\n",
    "               sql.Integer,\n",
    "               sql.ForeignKey(\"sampleID.sample_id\")),\n",
    "    sql.Column(\"bidirflow_id\",\n",
    "               sql.Integer,\n",
    "               sql.ForeignKey(\"bidirflowMetadata.bidirflow_id\")),\n",
    ")\n",
    "\n",
    "# The following were created by Zach and we may or may not use...\n",
    "\n",
    "# class tf(Base):\n",
    "#    __tablename__ = \"tf\"\n",
    "#    tf_id = sql.Column(sql.String(length=127), primary_key=True)\n",
    "#    tf_alias = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "# class pipeline_status(Base):\n",
    "#    __tablename__ = \"pipeline_status\"\n",
    "#    srr_id = sql.Column(\n",
    "#        sql.String(length=127),\n",
    "#        sql.ForeignKey(\"srr_metadata.srr_id\"),\n",
    "#        primary_key=True,\n",
    "#    )\n",
    "#    fastqc_complete = sql.Column(sql.Boolean)\n",
    "#    bbduk_complete = sql.Column(sql.Boolean)\n",
    "#    hisat2_complete = sql.Column(sql.Boolean)\n",
    "#    samtools_complete = sql.Column(sql.Boolean)\n",
    "#    fastq_dump_complete = sql.Column(sql.Boolean)\n",
    "#    pileup_complete = sql.Column(sql.String(length=127))\n",
    "#    preseq_complete = sql.Column(sql.Boolean)\n",
    "#    rseqc_complete = sql.Column(sql.String(length=127))\n",
    "#    bedtools_complete = sql.Column(sql.Boolean)\n",
    "#    igv_tools_complete = sql.Column(sql.Boolean)\n",
    "#    fstitch_complete = sql.Column(sql.Boolean)\n",
    "#    tfit_complete = sql.Column(sql.Boolean)\n",
    "\n",
    "\n",
    "# class md_score(Base):\n",
    "#    __tablename__ = \"md_score\"\n",
    "#    srr_id = sql.Column(\n",
    "#        sql.String(length=127),\n",
    "#        sql.ForeignKey(\"srr_metadata.srr_id\"),\n",
    "#        primary_key=True,\n",
    "#    )\n",
    "#    tf_id = sql.Column(sql.String, sql.ForeignKey(\"tf.tf_id\"))\n",
    "#    erna_type = sql.Column(sql.String(length=127))\n",
    "#    md_score_expected = sql.Column(sql.Integer)\n",
    "#    md_score_std = sql.Column(sql.Integer)\n",
    "\n",
    "\n",
    "# dborm.py ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load dbutils.py\n",
    "\"\"\"Functions for building and maintaining DBNascent.\n",
    "\n",
    "Filename: dbutils.py\n",
    "Authors: Lynn Sanford <lynn.sanford@colorado.edu> and Zach Maas\n",
    "\n",
    "Commentary:\n",
    "    This module contains utility functions and classes for\n",
    "    reducing the total amount of code needed for building and\n",
    "    updating the database\n",
    "\n",
    "Classes:\n",
    "    dbnascentConnection\n",
    "    Metatable\n",
    "\n",
    "Functions:\n",
    "    load_config(file) -> object\n",
    "    add_tables(db_url)\n",
    "    table_parse(file) -> list of dicts\n",
    "    key_grab(dict, list) -> list of lists\n",
    "    get_unique_table(file, list) -> dict\n",
    "    value_compare(object, dict, dict)\n",
    "    object_as_dict(object)\n",
    "    scrape_fastqc(object) -> list of dicts\n",
    "\n",
    "Misc variables:\n",
    "\"\"\"\n",
    "\n",
    "import configparser\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pymysql\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.serializer import loads, dumps\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import shutil\n",
    "from statistics import median\n",
    "import yaml\n",
    "import zipfile as zp\n",
    "#from . import dborm\n",
    "\n",
    "\n",
    "# Database Connection Handler\n",
    "class dbnascentConnection:\n",
    "    \"\"\"A class to handle connection to the mysql database.\n",
    "\n",
    "    Attributes:\n",
    "        engine (dialect, pool objects) : engine created by sqlalchemy\n",
    "\n",
    "        session (session object) : ORM session object created by sqlalchemy\n",
    "\n",
    "    Methods:\n",
    "        __enter__ :\n",
    "    \"\"\"\n",
    "\n",
    "    engine = None\n",
    "    _Session = None\n",
    "    session = None\n",
    "\n",
    "    def __init__(self, db_url, cred_path):\n",
    "        \"\"\"Initialize database connection.\n",
    "\n",
    "        Parameters:\n",
    "            db_url (str) : path to database (mandatory)\n",
    "\n",
    "            cred_path (str) : path to tab-delimited credentials\n",
    "                one line file with username tab password\n",
    "\n",
    "        Returns:\n",
    "            none\n",
    "        \"\"\"\n",
    "        if cred_path:\n",
    "            with open(cred_path) as f:\n",
    "                cred = next(f).split(\"\\t\")\n",
    "            self.engine = sql.create_engine(\"mysql+pymysql://\" + str(cred[0]) + \":\"\n",
    "                                            + str(cred[1].split(\"\\n\")[0])\n",
    "                                            + \"@localhost/dbnascent\", echo=False)\n",
    "        elif db_url:\n",
    "            self.engine = sql.create_engine(\"mysql+pymysql://\" + db_url, echo=False)\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                \"Database url must be provided\"\n",
    "            )\n",
    "        self.Session = sessionmaker(bind=self.engine)\n",
    "        self.session = self.Session()\n",
    "\n",
    "    def add_tables(self) -> None:\n",
    "        \"\"\"Add tables into database from ORM.\n",
    "\n",
    "        Does not update existing tables.\n",
    "\n",
    "        Parameters:\n",
    "            none\n",
    "\n",
    "        Returns:\n",
    "            none\n",
    "        \"\"\"\n",
    "        Base.metadata.create_all(self.engine)\n",
    "\n",
    "    def reflect_table(self, table, filter_crit=None) -> list:\n",
    "        \"\"\"Query all records from a specific table.\n",
    "\n",
    "        Can optionally add filtering criteria.\n",
    "\n",
    "        Parameters:\n",
    "            table (str) : string of table name from ORM\n",
    "\n",
    "            filter_crit (dict) : filter criteria for table\n",
    "\n",
    "        Returns:\n",
    "            query_data (list of dicts) : all data in table\n",
    "                                         matching filter criteria\n",
    "        \"\"\"\n",
    "        query_data = []\n",
    "\n",
    "        query_str = \"SELECT * FROM \" + table\n",
    "        if filter_crit is not None:\n",
    "            query_str = query_str + \" WHERE \"\n",
    "            i = 0\n",
    "            for key in filter_crit:\n",
    "                if i == 0:\n",
    "                    query_str = (query_str + str(key) +\n",
    "                                 ' = \"' + str(filter_crit[key]) + '\"')\n",
    "                    i = (i + 1)\n",
    "                else:\n",
    "                    query_str = (query_str + \" AND \" + str(key) +\n",
    "                                 ' = \"' + str(filter_crit[key]) + '\"')\n",
    "\n",
    "        sqlquery = self.session.execute(sql.text(query_str)).fetchall()\n",
    "\n",
    "        for entry in sqlquery:\n",
    "            query_data.append(dict(entry))\n",
    "\n",
    "        return query_data\n",
    "\n",
    "    def backup(self, out_path, tables=False):\n",
    "        \"\"\"Backup database (whole or specific tables).\n",
    "\n",
    "        Parameters:\n",
    "            out_path (str) : path to backup file directory\n",
    "\n",
    "            tables (list) : list of specific tables, if whole\n",
    "                            database backup is not desired\n",
    "\n",
    "        Returns:\n",
    "            none\n",
    "        \"\"\"\n",
    "        if not tables:\n",
    "            Base.metadata.reflect(bind=self.engine)\n",
    "            tables = list(Base.metadata.tables.keys())\n",
    "        for table in tables:\n",
    "            outfile = out_path + \"/\" + table + \".dbdump\"\n",
    "            q = self.session.query(table)\n",
    "            serialized_data = dumps(q.all())\n",
    "            with open(outfile, 'w') as out:\n",
    "                out.write(str(serialized_data))\n",
    "\n",
    "    def restore(self, in_path, tables):\n",
    "        \"\"\"Restore database (whole or specific tables).\n",
    "\n",
    "        Parameters:\n",
    "            in_path (str) : path to backup file directory\n",
    "\n",
    "            tables (list) : list of specific tables, if whole\n",
    "                            database backup is not desired\n",
    "\n",
    "        Returns:\n",
    "            none\n",
    "        \"\"\"\n",
    "        if not tables:\n",
    "            files = os.listdir(in_path)\n",
    "            tables = []\n",
    "            for file in files:\n",
    "                tables.append(file.split(\".\")[0])\n",
    "        for table in tables:\n",
    "            infile = in_path + \"/\" + table + \".dbdump\"\n",
    "            with open(infile) as f:\n",
    "                serialized_data = dict(f)\n",
    "            self.session.merge(serialized_data)\n",
    "\n",
    "#    def __enter__(self):\n",
    "#        return self.session\n",
    "#\n",
    "#    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "#        self.session.commit()\n",
    "#        self.engine.dispose()\n",
    "\n",
    "\n",
    "# Metatable class definition\n",
    "class Metatable:\n",
    "    \"\"\"A class to store metadata.\n",
    "\n",
    "    Attributes:\n",
    "        data (list of dicts) :\n",
    "\n",
    "    Methods:\n",
    "        load_file :\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, meta_path, dictlist=None):\n",
    "        \"\"\"Initialize metatable object.\n",
    "\n",
    "        Parameters:\n",
    "            meta_path (str) : path to metadata file\n",
    "                file must be tab-delimited with field names as header\n",
    "\n",
    "            dictlist (list of dicts) : if not path, list of dicts\n",
    "                this can convert a list of dicts into the self.data of\n",
    "                a metatable object\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "\n",
    "        if meta_path:\n",
    "            self.load_file(meta_path)\n",
    "        elif dictlist:\n",
    "            self.data = dictlist\n",
    "\n",
    "    def load_file(self, meta_path):\n",
    "        \"\"\"Load metatable object.\n",
    "\n",
    "        Parameters:\n",
    "            meta_path (str) : path to metadata file\n",
    "                file must be tab-delimited with field names as header\n",
    "\n",
    "        Returns:\n",
    "            self.data (list of dicts)\n",
    "        \"\"\"\n",
    "        # Check that the metadata file exists\n",
    "        if not (os.path.exists(meta_path)\n",
    "                and os.path.isfile(meta_path)):\n",
    "            raise FileNotFoundError(\n",
    "                \"Metadata file does not exist at the provided path\")\n",
    "\n",
    "        with open(meta_path, newline=\"\") as metatab:\n",
    "            full_table = list(csv.DictReader(metatab, delimiter=\"\\t\"))\n",
    "            if len(full_table[0]) == 1:\n",
    "                raise IndexError(\n",
    "                    \"Input must be tab-delimited. Double check input.\"\n",
    "                )\n",
    "            else:\n",
    "                for entry in full_table:\n",
    "                    self.data.append(dict(entry))\n",
    "\n",
    "    def value_grab(self, key_list) -> list:\n",
    "        \"\"\"Extract values for specific keys from metatable data.\n",
    "\n",
    "        Parameters:\n",
    "            key_list (list) : desired keys from dicts in table_list\n",
    "\n",
    "        Returns:\n",
    "            value_list (list of lists) : each entry containing the values\n",
    "                                         of the given keys\n",
    "        \"\"\"\n",
    "        # Load in file as a list of dicts\n",
    "        value_list = []\n",
    "\n",
    "        if len(self.data) == 0:\n",
    "            return value_list\n",
    "\n",
    "        # Check if keys are valid\n",
    "        for key in key_list:\n",
    "            if key not in self.data[0]:\n",
    "                raise KeyError(\n",
    "                    \"Key(s) not present in metatable object.\"\n",
    "                )\n",
    "\n",
    "        for entry in self.data:\n",
    "            value_subset = []\n",
    "            for key in key_list:\n",
    "                value_subset.append(entry[key])\n",
    "            value_list.append(value_subset)\n",
    "\n",
    "        return value_list\n",
    "\n",
    "    def key_grab(self, key_list) -> list:\n",
    "        \"\"\"Extract dicts with specific keys from metatable data.\n",
    "\n",
    "        Parameters:\n",
    "            key_list (list) : desired keys from dicts in table_list\n",
    "\n",
    "        Returns:\n",
    "            dict_list (list of dicts) : each entry containing the dicts\n",
    "                                        with only the given keys\n",
    "        \"\"\"\n",
    "        dict_list = []\n",
    "\n",
    "        if len(self.data) == 0:\n",
    "            return dict_list\n",
    "\n",
    "        # Check if keys are valid\n",
    "        for key in key_list:\n",
    "            if key not in self.data[0]:\n",
    "                raise KeyError(\n",
    "                    \"Key(s) not present in metatable object.\"\n",
    "                )\n",
    "\n",
    "        for entry in self.data:\n",
    "            newentry = dict()\n",
    "            for key in key_list:\n",
    "                newentry[key] = entry[key]\n",
    "            dict_list.append(newentry)\n",
    "\n",
    "        return dict_list\n",
    "\n",
    "    def key_replace(self, file_keys, db_keys):\n",
    "        \"\"\"Replace file keys with database keys.\n",
    "\n",
    "        Parameters:\n",
    "            file_keys (list) : list of keys in file\n",
    "\n",
    "            db_keys (list) : list of keys in database\n",
    "                Must be equivalent in length to file_keys\n",
    "\n",
    "        Returns:\n",
    "            self.data (list of dicts)\n",
    "        \"\"\"\n",
    "        # Check if keys are valid\n",
    "        for key in file_keys:\n",
    "            if key not in self.data[0]:\n",
    "                raise KeyError(\n",
    "                    \"Key(s) not present in metatable object.\"\n",
    "                )\n",
    "\n",
    "        for entry in self.data:\n",
    "            for i in range(len(file_keys)):\n",
    "                entry[db_keys[i]] = entry.pop(file_keys[i])\n",
    "\n",
    "    def unique(self, extract_keys) -> list:\n",
    "        \"\"\"Extract values for specific keys from a metatable filepath.\n",
    "\n",
    "        Parameters:\n",
    "            extract_keys (list) : list containing db key labels for binding\n",
    "\n",
    "        Returns:\n",
    "            unique_metatable (list of dicts) : each entry contains the values\n",
    "                                               of the extract keys; only\n",
    "                                               returns unique sets of values\n",
    "        \"\"\"\n",
    "        unique_metatable = []\n",
    "\n",
    "        if len(self.data) == 0:\n",
    "            return unique_metatable\n",
    "\n",
    "        # Check if keys are valid\n",
    "        for key in extract_keys:\n",
    "            if key not in self.data[0]:\n",
    "                raise KeyError(\n",
    "                    \"Key(s) not present in metatable object.\"\n",
    "                )\n",
    "\n",
    "        full_table_list = np.array(self.value_grab(extract_keys))\n",
    "        unique_list = np.unique(full_table_list, axis=0)\n",
    "\n",
    "        for entry in unique_list:\n",
    "            new_dict = dict(zip(extract_keys, entry))\n",
    "            unique_metatable.append(new_dict)\n",
    "\n",
    "        return unique_metatable\n",
    "\n",
    "\n",
    "# Configuration File Reader\n",
    "def load_config(filename: str):\n",
    "    \"\"\"Load database config file compatible with configparser package.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str) : path to config file\n",
    "\n",
    "    Returns:\n",
    "        config (configparser object) : parsed config file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(\n",
    "            \"Configuration file does not exist at the provided path\"\n",
    "        )\n",
    "    config = configparser.ConfigParser()\n",
    "    with open(filename) as confFile:\n",
    "        config.read_string(confFile.read())\n",
    "    return config\n",
    "\n",
    "\n",
    "def value_compare(db_row, metatable_row, key_dict) -> bool:\n",
    "    \"\"\"Compare values between two dicts.\n",
    "\n",
    "    Parameters:\n",
    "        db_row (dict) : dict extracted from one entry in\n",
    "                        one table of the database\n",
    "\n",
    "        metatable_row (dict) : dict extracted from a metadata table\n",
    "\n",
    "        key_dict (dict) : specific keys for comparison\n",
    "\n",
    "    Returns:\n",
    "        {0,1} (boolean) : whether the value in the database matches the\n",
    "                          metadata value; 1 if matching, 0 if not\n",
    "    \"\"\"\n",
    "    for key in key_dict:\n",
    "        if db_row[key] == metatable_row[key_dict[key]]:\n",
    "            continue\n",
    "        else:\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def listdict_compare(comp_dict, db_dict, db_keys) -> list:\n",
    "    \"\"\"Compare two lists of dicts and take any rows not already in db.\n",
    "    Converts all values to strings for comparison purposes\n",
    "\n",
    "    Parameters:\n",
    "        comp_dict (list of dicts) : list of dicts from metatable object\n",
    "\n",
    "        db_dict (list of dicts) : list of dicts extracted from db query\n",
    "\n",
    "        db_keys (list) : specific keys for comparison\n",
    "\n",
    "    Returns:\n",
    "        data_to_add (list of dicts) : any dicts in comp_dict not in db_dict\n",
    "    \"\"\"\n",
    "    data_to_add = []\n",
    "\n",
    "    for entry in comp_dict:\n",
    "        for key in db_keys:\n",
    "            entry[key] = str(entry[key])\n",
    "\n",
    "    for entry in db_dict:\n",
    "        for key in db_keys:\n",
    "            entry[key] = str(entry[key])\n",
    "\n",
    "    for comp_entry in comp_dict:\n",
    "        if comp_entry not in db_dict:\n",
    "            data_to_add.append(comp_entry)\n",
    "\n",
    "    return data_to_add\n",
    "\n",
    "\n",
    "def key_store_compare(comp_dict, db_dict, db_keys, store_keys) -> list:\n",
    "    \"\"\"Compare two lists of dicts and take any rows not already in db.\n",
    "\n",
    "    Converts all values to strings for comparison purposes\n",
    "\n",
    "    Parameters:\n",
    "        comp_dict (dict) : single dict from metatable object\n",
    "\n",
    "        db_dict (list of dicts) : list of dicts extracted from db query\n",
    "\n",
    "        db_keys (list) : specific keys for comparison\n",
    "\n",
    "        store_keys (list) : key(s) for adding to dict\n",
    "\n",
    "    Returns:\n",
    "        new_dict (dict) : dict with new value added\n",
    "    \"\"\"\n",
    "    for dbentry in db_dict:\n",
    "        comp = 0\n",
    "        for key in db_keys:\n",
    "            if str(comp_dict[key]) != str(dbentry[key]):\n",
    "                if str(comp_dict[key]) == \"\":\n",
    "                    if dbentry[key] != None:\n",
    "                        comp = 1\n",
    "                else:\n",
    "                    comp = 1\n",
    "        if comp == 0:\n",
    "            for stkey in store_keys:\n",
    "                comp_dict[stkey] = dbentry[stkey]\n",
    "\n",
    "    return comp_dict\n",
    "\n",
    "\n",
    "def object_as_dict(obj):\n",
    "    \"\"\"Convert queried database entry into dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj (str) : single row (entry) of a database query output\n",
    "\n",
    "    Returns:\n",
    "        db_dict (dict) : key-value pairs from database entry\n",
    "    \"\"\"\n",
    "    db_dict = {c.key: getattr(obj, c.key) for c\n",
    "               in sql.inspect(obj).mapper.column_attrs}\n",
    "    return db_dict\n",
    "\n",
    "\n",
    "def entry_update(dbtable, dbkeys, comp_table) -> list:\n",
    "    \"\"\"Find entries not already in database.\n",
    "\n",
    "    Parameters:\n",
    "        dbtable (str) : Which db table to search for entries\n",
    "\n",
    "        dbkeys (list) : list of keys to use for comparison\n",
    "\n",
    "        comp_table (list of dicts) : Entries to match (or not)\n",
    "                                     to db entries\n",
    "\n",
    "    Returns:\n",
    "        to_add (list of dicts) : New entries not in db to add\n",
    "    \"\"\"\n",
    "    db_dump = dbconnect.reflect_table(dbtable)\n",
    "    dbtab = Metatable(meta_path=None, dictlist=db_dump)\n",
    "    dbtab_data = dbtab.key_grab(dbkeys)\n",
    "    to_add = listdict_compare(comp_table, dbtab_data, dbkeys)\n",
    "\n",
    "    return to_add\n",
    "\n",
    "\n",
    "def scrape_fastqc(paper_id, sample_name, data_path, db_sample) -> dict:\n",
    "    \"\"\"Scrape read length and depth from fastQC report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "        db_sample (dict) : sample_accum entry dict from db query\n",
    "\n",
    "    Returns:\n",
    "        fastqc_dict (dict) : scraped fastqc metadata in dict format\n",
    "    \"\"\"\n",
    "    fastqc_dict = {}\n",
    "\n",
    "    # Determine paths for raw fastQC file to scrape, depending on SE/PE\n",
    "    fqc_path = data_path + \"qc/fastqc/zips/\"\n",
    "    if db_sample[\"single_paired\"] == \"paired\":\n",
    "        samp_zip = fqc_path + sample_name + \"_1_fastqc\"\n",
    "    else:\n",
    "        samp_zip = fqc_path + sample_name + \"_fastqc\"\n",
    "\n",
    "    # If fastQC files don't exist, return null values\n",
    "    if not (os.path.exists(samp_zip + \".zip\")):\n",
    "        fastqc_dict[\"raw_read_depth\"] = None\n",
    "        fastqc_dict[\"raw_read_length\"] = None\n",
    "        fastqc_dict[\"trim_read_depth\"] = None\n",
    "        return fastqc_dict\n",
    "\n",
    "    # Unzip fastQC report\n",
    "    with zp.ZipFile(samp_zip + \".zip\", \"r\") as zp_ref:\n",
    "        zp_ref.extractall(fqc_path)\n",
    "\n",
    "    # Extract raw depth and read length\n",
    "    fdata = open(samp_zip + \"/fastqc_data.txt\")\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Sequences\").search(line):\n",
    "            fastqc_dict[\"raw_read_depth\"] = int(line.split()[2])\n",
    "        if re.compile(\"Sequence length\").search(line):\n",
    "            fastqc_dict[\"raw_read_length\"] = int(line.split()[2].split(\"-\")[0])\n",
    "\n",
    "    # Remove unzipped file\n",
    "    shutil.rmtree(samp_zip)\n",
    "\n",
    "    # Determine paths for trimmed fastQC file to scrape, depending on SE/PE\n",
    "    # and whether reverse complemented or not\n",
    "    if str(db_sample[\"rcomp\"]) == '1':\n",
    "        if db_sample[\"single_paired\"] == \"paired\":\n",
    "            samp_zip = fqc_path + sample_name + \"_1.flip.trim_fastqc\"\n",
    "        else:\n",
    "            samp_zip = fqc_path + sample_name + \".flip.trim_fastqc\"\n",
    "    else:\n",
    "        if db_sample[\"single_paired\"] == \"paired\":\n",
    "            samp_zip = fqc_path + sample_name + \"_1.trim_fastqc\"\n",
    "        else:\n",
    "            samp_zip = fqc_path + sample_name + \".trim_fastqc\"\n",
    "\n",
    "    # If trimmed fastQC report doesn't exist, return null value for\n",
    "    # trimmed read depth\n",
    "    if not (os.path.exists(samp_zip + \".zip\")):\n",
    "        fastqc_dict[\"trim_read_depth\"] = None\n",
    "        return fastqc_dict\n",
    "\n",
    "    # Unzip trimmed fastQC report\n",
    "    with zp.ZipFile(samp_zip + \".zip\", \"r\") as zp_ref:\n",
    "        zp_ref.extractall(fqc_path)\n",
    "\n",
    "    # Extract trimmed read depth\n",
    "    fdata = open(samp_zip + \"/fastqc_data.txt\")\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Sequences\").search(line):\n",
    "            fastqc_dict[\"trim_read_depth\"] = int(line.split()[2])\n",
    "\n",
    "    # Remove unzipped file\n",
    "    shutil.rmtree(samp_zip)\n",
    "\n",
    "    return fastqc_dict\n",
    "\n",
    "\n",
    "def scrape_picard(paper_id, sample_name, data_path):\n",
    "    \"\"\"Scrape read length and depth from picard duplication report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "    Returns:\n",
    "        picard_dict (dict) : scraped picard metadata in dict format\n",
    "    \"\"\"\n",
    "    picard_dict = {}\n",
    "\n",
    "    dirpath = data_path + \"qc/picard/dups/\"\n",
    "    filepath = dirpath + sample_name + \".marked_dup_metrics.txt\"\n",
    "\n",
    "    # If picardtools data doesn't exist, return null value\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        picard_dict[\"duplication_picard\"] = None\n",
    "        return picard_dict\n",
    "\n",
    "    # Extract duplication data\n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Unknown Library\").search(line):\n",
    "            dup = float(line.split(\"\\t\")[8])\n",
    "            picard_dict[\"duplication_picard\"] = round(dup, 5)\n",
    "\n",
    "    return picard_dict\n",
    "\n",
    "\n",
    "def scrape_mapstats(paper_id, sample_name, data_path, db_sample):\n",
    "    \"\"\"Scrape read length and depth from hisat2 mapstats report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "        db_sample (dict) : sample_accum entry dict from db query\n",
    "\n",
    "    Returns:\n",
    "        mapstats_dict (dict) : scraped hisat2 metadata in dict format\n",
    "    \"\"\"\n",
    "    mapstats_dict = {}\n",
    "\n",
    "    dirpath = data_path + \"qc/hisat2_mapstats/\"\n",
    "    filepath = dirpath + sample_name + \".hisat2_mapstats.txt\"\n",
    "\n",
    "    # If hisat mapping data doesn't exist, return null values\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        mapstats_dict[\"single_map\"] = None\n",
    "        mapstats_dict[\"multi_map\"] = None\n",
    "        mapstats_dict[\"map_prop\"] = None\n",
    "        return mapstats_dict\n",
    "\n",
    "    fdata = open(filepath)\n",
    "\n",
    "    # Sum up and report mapped reads for paired end data\n",
    "    if db_sample[\"single_paired\"] == \"paired\":\n",
    "        for line in fdata:\n",
    "            if re.compile(\"concordantly 1 time\").search(line):\n",
    "                reads = int(line.split(\": \")[1].split(\" (\")[0]) * 2\n",
    "            if re.compile(\"Aligned 1 time\").search(line):\n",
    "                mapstats_dict[\"single_map\"] = reads + int(\n",
    "                    line.split(\": \")[1].split(\" (\")[0]\n",
    "                )\n",
    "            if re.compile(\"concordantly >1 times\").search(line):\n",
    "                reads = int(line.split(\": \")[1].split(\" (\")[0]) * 2\n",
    "            if re.compile(\"Aligned >1 times\").search(line):\n",
    "                mapstats_dict[\"multi_map\"] = reads + int(\n",
    "                    line.split(\": \")[1].split(\" (\")[0]\n",
    "                )\n",
    "            if re.compile(\"Overall alignment rate\").search(line):\n",
    "                alrate = float(line.split(\": \")[1].split(\"%\")[0]) / 100\n",
    "                mapstats_dict[\"map_prop\"] = round(alrate, 5)\n",
    "    # Report mapped reads for single end data\n",
    "    else:\n",
    "        for line in fdata:\n",
    "            if re.compile(\"Aligned 1 time\").search(line):\n",
    "                mapstats_dict[\"single_map\"] = int(line.split(\": \")[1].split(\" (\")[0])\n",
    "            if re.compile(\"Aligned >1 times\").search(line):\n",
    "                mapstats_dict[\"multi_map\"] = int(line.split(\": \")[1].split(\" (\")[0])\n",
    "            if re.compile(\"Overall alignment rate\").search(line):\n",
    "                alrate = float(line.split(\": \")[1].split(\"%\")[0]) / 100\n",
    "                mapstats_dict[\"map_prop\"] = round(alrate, 5)\n",
    "\n",
    "    return mapstats_dict\n",
    "\n",
    "\n",
    "def scrape_rseqc(paper_id, sample_name, data_path):\n",
    "    \"\"\"Scrape read length and depth from RSeQC report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "    Returns:\n",
    "        rseqc_dict (dict) : scraped RSeQC metadata in dict format\n",
    "    \"\"\"\n",
    "    rseqc_dict = {}\n",
    "\n",
    "    dirpath = data_path + \"qc/rseqc/read_distribution/\"\n",
    "    filepath = dirpath + sample_name + \".read_distribution.txt\"\n",
    "\n",
    "    # If rseqc read distribution data doesn't exist, return null values\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        rseqc_dict[\"rseqc_tags\"] = None\n",
    "        rseqc_dict[\"rseqc_cds\"] = None\n",
    "        rseqc_dict[\"cds_rpk\"] = None\n",
    "        rseqc_dict[\"rseqc_five_utr\"] = None\n",
    "        rseqc_dict[\"rseqc_three_utr\"] = None\n",
    "        rseqc_dict[\"rseqc_intron\"] = None\n",
    "        rseqc_dict[\"intron_rpk\"] = None\n",
    "        rseqc_dict[\"exint_ratio\"] = None\n",
    "        return rseqc_dict\n",
    "\n",
    "    # Extract RSeQC data\n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Assigned Tags\").search(line):\n",
    "            rseqc_dict[\"rseqc_tags\"] = int(line.split()[-1])\n",
    "        if re.compile(\"CDS_Exons\").search(line):\n",
    "            rseqc_dict[\"rseqc_cds\"] = int(line.split()[2])\n",
    "            cds = float(line.split()[-1])\n",
    "            rseqc_dict[\"cds_rpk\"] = round(cds, 5)\n",
    "        if re.compile(\"5'UTR_Exons\").search(line):\n",
    "            rseqc_dict[\"rseqc_five_utr\"] = int(line.split()[2])\n",
    "        if re.compile(\"3'UTR_Exons\").search(line):\n",
    "            rseqc_dict[\"rseqc_three_utr\"] = int(line.split()[2])\n",
    "        if re.compile(\"Introns\").search(line):\n",
    "            rseqc_dict[\"rseqc_intron\"] = int(line.split()[2])\n",
    "            intron = float(line.split()[-1])\n",
    "            rseqc_dict[\"intron_rpk\"] = round(intron, 5)\n",
    "\n",
    "    if rseqc_dict[\"intron_rpk\"] > 0:\n",
    "        exint_ratio = rseqc_dict[\"cds_rpk\"] / rseqc_dict[\"intron_rpk\"]\n",
    "        rseqc_dict[\"exint_ratio\"] = round(exint_ratio, 5)\n",
    "    else:\n",
    "        rseqc_dict[\"exint_ratio\"] = None\n",
    "\n",
    "    return rseqc_dict\n",
    "\n",
    "\n",
    "def scrape_preseq(paper_id, sample_name, data_path):\n",
    "    \"\"\"Scrape read length and depth from preseq complexity report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "    Returns:\n",
    "        preseq_dict (dict) : scraped preseq metadata in dict format\n",
    "    \"\"\"\n",
    "    preseq_dict = {}\n",
    "\n",
    "    dirpath = data_path + \"qc/preseq/\"\n",
    "    filepath = dirpath + sample_name + \".lc_extrap.txt\"\n",
    "\n",
    "    # If preseq complexity data doesn't exist, return null value\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        preseq_dict[\"distinct_tenmillion_prop\"] = None\n",
    "        return preseq_dict\n",
    "\n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if line.startswith(\"10000000.0\"):\n",
    "            distinct = float(line.split()[1]) / 10000000\n",
    "\n",
    "    preseq_dict[\"distinct_tenmillion_prop\"] = round(distinct, 5)\n",
    "\n",
    "    return preseq_dict\n",
    "\n",
    "\n",
    "def scrape_pileup(paper_id, sample_name, data_path):\n",
    "    \"\"\"Scrape read length and depth from pileup report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "    Returns:\n",
    "        pileup_dict (dict) : scraped pileup metadata in dict format\n",
    "    \"\"\"\n",
    "    pileup_dict = {}\n",
    "\n",
    "    dirpath = data_path + \"qc/pileup/\"\n",
    "    filepath = dirpath + sample_name + \".coverage.stats.txt\"\n",
    "\n",
    "    # If pileup complexity data doesn't exist, return null value\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        pileup_dict[\"genome_prop_cov\"] = None\n",
    "        pileup_dict[\"avg_fold_cov\"] = None\n",
    "        return pileup_dict\n",
    "\n",
    "    # Add up reads in different categories to calculate coverage\n",
    "    fdata = open(filepath)\n",
    "    x = 0\n",
    "    total = cov = fold = 0\n",
    "    for line in fdata:\n",
    "        if x == 0:\n",
    "            x = x + 1\n",
    "            continue\n",
    "        else:\n",
    "            x = x + 1\n",
    "            total = total + int(line.split(\"\\t\")[2])\n",
    "            cov = cov + int(line.split(\"\\t\")[5])\n",
    "            fold = fold + (float(line.split(\"\\t\")[1])\n",
    "                           * int(line.split(\"\\t\")[2]))\n",
    "\n",
    "    pileup_dict[\"genome_prop_cov\"] = round((cov / total), 5)\n",
    "    pileup_dict[\"avg_fold_cov\"] = round((fold / total), 5)\n",
    "\n",
    "    return pileup_dict\n",
    "\n",
    "\n",
    "def sample_qc_calc(db_sample):\n",
    "    \"\"\"Calculate sample qc and data scores.\n",
    "\n",
    "    Parameters:\n",
    "        db_sample (dict) : sample_accum entry dict from db query\n",
    "\n",
    "    Returns:\n",
    "        samp_score (int) : calculated sample scores in dict format\n",
    "    \"\"\"\n",
    "    samp_score = dict()\n",
    "    trimrd = db_sample[\"trim_read_depth\"]\n",
    "    dup = db_sample[\"duplication_picard\"]\n",
    "    mapped = db_sample[\"map_prop\"]\n",
    "    complexity = db_sample[\"distinct_tenmillion_prop\"]\n",
    "    genome = db_sample[\"genome_prop_cov\"]\n",
    "    exint = db_sample[\"exint_ratio\"]\n",
    "\n",
    "    # Determine sample QC score\n",
    "    if (trimrd is None\n",
    "       or dup is None\n",
    "       or mapped is None\n",
    "       or complexity is None):\n",
    "\n",
    "        samp_score[\"samp_qc_score\"] = 0\n",
    "\n",
    "    elif (trimrd <= 5000000\n",
    "          or dup >= 0.95\n",
    "          or (mapped * trimrd) <= 4000000\n",
    "          or complexity < 0.05):\n",
    "\n",
    "        samp_score[\"samp_qc_score\"] = 5\n",
    "\n",
    "    elif (trimrd <= 10000000\n",
    "          or dup >= 0.80\n",
    "          or (mapped * trimrd) <= 8000000\n",
    "          or complexity < 0.2):\n",
    "\n",
    "        samp_score[\"samp_qc_score\"] = 4\n",
    "\n",
    "    elif (trimrd <= 15000000\n",
    "          or dup >= 0.65\n",
    "          or (mapped * trimrd) <= 12000000\n",
    "          or complexity < 0.35):\n",
    "\n",
    "        samp_score[\"samp_qc_score\"] = 3\n",
    "\n",
    "    elif (trimrd <= 20000000\n",
    "          or dup >= 0.5\n",
    "          or (mapped * trimrd) <= 16000000\n",
    "          or complexity < 0.5):\n",
    "\n",
    "        samp_score[\"samp_qc_score\"] = 2\n",
    "\n",
    "    else:\n",
    "        samp_score[\"samp_qc_score\"] = 1\n",
    "\n",
    "    # Determine sample data score\n",
    "    if (genome is None\n",
    "       or exint is None):\n",
    "\n",
    "        samp_score[\"samp_data_score\"] = 0\n",
    "\n",
    "    elif (genome <= 0.04\n",
    "          or exint >= 9):\n",
    "\n",
    "        samp_score[\"samp_data_score\"] = 5\n",
    "\n",
    "    elif (genome <= 0.08\n",
    "          or exint >= 7):\n",
    "\n",
    "        samp_score[\"samp_data_score\"] = 4\n",
    "\n",
    "    elif (genome <= 0.12\n",
    "          or exint >= 5):\n",
    "\n",
    "        samp_score[\"samp_data_score\"] = 3\n",
    "\n",
    "    elif (genome <= 0.16\n",
    "          or exint >= 3):\n",
    "        samp_score[\"samp_data_score\"] = 2\n",
    "\n",
    "    else:\n",
    "        samp_score[\"samp_data_score\"] = 1\n",
    "\n",
    "    return samp_score\n",
    "\n",
    "\n",
    "def paper_qc_calc(db_samples):\n",
    "    \"\"\"Calculate sample qc and data scores.\n",
    "\n",
    "    Parameters:\n",
    "        db_samples (list of dicts) : sample_accum entries from db query\n",
    "\n",
    "    Returns:\n",
    "        paper_scores (float) : calculated median scores in dict format\n",
    "    \"\"\"\n",
    "    qc_scores = []\n",
    "    data_scores = []\n",
    "    paper_scores = {}\n",
    "\n",
    "    for entry in db_samples:\n",
    "        qc_scores.append(int(entry[\"samp_qc_score\"]))\n",
    "        data_scores.append(int(entry[\"samp_data_score\"]))\n",
    "\n",
    "    paper_scores[\"paper_qc_score\"] = median(qc_scores)\n",
    "    paper_scores[\"paper_data_score\"] = median(data_scores)\n",
    "\n",
    "    return paper_scores\n",
    "\n",
    "\n",
    "def add_version_info(paper_id, data_path, vertype, dbver_keys):\n",
    "    \"\"\"Find nascentflow/bidirflow version info for a paper.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        data_path (str) : path to dbnascent data\n",
    "\n",
    "        vertype (str) : {\"nascent\", \"bidir\"} : Which nextflow type\n",
    "\n",
    "        dbver_keys (list) : list of keys for version tables\n",
    "\n",
    "    Returns:\n",
    "        ver_table (list of dicts) : all relevant version info for\n",
    "                                      entry into db\n",
    "    \"\"\"\n",
    "    ver_table = []\n",
    "\n",
    "    dblink_dump = dbconnect.reflect_table(\"linkIDs\", {\"paper_id\": paper_id})\n",
    "    for entry in dblink_dump:\n",
    "        del entry[\"genetic_id\"]\n",
    "        del entry[\"expt_id\"]\n",
    "        ver_path = (data_path +\n",
    "                    entry[\"sample_name\"] + \"_\" + vertype + \".yaml\")\n",
    "\n",
    "        if not (os.path.exists(ver_path) and os.path.isfile(ver_path)):\n",
    "            for key in dbver_keys:\n",
    "                entry.update({key: None})\n",
    "            ver_table.append(entry)\n",
    "            continue\n",
    "\n",
    "        with open(ver_path) as f:\n",
    "            for run in yaml.safe_load_all(f):\n",
    "                add_entry = dict()\n",
    "                add_entry.update(entry)\n",
    "                add_entry.update(run)\n",
    "                for key in dbver_keys:\n",
    "                    if not key in add_entry.keys():\n",
    "                        add_entry.update({key: None})\n",
    "                ver_table.append(add_entry)\n",
    "\n",
    "    return ver_table\n",
    "\n",
    "\n",
    "def dbnascent_backup(db, basedir, tables):\n",
    "    \"\"\"Create new database backup.\n",
    "\n",
    "    Parameters:\n",
    "        db (dbnascentConnection object) : current database connection\n",
    "\n",
    "        basedir (str) : path to base backup directory\n",
    "                        default /home/lsanford/Documents/data/dbnascent_backups\n",
    "\n",
    "        tables (list) : list of specific tables if whole db backup\n",
    "                        is not desired\n",
    "\n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    if not basedir:\n",
    "        basedir = \"/home/lsanford/Documents/data/dbnascent_backups\"\n",
    "    now = datetime.datetime.now()\n",
    "    nowdir = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    os.makedirs(basedir + \"/\" + nowdir)\n",
    "\n",
    "    if tables:\n",
    "        db.backup((basedir + \"/\" + nowdir), tables)\n",
    "    else:\n",
    "        db.backup((basedir + \"/\" + nowdir))\n",
    "\n",
    "\n",
    "def paper_add_update(db, config, identifier, basedir):\n",
    "    \"\"\"Add or update paper and associated sample metadata.\n",
    "\n",
    "    Parameters:\n",
    "        db (dbnascentConnection object) : current database connection\n",
    "\n",
    "        config (configParser object) : parsed config file\n",
    "\n",
    "        identifier (str) : paper identifier, used to locate all (meta)data\n",
    "\n",
    "        basedir (str) : path to base database data directory\n",
    "                        default /Shares/dbnascent\n",
    "\n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    # Add experimental metadata\n",
    "    expt_keys = list(dict(config[\"expt keys\"]).values())\n",
    "    if not basedir:\n",
    "        basedir = \"/Shares/dbnascent\"\n",
    "    exptmeta_path = basedir + \"/\" + identifier + \"/\"\n",
    "\n",
    "    # Read in expt metadata and make sure entries are unique\n",
    "    exptmeta = utils.Metatable(exptmeta_path + \"metadata/expt_metadata.txt\")\n",
    "    expt_unique = exptmeta.unique(expt_keys)\n",
    "\n",
    "    # Add expt metadata to database\n",
    "    db.engine.execute(exptMetadata.__table__.insert(), expt_unique.data())\n",
    "\n",
    "    # Add sample ids\n",
    "\n",
    "\n",
    "#engine.execute(tablename.__table__.insert(),listofdicts)\n",
    "#\n",
    "# dbutils.py ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = dbutils.load_config(\"/home/lsanford/Documents/data/repositories/dbnascent_build/config.txt\")\n",
    "config = load_config(\n",
    "    \"/home/lsanford/Documents/data/repositories/DBNascent-build/config.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define database location and (optionally) back up database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = config[\"file_locations\"][\"database\"]\n",
    "creds = config[\"file_locations\"][\"credentials\"]\n",
    "\n",
    "#dbconnect = utils.dbnascentConnection(db_url, creds)\n",
    "dbconnect = dbnascentConnection(db_url, creds)\n",
    "#utils.dbnascent_backup(dbconnect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add/update organism table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconnect = dbnascentConnection(db_url, creds)\n",
    "\n",
    "org_keys = list(dict(config[\"organism keys\"]).values())\n",
    "dborg_keys = list(dict(config[\"organism keys\"]).keys())\n",
    "orgtable_path = config[\"file_locations\"][\"organism_table\"]\n",
    "\n",
    "# Read in organism table and make sure entries are unique\n",
    "#orgs = dbutils.Metatable(orgtable_path)\n",
    "orgs = Metatable(orgtable_path)\n",
    "orgs.key_replace(org_keys, dborg_keys)\n",
    "orgs_unique = orgs.unique(dborg_keys)\n",
    "\n",
    "# If not already present, add data to database\n",
    "orgs_to_add = entry_update(\"organismInfo\", dborg_keys, orgs_unique)\n",
    "\n",
    "if len(orgs_to_add) > 0:\n",
    "    dbconnect.engine.execute(organismInfo.__table__.insert(), orgs_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse paper and sample metadata tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconnect = dbnascentConnection(db_url, creds)\n",
    "\n",
    "expt_keys = list(dict(config[\"expt keys\"]).values())\n",
    "dbexpt_keys = list(dict(config[\"expt keys\"]).keys())\n",
    "exptmeta_path = str(config[\"file_locations\"][\"db_data\"]) + str(paper_id) + \"/metadata/expt_metadata.txt\"\n",
    "samp_keys = list(dict(config[\"sample keys\"]).values())\n",
    "dbsamp_keys = list(dict(config[\"sample keys\"]).keys())\n",
    "sampmeta_path = str(config[\"file_locations\"][\"db_data\"]) + str(paper_id) + \"/metadata/sample_metadata.txt\"\n",
    "genetic_keys = list(dict(config[\"genetic keys\"]).values())\n",
    "dbgenetic_keys = list(dict(config[\"genetic keys\"]).keys())\n",
    "\n",
    "# Read in experimental metadata\n",
    "# expt = dbutils.Metatable(exptmeta_path)\n",
    "expt = Metatable(exptmeta_path)\n",
    "\n",
    "# Read in sample metadata and append experimental for whole metadata table\n",
    "# samp = dbutils.Metatable(sampmeta_path)\n",
    "samp = Metatable(sampmeta_path)\n",
    "for entry in samp.data:\n",
    "    entry.update(expt.data[0])\n",
    "    if not entry[\"srz\"]:\n",
    "        entry[\"srz\"] = entry[\"srr\"]\n",
    "\n",
    "samp.key_replace(samp_keys, dbsamp_keys)\n",
    "samp.key_replace(expt_keys, dbexpt_keys)\n",
    "samp.key_replace(genetic_keys, dbgenetic_keys)\n",
    "\n",
    "expt_unique = samp.unique(dbexpt_keys)\n",
    "samp_unique = samp.unique(dbsamp_keys)\n",
    "gene_unique = samp.unique(dbgenetic_keys)\n",
    "\n",
    "# If not already present, add data to database\n",
    "expt_to_add = entry_update(\"exptMetadata\", dbexpt_keys, expt_unique)\n",
    "for entry in expt_to_add:\n",
    "    for key in entry:\n",
    "        if entry[key] == '1':\n",
    "            entry[key] = True\n",
    "        elif entry[key] == '0':\n",
    "            entry[key] = False\n",
    "\n",
    "if len(expt_to_add) > 0:\n",
    "    dbconnect.engine.execute(exptMetadata.__table__.insert(), expt_to_add)\n",
    "\n",
    "dbsamp_dump = dbconnect.reflect_table(\"sampleID\")\n",
    "dbsamp = Metatable(meta_path=None, dictlist=dbsamp_dump)\n",
    "curr_id = 0\n",
    "for entry in dbsamp.data:\n",
    "    if entry[\"sample_id\"] > curr_id:\n",
    "        curr_id = entry[\"sample_id\"]\n",
    "\n",
    "samp_to_add = entry_update(\"sampleID\", dbsamp_keys, samp_unique)\n",
    "\n",
    "if len(samp_to_add) > 0:\n",
    "    samps_meta = Metatable(meta_path=None, dictlist=samp_to_add)\n",
    "    samp_id_hash = samps_meta.unique([\"sample_name\"])\n",
    "    for entry in samp_id_hash:\n",
    "        curr_id = curr_id + 1\n",
    "        entry[\"sample_id\"] = curr_id\n",
    "    for entry in samp_to_add:\n",
    "        hash_entry = list(filter(lambda samp_id_hash: samp_id_hash[\"sample_name\"]\n",
    "                                 == entry[\"sample_name\"], samp_id_hash))[0]\n",
    "        entry[\"sample_id\"] = hash_entry[\"sample_id\"]\n",
    "\n",
    "    dbconnect.engine.execute(sampleID.__table__.insert(), samp_to_add)\n",
    "\n",
    "gene_to_add = entry_update(\"geneticInfo\", dbgenetic_keys, gene_unique)\n",
    "\n",
    "if len(gene_to_add) > 0:\n",
    "    dbconnect.engine.execute(geneticInfo.__table__.insert(), gene_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Make linkIDs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconnect = dbnascentConnection(db_url, creds)\n",
    "\n",
    "link_keys = [\"sample_id\", \"genetic_id\", \"expt_id\",\n",
    "             \"sample_name\", \"paper_id\"]\n",
    "\n",
    "dbsamp_dump = dbconnect.reflect_table(\"sampleID\")\n",
    "dbexpt_dump = dbconnect.reflect_table(\"exptMetadata\")\n",
    "dbgene_dump = dbconnect.reflect_table(\"geneticInfo\")\n",
    "\n",
    "for entry in samp.data:\n",
    "    key_store_compare(entry, dbsamp_dump,\n",
    "                      dbsamp_keys, [\"sample_id\"])\n",
    "    key_store_compare(entry, dbexpt_dump,\n",
    "                      dbexpt_keys, [\"expt_id\"])\n",
    "    key_store_compare(entry, dbgene_dump,\n",
    "                      dbgenetic_keys, [\"genetic_id\"])\n",
    "\n",
    "link_unique = samp.unique(link_keys)\n",
    "\n",
    "link_to_add = entry_update(\"linkIDs\", link_keys, link_unique)\n",
    "\n",
    "if len(link_to_add) > 0:\n",
    "    dbconnect.engine.execute(linkIDs.__table__.insert(), link_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add condition info and build condition table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconnect = dbnascentConnection(db_url, creds)\n",
    "\n",
    "cond_keys = list(dict(config[\"metatable condition keys\"]).values())\n",
    "dbcond_keys = list(dict(config[\"metatable condition keys\"]).keys())\n",
    "dbcond_keys.append(\"sample_name\")\n",
    "cond_full_keys = list(dict(config[\"condition keys\"]).values())\n",
    "dbcond_full_keys = list(dict(config[\"condition keys\"]).keys())\n",
    "\n",
    "samp.key_replace(cond_keys, dbcond_keys)\n",
    "cond = samp.key_grab(dbcond_keys)\n",
    "cond_parsed = []\n",
    "for entry in cond:\n",
    "    if entry[\"treatment\"]:\n",
    "        cond_types = entry[\"condition_type\"].split(\";\")\n",
    "        treatments = entry[\"treatment\"].split(\";\")\n",
    "        times = entry[\"times\"].split(\";\")\n",
    "        for i in range(len(cond_types)):\n",
    "            new_entry = dict()\n",
    "            tx = treatments[i].split(\"(\")\n",
    "            tm = times[i].split(\",\")\n",
    "            new_entry[\"sample_name\"] = entry[\"sample_name\"]\n",
    "            new_entry[\"condition_type\"] = cond_types[i]\n",
    "            new_entry[\"treatment\"] = tx[0]\n",
    "            if len(tx) > 1:\n",
    "                new_entry[\"conc_intens\"] = tx[1].split(\")\")[0]\n",
    "            else:\n",
    "                new_entry[\"conc_intens\"] = \"\"\n",
    "            new_entry[\"start_time\"] = int(tm[0])\n",
    "            new_entry[\"end_time\"] = int(tm[1])\n",
    "            new_entry[\"time_unit\"] = tm[2]\n",
    "\n",
    "            duration = int(tm[1]) - int(tm[0])\n",
    "\n",
    "            # Calculate duration and units\n",
    "            if tm[2] == \"s\":\n",
    "                if duration % 60 == 0:\n",
    "                    if duration % 3600 == 0:\n",
    "                        if duration % 86400 == 0:\n",
    "                            duration = duration / 86400\n",
    "                            duration_unit = \"day\"\n",
    "                        else:\n",
    "                            duration = duration / 3600\n",
    "                            duration_unit = \"hr\"\n",
    "                    else:\n",
    "                        duration = duration / 60\n",
    "                        duration_unit = \"min\"\n",
    "                else:\n",
    "                    duration_unit = \"s\"\n",
    "            elif tm[2] == \"min\":\n",
    "                if duration % 60 == 0:\n",
    "                    if duration % 1440 == 0:\n",
    "                        duration = duration / 1440\n",
    "                        duration_unit = \"day\"\n",
    "                    else:\n",
    "                        duration = duration / 60\n",
    "                        duration_unit = \"hr\"\n",
    "                else:\n",
    "                    duration_unit = \"min\"\n",
    "            elif tm[2] == \"hr\":\n",
    "                if duration % 24 == 0:\n",
    "                    duration = duration / 24\n",
    "                    duration_unit = \"day\"\n",
    "                else:\n",
    "                    duration_unit = \"hr\"\n",
    "            else:\n",
    "                duration_unit = \"day\"\n",
    "\n",
    "            new_entry[\"duration\"] = int(duration)\n",
    "            new_entry[\"duration_unit\"] = duration_unit\n",
    "\n",
    "            cond_parsed.append(new_entry)\n",
    "\n",
    "    else:\n",
    "        new_entry = dict()\n",
    "        new_entry[\"sample_name\"] = entry[\"sample_name\"]\n",
    "        new_entry[\"condition_type\"] = \"no treatment\"\n",
    "        new_entry[\"treatment\"] = \"\"\n",
    "        new_entry[\"conc_intens\"] = \"\"\n",
    "        new_entry[\"start_time\"] = \"\"\n",
    "        new_entry[\"end_time\"] = \"\"\n",
    "        new_entry[\"time_unit\"] = \"\"\n",
    "        new_entry[\"duration\"] = \"\"\n",
    "        new_entry[\"duration_unit\"] = \"\"\n",
    "        cond_parsed.append(new_entry)\n",
    "\n",
    "cond = Metatable(meta_path=None, dictlist=cond_parsed)\n",
    "cond_unique = cond.unique(dbcond_full_keys)\n",
    "for entry in cond_unique:\n",
    "    if entry[\"start_time\"] == \"\":\n",
    "        entry[\"start_time\"] = None\n",
    "    if entry[\"end_time\"] == \"\":\n",
    "        entry[\"end_time\"] = None\n",
    "    if entry[\"duration\"] == \"\":\n",
    "        entry[\"duration\"] = None\n",
    "\n",
    "cond_to_add = entry_update(\"conditionInfo\", dbcond_full_keys, cond_unique)\n",
    "\n",
    "if len(cond_to_add) > 0:\n",
    "    for entry in cond_to_add:\n",
    "        if entry[\"start_time\"] == \"None\":\n",
    "            entry[\"start_time\"] = None\n",
    "        if entry[\"end_time\"] == \"None\":\n",
    "            entry[\"end_time\"] = None\n",
    "        if entry[\"duration\"] == \"None\":\n",
    "            entry[\"duration\"] = None\n",
    "    dbconnect.engine.execute(conditionInfo.__table__.insert(), cond_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make condition match table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconnect = dbnascentConnection(db_url, creds)\n",
    "\n",
    "dbcond_add_keys = list(dict(config[\"condition keys\"]).keys())\n",
    "dbcond_add_keys.append(\"condition_id\")\n",
    "\n",
    "dbcond_dump = dbconnect.reflect_table(\"conditionInfo\")\n",
    "dbcond = Metatable(meta_path=None, dictlist=dbcond_dump)\n",
    "dbcond_data = dbcond.key_grab(dbcond_add_keys)\n",
    "\n",
    "dbsamp_dump = dbconnect.reflect_table(\"sampleID\")\n",
    "dbsamp = Metatable(meta_path=None, dictlist=dbsamp_dump)\n",
    "name_id = dbsamp.unique([\"sample_name\", \"sample_id\"])\n",
    "\n",
    "for entry in cond_parsed:\n",
    "    for eq in name_id:\n",
    "        if entry[\"sample_name\"] == eq[\"sample_name\"]:\n",
    "            entry[\"sample_id\"] = eq[\"sample_id\"]\n",
    "#    dbutils.key_store_compare(entry, dbcond_data,\n",
    "#                              dbcond_full_keys, \"condition_id\")\n",
    "    key_store_compare(entry, dbcond_data,\n",
    "                      dbcond_full_keys, [\"condition_id\"])\n",
    "\n",
    "exptcond = Metatable(meta_path=None, dictlist=cond_parsed)\n",
    "exptcond_unique = exptcond.unique([\"sample_id\", \"condition_id\"])\n",
    "\n",
    "exptcond_to_add = entry_update(\"exptCondition\",\n",
    "                               [\"sample_id\", \"condition_id\"],\n",
    "                               exptcond_unique)\n",
    "\n",
    "if len(exptcond_to_add) > 0:\n",
    "    dbconnect.engine.execute(exptCondition.insert(), exptcond_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sampleAccum table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "IntegrityError",
     "evalue": "(pymysql.err.IntegrityError) (1062, \"Duplicate entry '13' for key 'PRIMARY'\")\n[SQL: INSERT INTO `sampleAccum` (sample_id, replicate, single_paired, rcomp, expt_unusable, timecourse, baseline_control_expt, notes, raw_read_depth, trim_read_depth, raw_read_length, duplication_picard, single_map, multi_map, map_prop, rseqc_tags, rseqc_cds, rseqc_five_utr, rseqc_three_utr, rseqc_intron, cds_rpk, intron_rpk, exint_ratio, distinct_tenmillion_prop, genome_prop_cov, avg_fold_cov, samp_qc_score, samp_data_score) VALUES (%(sample_id)s, %(replicate)s, %(single_paired)s, %(rcomp)s, %(expt_unusable)s, %(timecourse)s, %(baseline_control_expt)s, %(notes)s, %(raw_read_depth)s, %(trim_read_depth)s, %(raw_read_length)s, %(duplication_picard)s, %(single_map)s, %(multi_map)s, %(map_prop)s, %(rseqc_tags)s, %(rseqc_cds)s, %(rseqc_five_utr)s, %(rseqc_three_utr)s, %(rseqc_intron)s, %(cds_rpk)s, %(intron_rpk)s, %(exint_ratio)s, %(distinct_tenmillion_prop)s, %(genome_prop_cov)s, %(avg_fold_cov)s, %(samp_qc_score)s, %(samp_data_score)s)]\n[parameters: ({'sample_id': '13', 'replicate': '1', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'control', 'notes': '', 'raw_read_depth': '56094956', 'trim_read_depth': '36189411', 'raw_read_length': '75', 'duplication_picard': '0.39838', 'single_map': '26645914', 'multi_map': '2995616', 'map_prop': '0.8191', 'rseqc_tags': '25639716', 'rseqc_cds': '1653249', 'rseqc_five_utr': '2577663', 'rseqc_three_utr': '1589094', 'rseqc_intron': '15608076', 'cds_rpk': '42.94', 'intron_rpk': '11.61', 'exint_ratio': '3.69854', 'distinct_tenmillion_prop': '0.69112', 'genome_prop_cov': '0.11552', 'avg_fold_cov': '0.5171', 'samp_qc_score': '1', 'samp_data_score': '3'}, {'sample_id': '14', 'replicate': '2', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'control', 'notes': '', 'raw_read_depth': '44129264', 'trim_read_depth': '18573270', 'raw_read_length': '75', 'duplication_picard': '0.32614', 'single_map': '13446032', 'multi_map': '1540694', 'map_prop': '0.8069', 'rseqc_tags': '12987278', 'rseqc_cds': '687701', 'rseqc_five_utr': '1054746', 'rseqc_three_utr': '823928', 'rseqc_intron': '8328891', 'cds_rpk': '17.86', 'intron_rpk': '6.2', 'exint_ratio': '2.88065', 'distinct_tenmillion_prop': '0.69283', 'genome_prop_cov': '0.07505', 'avg_fold_cov': '0.22618', 'samp_qc_score': '2', 'samp_data_score': '4'}, {'sample_id': '15', 'replicate': '1', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '84710660', 'trim_read_depth': '41087789', 'raw_read_length': '75', 'duplication_picard': '0.43535', 'single_map': '29047411', 'multi_map': '3393104', 'map_prop': '0.7895', 'rseqc_tags': '28050940', 'rseqc_cds': '1797694', 'rseqc_five_utr': '2492047', 'rseqc_three_utr': '1759399', 'rseqc_intron': '17424953', 'cds_rpk': '46.69', 'intron_rpk': '12.96', 'exint_ratio': '3.60262', 'distinct_tenmillion_prop': '0.68799', 'genome_prop_cov': '0.11505', 'avg_fold_cov': '0.50455', 'samp_qc_score': '1', 'samp_data_score': '3'}, {'sample_id': '16', 'replicate': '2', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '59525596', 'trim_read_depth': '39172480', 'raw_read_length': '75', 'duplication_picard': '0.38753', 'single_map': '29129825', 'multi_map': '3122930', 'map_prop': '0.8234', 'rseqc_tags': '27916474', 'rseqc_cds': '1717499', 'rseqc_five_utr': '2541045', 'rseqc_three_utr': '1731260', 'rseqc_intron': '17312138', 'cds_rpk': '44.61', 'intron_rpk': '12.88', 'exint_ratio': '3.46351', 'distinct_tenmillion_prop': '0.71163', 'genome_prop_cov': '0.12578', 'avg_fold_cov': '0.565', 'samp_qc_score': '1', 'samp_data_score': '2'}, {'sample_id': '17', 'replicate': '1', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '60523507', 'trim_read_depth': '39951956', 'raw_read_length': '75', 'duplication_picard': '0.39059', 'single_map': '29195629', 'multi_map': '3405099', 'map_prop': '0.816', 'rseqc_tags': '28295177', 'rseqc_cds': '1792066', 'rseqc_five_utr': '2849165', 'rseqc_three_utr': '1723644', 'rseqc_intron': '17329426', 'cds_rpk': '46.55', 'intron_rpk': '12.89', 'exint_ratio': '3.61133', 'distinct_tenmillion_prop': '0.71284', 'genome_prop_cov': '0.12519', 'avg_fold_cov': '0.58644', 'samp_qc_score': '1', 'samp_data_score': '2'}, {'sample_id': '18', 'replicate': '2', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '52641132', 'trim_read_depth': '35630012', 'raw_read_length': '75', 'duplication_picard': '0.37848', 'single_map': '26569716', 'multi_map': '2999461', 'map_prop': '0.8299', 'rseqc_tags': '25721784', 'rseqc_cds': '1644545', 'rseqc_five_utr': '2529541', 'rseqc_three_utr': '1584259', 'rseqc_intron': '15796934', 'cds_rpk': '42.72', 'intron_rpk': '11.75', 'exint_ratio': '3.63574', 'distinct_tenmillion_prop': '0.71142', 'genome_prop_cov': '0.12037', 'avg_fold_cov': '0.55256', 'samp_qc_score': '1', 'samp_data_score': '2'}, {'sample_id': '19', 'replicate': '1', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '59529059', 'trim_read_depth': '40781058', 'raw_read_length': '75', 'duplication_picard': '0.40753', 'single_map': '30122417', 'multi_map': '3261109', 'map_prop': '0.8186', 'rseqc_tags': '29030861', 'rseqc_cds': '1855487', 'rseqc_five_utr': '2788554', 'rseqc_three_utr': '1821217', 'rseqc_intron': '17886108', 'cds_rpk': '48.2', 'intron_rpk': '13.31', 'exint_ratio': '3.62134', 'distinct_tenmillion_prop': '0.70478', 'genome_prop_cov': '0.1235', 'avg_fold_cov': '0.57963', 'samp_qc_score': '1', 'samp_data_score': '2'}, {'sample_id': '20', 'replicate': '2', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '63176118', 'trim_read_depth': '31904393', 'raw_read_length': '75', 'duplication_picard': '0.43755', 'single_map': '21704085', 'multi_map': '2818342', 'map_prop': '0.7686', 'rseqc_tags': '21272991', 'rseqc_cds': '1466412', 'rseqc_five_utr': '2005314', 'rseqc_three_utr': '1346742', 'rseqc_intron': '13049900', 'cds_rpk': '38.09', 'intron_rpk': '9.71', 'exint_ratio': '3.92276', 'distinct_tenmillion_prop': '0.67936', 'genome_prop_cov': '0.09965', 'avg_fold_cov': '0.39567', 'samp_qc_score': '1', 'samp_data_score': '3'})]\n(Background on this error at: http://sqlalche.me/e/13/gkpj)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1256\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m                     self.dialect.do_executemany(\n\u001b[0m\u001b[1;32m   1258\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\u001b[0m in \u001b[0;36mdo_executemany\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_executemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mrowcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36mexecutemany\u001b[0;34m(self, query, args)\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"(\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\")\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             return self._do_execute_many(\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0mq_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36m_do_execute_many\u001b[0;34m(self, prefix, values, postfix, args, max_stmt_length, encoding)\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0msql\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mrows\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpostfix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, query, args)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, sql, unbuffered)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMMAND\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOM_QUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_affected_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_query_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munbuffered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munbuffered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_affected_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_query_result\u001b[0;34m(self, unbuffered)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMySQLResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m             \u001b[0mfirst_packet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_packet\u001b[0;34m(self, packet_type)\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbuffered_active\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0mpacket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpacket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/protocol.py\u001b[0m in \u001b[0;36mraise_for_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errno =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_mysql_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/err.py\u001b[0m in \u001b[0;36mraise_mysql_exception\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0merrorclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInternalError\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0merrno\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrorclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIntegrityError\u001b[0m: (1062, \"Duplicate entry '13' for key 'PRIMARY'\")",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mIntegrityError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-918452277d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'None'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mdbconnect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampleAccum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__table__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccum_to_add\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, statement, *multiparams, **params)\u001b[0m\n\u001b[1;32m   2236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2237\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_contextual_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclose_with_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2238\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, object_, *multiparams, **params)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             )\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/sql/elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[0;34m(self, connection, multiparams, params)\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_execute_on_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupports_execution\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_clauseelement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[0;34m(self, elem, multiparams, params)\u001b[0m\n\u001b[1;32m   1125\u001b[0m             )\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m         ret = self._execute_context(\n\u001b[0m\u001b[1;32m   1128\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_ctx_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_compiled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m             self._handle_dbapi_exception(\n\u001b[0m\u001b[1;32m   1318\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[0;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[1;32m   1509\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewraise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m                 util.raise_(\n\u001b[0m\u001b[1;32m   1512\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/util/compat.py\u001b[0m in \u001b[0;36mraise_\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# credit to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/engine/base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[0;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m                             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m                     self.dialect.do_executemany(\n\u001b[0m\u001b[1;32m   1258\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m                     )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sqlalchemy/dialects/mysql/mysqldb.py\u001b[0m in \u001b[0;36mdo_executemany\u001b[0;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_executemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mrowcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rowcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrowcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36mexecutemany\u001b[0;34m(self, query, args)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mq_postfix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"(\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\")\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             return self._do_execute_many(\n\u001b[0m\u001b[1;32m    174\u001b[0m                 \u001b[0mq_prefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 \u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36m_do_execute_many\u001b[0;34m(self, prefix, values, postfix, args, max_stmt_length, encoding)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0msql\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mb\",\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0msql\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mrows\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msql\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpostfix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, query, args)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmogrify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/cursors.py\u001b[0m in \u001b[0;36m_query\u001b[0;34m(self, q)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_executed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clear_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrowcount\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mquery\u001b[0;34m(self, sql, unbuffered)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0msql\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"surrogateescape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execute_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCOMMAND\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOM_QUERY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_affected_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_query_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munbuffered\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munbuffered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_affected_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_query_result\u001b[0;34m(self, unbuffered)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMySQLResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_status\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m             \u001b[0mfirst_packet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfirst_packet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_ok_packet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/connections.py\u001b[0m in \u001b[0;36m_read_packet\u001b[0;34m(self, packet_type)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbuffered_active\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munbuffered_active\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0mpacket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpacket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/protocol.py\u001b[0m in \u001b[0;36mraise_for_error\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"errno =\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_mysql_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pymysql/err.py\u001b[0m in \u001b[0;36mraise_mysql_exception\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merrorclass\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0merrorclass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInternalError\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0merrno\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0merrorclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIntegrityError\u001b[0m: (pymysql.err.IntegrityError) (1062, \"Duplicate entry '13' for key 'PRIMARY'\")\n[SQL: INSERT INTO `sampleAccum` (sample_id, replicate, single_paired, rcomp, expt_unusable, timecourse, baseline_control_expt, notes, raw_read_depth, trim_read_depth, raw_read_length, duplication_picard, single_map, multi_map, map_prop, rseqc_tags, rseqc_cds, rseqc_five_utr, rseqc_three_utr, rseqc_intron, cds_rpk, intron_rpk, exint_ratio, distinct_tenmillion_prop, genome_prop_cov, avg_fold_cov, samp_qc_score, samp_data_score) VALUES (%(sample_id)s, %(replicate)s, %(single_paired)s, %(rcomp)s, %(expt_unusable)s, %(timecourse)s, %(baseline_control_expt)s, %(notes)s, %(raw_read_depth)s, %(trim_read_depth)s, %(raw_read_length)s, %(duplication_picard)s, %(single_map)s, %(multi_map)s, %(map_prop)s, %(rseqc_tags)s, %(rseqc_cds)s, %(rseqc_five_utr)s, %(rseqc_three_utr)s, %(rseqc_intron)s, %(cds_rpk)s, %(intron_rpk)s, %(exint_ratio)s, %(distinct_tenmillion_prop)s, %(genome_prop_cov)s, %(avg_fold_cov)s, %(samp_qc_score)s, %(samp_data_score)s)]\n[parameters: ({'sample_id': '13', 'replicate': '1', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'control', 'notes': '', 'raw_read_depth': '56094956', 'trim_read_depth': '36189411', 'raw_read_length': '75', 'duplication_picard': '0.39838', 'single_map': '26645914', 'multi_map': '2995616', 'map_prop': '0.8191', 'rseqc_tags': '25639716', 'rseqc_cds': '1653249', 'rseqc_five_utr': '2577663', 'rseqc_three_utr': '1589094', 'rseqc_intron': '15608076', 'cds_rpk': '42.94', 'intron_rpk': '11.61', 'exint_ratio': '3.69854', 'distinct_tenmillion_prop': '0.69112', 'genome_prop_cov': '0.11552', 'avg_fold_cov': '0.5171', 'samp_qc_score': '1', 'samp_data_score': '3'}, {'sample_id': '14', 'replicate': '2', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'control', 'notes': '', 'raw_read_depth': '44129264', 'trim_read_depth': '18573270', 'raw_read_length': '75', 'duplication_picard': '0.32614', 'single_map': '13446032', 'multi_map': '1540694', 'map_prop': '0.8069', 'rseqc_tags': '12987278', 'rseqc_cds': '687701', 'rseqc_five_utr': '1054746', 'rseqc_three_utr': '823928', 'rseqc_intron': '8328891', 'cds_rpk': '17.86', 'intron_rpk': '6.2', 'exint_ratio': '2.88065', 'distinct_tenmillion_prop': '0.69283', 'genome_prop_cov': '0.07505', 'avg_fold_cov': '0.22618', 'samp_qc_score': '2', 'samp_data_score': '4'}, {'sample_id': '15', 'replicate': '1', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '84710660', 'trim_read_depth': '41087789', 'raw_read_length': '75', 'duplication_picard': '0.43535', 'single_map': '29047411', 'multi_map': '3393104', 'map_prop': '0.7895', 'rseqc_tags': '28050940', 'rseqc_cds': '1797694', 'rseqc_five_utr': '2492047', 'rseqc_three_utr': '1759399', 'rseqc_intron': '17424953', 'cds_rpk': '46.69', 'intron_rpk': '12.96', 'exint_ratio': '3.60262', 'distinct_tenmillion_prop': '0.68799', 'genome_prop_cov': '0.11505', 'avg_fold_cov': '0.50455', 'samp_qc_score': '1', 'samp_data_score': '3'}, {'sample_id': '16', 'replicate': '2', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '59525596', 'trim_read_depth': '39172480', 'raw_read_length': '75', 'duplication_picard': '0.38753', 'single_map': '29129825', 'multi_map': '3122930', 'map_prop': '0.8234', 'rseqc_tags': '27916474', 'rseqc_cds': '1717499', 'rseqc_five_utr': '2541045', 'rseqc_three_utr': '1731260', 'rseqc_intron': '17312138', 'cds_rpk': '44.61', 'intron_rpk': '12.88', 'exint_ratio': '3.46351', 'distinct_tenmillion_prop': '0.71163', 'genome_prop_cov': '0.12578', 'avg_fold_cov': '0.565', 'samp_qc_score': '1', 'samp_data_score': '2'}, {'sample_id': '17', 'replicate': '1', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '60523507', 'trim_read_depth': '39951956', 'raw_read_length': '75', 'duplication_picard': '0.39059', 'single_map': '29195629', 'multi_map': '3405099', 'map_prop': '0.816', 'rseqc_tags': '28295177', 'rseqc_cds': '1792066', 'rseqc_five_utr': '2849165', 'rseqc_three_utr': '1723644', 'rseqc_intron': '17329426', 'cds_rpk': '46.55', 'intron_rpk': '12.89', 'exint_ratio': '3.61133', 'distinct_tenmillion_prop': '0.71284', 'genome_prop_cov': '0.12519', 'avg_fold_cov': '0.58644', 'samp_qc_score': '1', 'samp_data_score': '2'}, {'sample_id': '18', 'replicate': '2', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '52641132', 'trim_read_depth': '35630012', 'raw_read_length': '75', 'duplication_picard': '0.37848', 'single_map': '26569716', 'multi_map': '2999461', 'map_prop': '0.8299', 'rseqc_tags': '25721784', 'rseqc_cds': '1644545', 'rseqc_five_utr': '2529541', 'rseqc_three_utr': '1584259', 'rseqc_intron': '15796934', 'cds_rpk': '42.72', 'intron_rpk': '11.75', 'exint_ratio': '3.63574', 'distinct_tenmillion_prop': '0.71142', 'genome_prop_cov': '0.12037', 'avg_fold_cov': '0.55256', 'samp_qc_score': '1', 'samp_data_score': '2'}, {'sample_id': '19', 'replicate': '1', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '59529059', 'trim_read_depth': '40781058', 'raw_read_length': '75', 'duplication_picard': '0.40753', 'single_map': '30122417', 'multi_map': '3261109', 'map_prop': '0.8186', 'rseqc_tags': '29030861', 'rseqc_cds': '1855487', 'rseqc_five_utr': '2788554', 'rseqc_three_utr': '1821217', 'rseqc_intron': '17886108', 'cds_rpk': '48.2', 'intron_rpk': '13.31', 'exint_ratio': '3.62134', 'distinct_tenmillion_prop': '0.70478', 'genome_prop_cov': '0.1235', 'avg_fold_cov': '0.57963', 'samp_qc_score': '1', 'samp_data_score': '2'}, {'sample_id': '20', 'replicate': '2', 'single_paired': 'single', 'rcomp': 1, 'expt_unusable': 0, 'timecourse': 1, 'baseline_control_expt': 'experimental', 'notes': '', 'raw_read_depth': '63176118', 'trim_read_depth': '31904393', 'raw_read_length': '75', 'duplication_picard': '0.43755', 'single_map': '21704085', 'multi_map': '2818342', 'map_prop': '0.7686', 'rseqc_tags': '21272991', 'rseqc_cds': '1466412', 'rseqc_five_utr': '2005314', 'rseqc_three_utr': '1346742', 'rseqc_intron': '13049900', 'cds_rpk': '38.09', 'intron_rpk': '9.71', 'exint_ratio': '3.92276', 'distinct_tenmillion_prop': '0.67936', 'genome_prop_cov': '0.09965', 'avg_fold_cov': '0.39567', 'samp_qc_score': '1', 'samp_data_score': '3'})]\n(Background on this error at: http://sqlalche.me/e/13/gkpj)"
     ]
    }
   ],
   "source": [
    "dbconnect = dbnascentConnection(db_url, creds)\n",
    "\n",
    "# Load data location and keys\n",
    "data_path = config[\"file_locations\"][\"data\"]\n",
    "\n",
    "accum_keys = list(dict(config[\"metatable accum keys\"]).values())\n",
    "dbaccum_keys = list(dict(config[\"metatable accum keys\"]).keys())\n",
    "dbaccum_full_keys = list(dict(config[\"accum keys\"]).keys())\n",
    "\n",
    "samp.key_replace(accum_keys, dbaccum_keys)\n",
    "\n",
    "for entry in samp.data:\n",
    "    fastqc_dict = scrape_fastqc(entry[\"paper_id\"],\n",
    "                                entry[\"sample_name\"],\n",
    "                                data_path,\n",
    "                                entry)\n",
    "    pic_dict = scrape_picard(entry[\"paper_id\"],\n",
    "                             entry[\"sample_name\"],\n",
    "                             data_path)\n",
    "    mapstats_dict = scrape_mapstats(entry[\"paper_id\"],\n",
    "                               entry[\"sample_name\"],\n",
    "                               data_path,\n",
    "                               entry)\n",
    "    rseqc_dict = scrape_rseqc(entry[\"paper_id\"],\n",
    "                              entry[\"sample_name\"],\n",
    "                              data_path)\n",
    "    preseq_dict = scrape_preseq(entry[\"paper_id\"],\n",
    "                                entry[\"sample_name\"],\n",
    "                                data_path)\n",
    "    pileup_dict = scrape_pileup(entry[\"paper_id\"],\n",
    "                                entry[\"sample_name\"],\n",
    "                                data_path)\n",
    "    entry.update(fastqc_dict)\n",
    "    entry.update(pic_dict)\n",
    "    entry.update(mapstats_dict)\n",
    "    entry.update(rseqc_dict)\n",
    "    entry.update(preseq_dict)\n",
    "    entry.update(pileup_dict)\n",
    "    score_dict = sample_qc_calc(entry)\n",
    "    entry.update(score_dict)\n",
    "    rep_num = re.split(r'(\\d+)', entry[\"replicate\"])\n",
    "    entry[\"replicate\"] = rep_num[1]\n",
    "\n",
    "for entry in samp.data:\n",
    "    for key in entry:\n",
    "        entry[key] = str(entry[key])\n",
    "accum_unique = samp.unique(dbaccum_full_keys)\n",
    "accum_to_add = entry_update(\"sampleAccum\", dbaccum_full_keys, accum_unique)\n",
    "\n",
    "if len(accum_to_add) > 0:\n",
    "    for entry in accum_unique:\n",
    "        for key in [\"rcomp\", \"expt_unusable\", \"timecourse\"]:\n",
    "            if entry[key] == '1':\n",
    "                entry[key] = True\n",
    "            elif entry[key] == '0':\n",
    "                entry[key] = False\n",
    "        for key in entry:\n",
    "            if entry[key] == 'None':\n",
    "                entry[key] = None\n",
    "    dbconnect.engine.execute(sampleAccum.__table__.insert(), accum_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add paper qc/data scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconnect = dbnascentConnection(db_url, creds)\n",
    "paper_scores = paper_qc_calc(accum_unique)\n",
    "dbconnect.engine.execute(exptMetadata.__table__.update().\n",
    "                         where(exptMetadata.__table__.c.paper_id == paper_id),\n",
    "                         paper_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add version data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconnect = dbnascentConnection(db_url, creds)\n",
    "\n",
    "nf_keys = list(dict(config[\"nascentflow keys\"]).values())\n",
    "dbnf_keys = list(dict(config[\"nascentflow keys\"]).keys())\n",
    "bf_keys = list(dict(config[\"bidirflow keys\"]).values())\n",
    "dbbf_keys = list(dict(config[\"bidirflow keys\"]).keys())\n",
    "dirpath = config[\"file_locations\"][\"version_data\"]\n",
    "\n",
    "nftab = add_version_info(paper_id, dirpath, \"nascent\", dbnf_keys)\n",
    "for entry in nftab:\n",
    "    for key in entry:\n",
    "        entry[key] = str(entry[key])\n",
    "\n",
    "bidirtab = add_version_info(paper_id, dirpath, \"bidir\", dbbf_keys)\n",
    "for entry in bidirtab:\n",
    "    for key in entry:\n",
    "        entry[key] = str(entry[key])\n",
    "\n",
    "nf_table = Metatable(meta_path=None, dictlist=nftab)\n",
    "nf_unique = nf_table.unique(dbnf_keys)\n",
    "nf_to_add = entry_update(\"nascentflowMetadata\", dbnf_keys, nf_unique)\n",
    "\n",
    "if len(nf_to_add) > 0:\n",
    "    for entry in nf_to_add:\n",
    "        for key in dbnf_keys:\n",
    "            if entry[key] == \"None\":\n",
    "                entry[key] = None\n",
    "    dbconnect.engine.execute(nascentflowMetadata.__table__.insert(), nf_to_add)\n",
    "\n",
    "bidir_table = Metatable(meta_path=None, dictlist=bidirtab)\n",
    "bidir_unique = bidir_table.unique(dbbf_keys)\n",
    "bf_to_add = entry_update(\"bidirflowMetadata\", dbbf_keys, bidir_unique)\n",
    "\n",
    "if len(bf_to_add) > 0:\n",
    "    for entry in bf_to_add:\n",
    "        for key in dbbf_keys:\n",
    "            if entry[key] == \"None\":\n",
    "                entry[key] = None\n",
    "    dbconnect.engine.execute(bidirflowMetadata.__table__.insert(), bf_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect version data to samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconnect = dbnascentConnection(db_url, creds)\n",
    "dbnf_add_keys = list(dict(config[\"nascentflow keys\"]).keys())\n",
    "dbnf_add_keys.append(\"nascentflow_id\")\n",
    "dbbf_add_keys = list(dict(config[\"bidirflow keys\"]).keys())\n",
    "dbbf_add_keys.append(\"bidirflow_id\")\n",
    "\n",
    "dbnf_dump = dbconnect.reflect_table(\"nascentflowMetadata\")\n",
    "dbnf = Metatable(meta_path=None, dictlist=dbnf_dump)\n",
    "dbnf_data = dbnf.key_grab(dbnf_add_keys)\n",
    "\n",
    "dbbf_dump = dbconnect.reflect_table(\"bidirflowMetadata\")\n",
    "dbbf = Metatable(meta_path=None, dictlist=dbbf_dump)\n",
    "dbbf_data = dbbf.key_grab(dbbf_add_keys)\n",
    "\n",
    "for entry in nf_table.data:\n",
    "    key_store_compare(entry, dbnf_data,\n",
    "                      dbnf_keys, [\"nascentflow_id\"])\n",
    "\n",
    "for entry in bidir_table.data:\n",
    "    key_store_compare(entry, dbbf_data,\n",
    "                      dbbf_keys, [\"bidirflow_id\"])\n",
    "\n",
    "exptnf = Metatable(meta_path=None, dictlist=nf_table.data)\n",
    "exptnf_unique = exptnf.unique([\"sample_id\", \"nascentflow_id\"])\n",
    "exptnf_to_add = entry_update(\"exptNascentflow\",\n",
    "                             [\"sample_id\", \"nascentflow_id\"],\n",
    "                             exptnf_unique)\n",
    "\n",
    "if len(exptnf_to_add) > 0:\n",
    "    dbconnect.engine.execute(exptNascentflow.insert(), exptnf_to_add)\n",
    "\n",
    "exptbf = Metatable(meta_path=None, dictlist=bidir_table.data)\n",
    "exptbf_unique = exptbf.unique([\"sample_id\", \"bidirflow_id\"])\n",
    "exptbf_to_add = entry_update(\"exptBidirflow\",\n",
    "                             [\"sample_id\", \"bidirflow_id\"],\n",
    "                             exptbf_unique)\n",
    "\n",
    "if len(exptbf_to_add) > 0:\n",
    "    dbconnect.engine.execute(exptBidirflow.insert(), exptbf_to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
