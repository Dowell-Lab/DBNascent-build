{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for adding and updating DBNascent values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from . import dbutils (in script)\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load dbutils.py\n",
    "\"\"\"Functions for building and maintaining DBNascent.\n",
    "\n",
    "Filename: utils.py\n",
    "Authors: Lynn Sanford <lynn.sanford@colorado.edu> and Zach Maas\n",
    "\n",
    "Commentary:\n",
    "    This module contains utility functions and classes for\n",
    "    reducing the total amount of code needed for building and\n",
    "    updating the database\n",
    "\n",
    "Classes:\n",
    "    dbnascentConnection\n",
    "    Metatable\n",
    "\n",
    "Functions:\n",
    "    load_config(file) -> object\n",
    "    add_tables(db_url)\n",
    "    table_parse(file) -> list of dicts\n",
    "    key_grab(dict, list) -> list of lists\n",
    "    get_unique_table(file, list) -> dict\n",
    "    value_compare(object, dict, dict)\n",
    "    object_as_dict(object)\n",
    "    scrape_fastqc(object) -> list of dicts\n",
    "\n",
    "Misc variables:\n",
    "\"\"\"\n",
    "\n",
    "import configparser\n",
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.serializer import loads, dumps\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import shutil\n",
    "from statistics import median\n",
    "import yaml\n",
    "import zipfile as zp\n",
    "from . import dborm\n",
    "\n",
    "\n",
    "# Database Connection Handler\n",
    "class dbnascentConnection:\n",
    "    \"\"\"A class to handle connection to the mysql database.\n",
    "\n",
    "    Attributes:\n",
    "        engine (dialect, pool objects) : engine created by sqlalchemy\n",
    "\n",
    "        session (session object) : ORM session object created by sqlalchemy\n",
    "\n",
    "    Methods:\n",
    "        __enter__ :\n",
    "    \"\"\"\n",
    "\n",
    "    engine = None\n",
    "    _Session = None\n",
    "    session = None\n",
    "\n",
    "    def __init__(self, db_url, cred_path):\n",
    "        \"\"\"Initialize database connection.\n",
    "\n",
    "        Parameters:\n",
    "            db_url (str) : path to database (mandatory)\n",
    "\n",
    "            cred_path (str) : path to tab-delimited credentials\n",
    "                one line file with username tab password\n",
    "\n",
    "        Returns:\n",
    "            none\n",
    "        \"\"\"\n",
    "        if cred_path:\n",
    "            with open(cred_path) as f:\n",
    "                cred = next(f).split(\"\\t\")\n",
    "            self.engine = sql.create_engine(\"mysql://\" + cred[0] + \":\"\n",
    "                                            + cred[1] + db_url, echo=False)\n",
    "        elif db_url:\n",
    "            self.engine = sql.create_engine(\"mysql://\" + db_url, echo=False)\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                \"Database url must be provided\"\n",
    "            )\n",
    "        self.Session = sessionmaker(bind=self.engine)\n",
    "        self.session = self.Session()\n",
    "\n",
    "    def add_tables(self) -> None:\n",
    "        \"\"\"Add tables into database from ORM.\n",
    "\n",
    "        Does not update existing tables.\n",
    "\n",
    "        Parameters:\n",
    "            none\n",
    "\n",
    "        Returns:\n",
    "            none\n",
    "        \"\"\"\n",
    "        Base.metadata.create_all(self.engine)\n",
    "\n",
    "    def backup(self, out_path, tables):\n",
    "        \"\"\"Backup database (whole or specific tables).\n",
    "\n",
    "        Parameters:\n",
    "            out_path (str) : path to backup file directory\n",
    "\n",
    "            tables (list) : list of specific tables, if whole\n",
    "                            database backup is not desired\n",
    "\n",
    "        Returns:\n",
    "            none\n",
    "        \"\"\"\n",
    "        if not tables:\n",
    "            Base.metadata.reflect(bind=engine)\n",
    "            tables = Base.metadata.tables.keys()\n",
    "        for table in tables:\n",
    "            outfile = out_path + \"/\" + table + \".dbdump\"\n",
    "            q = self.session.query(table)\n",
    "            serialized_data = dumps(q.all())\n",
    "            with open(outfile,'w') as out: \n",
    "                out.write(str(serialized_data))\n",
    "\n",
    "    def restore(self, in_path, tables):\n",
    "        \"\"\"Restore database (whole or specific tables).\n",
    "\n",
    "        Parameters:\n",
    "            in_path (str) : path to backup file directory\n",
    "\n",
    "            tables (list) : list of specific tables, if whole\n",
    "                            database backup is not desired\n",
    "\n",
    "        Returns:\n",
    "            none\n",
    "        \"\"\"\n",
    "        if not tables:\n",
    "            files = os.listdir(in_path)\n",
    "            tables = []\n",
    "            for file in files:\n",
    "                tables.append(file.split(\".\")[0])\n",
    "        for table in tables:\n",
    "            infile = in_path + \"/\" + table + \".dbdump\"\n",
    "            with open(infile) as f:\n",
    "                serialized_data = dict(f)\n",
    "            self.session.merge(serialized_data)\n",
    "            \n",
    "\n",
    "#    def __enter__(self):\n",
    "#        return self.session\n",
    "#\n",
    "#    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "#        self.session.commit()\n",
    "#        self.engine.dispose()\n",
    "\n",
    "\n",
    "# Metatable class definition\n",
    "class Metatable:\n",
    "    \"\"\"A class to store metadata.\n",
    "\n",
    "    Attributes:\n",
    "        data (list of dicts) :\n",
    "\n",
    "    Methods:\n",
    "        load_file :\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, meta_path):\n",
    "        \"\"\"Initialize metatable object.\n",
    "\n",
    "        Parameters:\n",
    "            meta_path (str) : path to metadata file\n",
    "                file must be tab-delimited with field names as header\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "\n",
    "        if meta_path:\n",
    "            self.load_file(meta_path)\n",
    "\n",
    "    def load_file(meta_path):\n",
    "        \"\"\"Load metatable object.\n",
    "\n",
    "        Parameters:\n",
    "            meta_path (str) : path to metadata file\n",
    "                file must be tab-delimited with field names as header\n",
    "\n",
    "        Returns:\n",
    "            self.data (list of dicts)\n",
    "        \"\"\"\n",
    "        # Check that the metadata file exists\n",
    "        if not (os.path.exists(meta_path)\n",
    "                and os.path.isfile(meta_path)):\n",
    "            raise FileNotFoundError(\n",
    "                \"Metadata file does not exist at the provided path\")\n",
    "\n",
    "        with open(meta_path, newline=\"\") as metatab:\n",
    "            full_table = csv.DictReader(metatab, delimiter=\"\\t\")\n",
    "            if len(full_table[0]) == 1:\n",
    "                raise IndexError(\n",
    "                    \"Input must be tab-delimited. Double check input.\"\n",
    "                )\n",
    "            else:\n",
    "                for entry in full_table:\n",
    "                    self.data.append(dict(entry))\n",
    "\n",
    "    def key_grab(self, key_list) -> list:\n",
    "        \"\"\"Extract values for specific keys from metatable data.\n",
    "\n",
    "        Parameters:\n",
    "            key_list (list) : desired keys from dicts in table_list\n",
    "\n",
    "        Returns:\n",
    "            value_list (list of lists) : each entry containing the values\n",
    "                                         of the given keys\n",
    "        \"\"\"\n",
    "        # Load in file as a list of dicts\n",
    "        value_list = []\n",
    "\n",
    "        # Check if keys are valid\n",
    "        for key in key_list:\n",
    "            if key not in self.data[0]:\n",
    "                raise KeyError(\n",
    "                    \"Key(s) not present in metatable object.\"\n",
    "                )\n",
    "\n",
    "        for entry in self.data:\n",
    "            value_subset = []\n",
    "            for key in key_list:\n",
    "                value_subset.append(entry[key])\n",
    "            value_list.append(value_subset)\n",
    "\n",
    "        return value_list\n",
    "\n",
    "    def unique(self, extract_keys) -> dict:\n",
    "        \"\"\"Extract values for specific keys from a metatable filepath.\n",
    "\n",
    "        Parameters:\n",
    "            extract_keys (list) : list containing desired keys in\n",
    "                                  metatable data for value extraction\n",
    "\n",
    "        Returns:\n",
    "            unique_metatable (list of dicts) : each entry contains the values\n",
    "                                               of the extract keys; only\n",
    "                                               returns unique sets of values\n",
    "        \"\"\"\n",
    "        # Check if keys are valid\n",
    "        for key in extract_keys:\n",
    "            if key not in self.data[0]:\n",
    "                raise KeyError(\n",
    "                    \"Key(s) not present in metatable object.\"\n",
    "                )\n",
    "\n",
    "        full_table_list = np.array(self.key_grab(extract_keys))\n",
    "        unique_list = np.unique(full_table_list, axis=0)\n",
    "\n",
    "        unique_metatable = []\n",
    "        for entry in unique_list:\n",
    "            new_dict = dict(zip(extract_keys, entry))\n",
    "            unique_metatable.append(new_dict)\n",
    "\n",
    "        return unique_metatable\n",
    "\n",
    "\n",
    "# Configuration File Reader\n",
    "def load_config(filename: str):\n",
    "    \"\"\"Load database config file compatible with configparser package.\n",
    "\n",
    "    Parameters:\n",
    "        filename (str) : path to config file\n",
    "\n",
    "    Returns:\n",
    "        config (configparser object) : parsed config file\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(\n",
    "            \"Configuration file does not exist at the provided path\"\n",
    "        )\n",
    "    config = configparser.ConfigParser()\n",
    "    with open(filename) as confFile:\n",
    "        config.read_string(confFile.read())\n",
    "    return config\n",
    "\n",
    "\n",
    "def value_compare(db_row, metatable_row, key_dict) -> bool:\n",
    "    \"\"\"Compare values between two dicts.\n",
    "\n",
    "    Parameters:\n",
    "        db_row (dict) : dict extracted from one entry in\n",
    "                        one table of the database\n",
    "\n",
    "        metatable_row (dict) : dict extracted from a metadata table\n",
    "\n",
    "        key_dict (dict) : specific keys for comparison\n",
    "\n",
    "    Returns:\n",
    "        {0,1} (boolean) : whether the value in the database matches the\n",
    "                          metadata value; 1 if matching, 0 if not\n",
    "    \"\"\"\n",
    "    for key in key_dict:\n",
    "        if db_row[key] == metatable_row[key_dict[key]]:\n",
    "            continue\n",
    "        else:\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def object_as_dict(obj):\n",
    "    \"\"\"Convert queried database entry into dict.\n",
    "\n",
    "    Parameters:\n",
    "        obj (str) : single row (entry) of a database query output\n",
    "\n",
    "    Returns:\n",
    "        db_dict (dict) : key-value pairs from database entry\n",
    "    \"\"\"\n",
    "    db_dict = {c.key: getattr(obj, c.key) for c\n",
    "               in sql.inspect(obj).mapper.column_attrs}\n",
    "    return db_dict\n",
    "\n",
    "\n",
    "def add_meta_columns(db_row, metatab, comp_keys, fields):\n",
    "    \"\"\"Extract db columns to add to metatable.\n",
    "    \n",
    "    Parameters:\n",
    "        db_row (dict) : single row (entry) of a database query output\n",
    "\n",
    "        metatab (Metatable object) : table to which to add fields\n",
    "\n",
    "        comp_keys (dict) : specific keys for comparison as derived\n",
    "                           from config file (db cols as keys and\n",
    "                           metadata equivalent field names as values)\n",
    "\n",
    "        fields (list) : list of fields to add from db to table\n",
    "\n",
    "    Returns:\n",
    "        metatab (Metatable object) : input table with added field\n",
    "    \"\"\"\n",
    "    db_row_dict = object_as_dict(db_row)\n",
    "    for entry in metatab:\n",
    "        if value_compare(db_row_dict, entry, comp_keys):\n",
    "            for field in fields:\n",
    "                entry[field] = db_row_dict[field]\n",
    "\n",
    "    return metatab\n",
    "\n",
    "\n",
    "def scrape_fastqc(paper_id, sample_name, data_path, db_sample, dbconfig):\n",
    "    \"\"\"Scrape read length and depth from fastQC report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "        db_sample (dict) : sample_accum entry dict from db query\n",
    "\n",
    "        dbconfig (configparser object) : config data\n",
    "\n",
    "    Returns:\n",
    "        fastqc_dict (dict) : scraped fastqc metadata in dict format\n",
    "    \"\"\"\n",
    "    fastqc_dict = {}\n",
    "    \n",
    "    # Determine paths for raw fastQC file to scrape, depending on SE/PE\n",
    "    fqc_path = data_path + paper_id + \"/qc/fastqc/zips/\"\n",
    "    if db_sample[dbconfig[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "        samp_zip = dirpath + sample + \"_1_fastqc\"\n",
    "    else:\n",
    "        samp_zip = dirpath + sample + \"_fastqc\"\n",
    "\n",
    "    # If fastQC files don't exist, return null values\n",
    "    if not (os.path.exists(samp_zip + \".zip\")):\n",
    "        fastqc_dict[\"raw_read_depth\"] = None\n",
    "        fastqc_dict[\"raw_read_length\"] = None\n",
    "        fastqc_dict[\"trim_read_depth\"] = None\n",
    "        return fastqc_dict\n",
    "\n",
    "    # Unzip fastQC report\n",
    "    with zp.ZipFile(samp_zip + \".zip\", \"r\") as zp_ref:\n",
    "        zp_ref.extractall(dirpath)\n",
    "\n",
    "    # Extract raw depth and read length\n",
    "    fdata = open(samp_zip + \"/fastqc_data.txt\")\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Sequences\").search(line):\n",
    "            fastqc_dict[\"raw_read_depth\"] = int(line.split()[2])\n",
    "        if re.compile(\"Sequence length\").search(line):\n",
    "            fastqc_dict[\"raw_read_length\"] = int(line.split()[2].split(\"-\")[0])\n",
    "\n",
    "    # Remove unzipped file\n",
    "    shutil.rmtree(samp_zip)\n",
    "\n",
    "    # Determine paths for trimmed fastQC file to scrape, depending on SE/PE\n",
    "    # and whether reverse complemented or not\n",
    "    if db_sample[dbconfig[\"accum keys\"][\"rcomp\"]] == 1:\n",
    "        if db_sample[dbconfig[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "            samp_zip = dirpath + sample + \"_1.flip.trim_fastqc\"\n",
    "        else:\n",
    "            samp_zip = dirpath + sample + \".flip.trim_fastqc\"\n",
    "    else:\n",
    "        if db_sample[dbconfig[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "            samp_zip = dirpath + sample + \"_1.trim_fastqc\"\n",
    "        else:\n",
    "            samp_zip = dirpath + sample + \".trim_fastqc\"\n",
    "\n",
    "    # If trimmed fastQC report doesn't exist, return null value for \n",
    "    # trimmed read depth\n",
    "    if not (os.path.exists(samp_zip + \".zip\")):\n",
    "        fastqc_dict[\"trim_read_depth\"] = None\n",
    "        return fastqc_dict\n",
    "\n",
    "    # Unzip trimmed fastQC report\n",
    "    with zp.ZipFile(samp_zip + \".zip\", \"r\") as zp_ref:\n",
    "        zp_ref.extractall(dirpath)\n",
    "\n",
    "    # Extract trimmed read depth\n",
    "    fdata = open(samp_zip + \"/fastqc_data.txt\")\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Sequences\").search(line):\n",
    "            fastqc_dict[\"trim_read_depth\"] = int(line.split()[2])\n",
    "\n",
    "    # Remove unzipped file\n",
    "    shutil.rmtree(samp_zip)\n",
    "    \n",
    "    return fastqc_dict\n",
    "\n",
    "\n",
    "def scrape_picard(paper_id, sample_name, data_path):\n",
    "    \"\"\"Scrape read length and depth from picard duplication report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "    Returns:\n",
    "        picard_dict (dict) : scraped picard metadata in dict format\n",
    "    \"\"\"\n",
    "    picard_dict = {}\n",
    "    \n",
    "    dirpath = data_path + paper_id + \"/qc/picard/dups/\"\n",
    "    filepath = dirpath + sample_name + \".marked_dup_metrics.txt\"\n",
    "\n",
    "    # If picardtools data doesn't exist, return null value\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        picard_dict[\"duplication_picard\"] = None\n",
    "        return picard_dict\n",
    "\n",
    "    # Extract duplication data\n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Unknown Library\").search(line):\n",
    "            picard_dict[\"duplication_picard\"] = float(line.split(\"\\t\")[8])\n",
    "\n",
    "\n",
    "def scrape_mapstats(paper_id, sample_name, data_path, db_sample, dbconfig):\n",
    "    \"\"\"Scrape read length and depth from hisat2 mapstats report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "        db_sample (dict) : sample_accum entry dict from db query\n",
    "\n",
    "        dbconfig (configparser object) : config data\n",
    "\n",
    "    Returns:\n",
    "        mapstats_dict (dict) : scraped hisat2 metadata in dict format\n",
    "    \"\"\"\n",
    "    mapstats_dict = {}\n",
    "\n",
    "    dirpath = data_path + paper_id + \"/qc/hisat2_mapstats/\"\n",
    "    filepath = dirpath + sample_name + \".hisat2_mapstats.txt\"\n",
    "\n",
    "    # If hisat mapping data doesn't exist, return null values\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        mapstats_dict[\"single_map\"] = None\n",
    "        mapstats_dict[\"multi_map\"] = None\n",
    "        mapstats_dict[\"map_prop\"] = None\n",
    "        return mapstats_dict\n",
    "\n",
    "    fdata = open(filepath)\n",
    "    \n",
    "    # Sum up and report mapped reads for paired end data\n",
    "    if db_sample[dbconfig[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "        for line in fdata:\n",
    "            if re.compile(\"concordantly 1 time\").search(line):\n",
    "                reads = int(line.split(\": \")[1].split(\" (\")[0]) * 2\n",
    "            if re.compile(\"Aligned 1 time\").search(line):\n",
    "                mapstats_dict[\"single_map\"] = reads + int(\n",
    "                    line.split(\": \")[1].split(\" (\")[0]\n",
    "                )\n",
    "            if re.compile(\"concordantly >1 times\").search(line):\n",
    "                reads = int(line.split(\": \")[1].split(\" (\")[0]) * 2\n",
    "            if re.compile(\"Aligned >1 times\").search(line):\n",
    "                mapstats_dict[\"multi_map\"] = reads + int(\n",
    "                    line.split(\": \")[1].split(\" (\")[0]\n",
    "                )\n",
    "            if re.compile(\"Overall alignment rate\").search(line):\n",
    "                mapstats_dict[\"map_prop\"] = (\n",
    "                    float(line.split(\": \")[1].split(\"%\")[0]) / 100\n",
    "                )\n",
    "    # Report mapped reads for single end data\n",
    "    else:\n",
    "        for line in fdata:\n",
    "            if re.compile(\"Aligned 1 time\").search(line):\n",
    "                mapstats_dict[\"single_map\"] = int(line.split(\": \")[1].split(\" (\")[0])\n",
    "            if re.compile(\"Aligned >1 times\").search(line):\n",
    "                mapstats_dict[\"multi_map\"] = int(line.split(\": \")[1].split(\" (\")[0])\n",
    "            if re.compile(\"Overall alignment rate\").search(line):\n",
    "                mapstats_dict[\"map_prop\"] = (\n",
    "                    float(line.split(\": \")[1].split(\"%\")[0]) / 100\n",
    "                )\n",
    "\n",
    "\n",
    "def scrape_rseqc(paper_id, sample_name, data_path):\n",
    "    \"\"\"Scrape read length and depth from RSeQC report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "    Returns:\n",
    "        rseqc_dict (dict) : scraped RSeQC metadata in dict format\n",
    "    \"\"\"\n",
    "    rseqc_dict = {}\n",
    "\n",
    "    dirpath = data_path + paper_id + \"/qc/rseqc/read_distribution/\"\n",
    "    filepath = dirpath + sample_name + \".read_distribution.txt\"\n",
    "\n",
    "    # If rseqc read distribution data doesn't exist, return null values\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        rseqc_dict[\"rseqc_tags\"] = None\n",
    "        rseqc_dict[\"rseqc_cds\"] = None\n",
    "        rseqc_dict[\"cds_rpk\"] = None\n",
    "        rseqc_dict[\"rseqc_five_utr\"] = None\n",
    "        rseqc_dict[\"rseqc_three_utr\"] = None\n",
    "        rseqc_dict[\"rseqc_intron\"] = None\n",
    "        rseqc_dict[\"intron_rpk\"] = None\n",
    "        rseqc_dict[\"exint_ratio\"] = None\n",
    "        return rseqc_dict\n",
    "\n",
    "    # Extract RSeQC data\n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Assigned Tags\").search(line):\n",
    "            rseqc_dict[\"rseqc_tags\"] = int(line.split()[-1])\n",
    "        if re.compile(\"CDS_Exons\").search(line):\n",
    "            rseqc_dict[\"rseqc_cds\"] = int(line.split()[2])\n",
    "            rseqc_dict[\"cds_rpk\"] = float(line.split()[-1])\n",
    "        if re.compile(\"5'UTR_Exons\").search(line):\n",
    "            rseqc_dict[\"rseqc_five_utr\"] = int(line.split()[2])\n",
    "        if re.compile(\"3'UTR_Exons\").search(line):\n",
    "            rseqc_dict[\"rseqc_three_utr\"] = int(line.split()[2])\n",
    "        if re.compile(\"Introns\").search(line):\n",
    "            rseqc_dict[\"rseqc_intron\"] = int(line.split()[2])\n",
    "            rseqc_dict[\"intron_rpk\"] = float(line.split()[-1])\n",
    "\n",
    "    if rseqc_dict[\"intron_rpk\"] > 0:\n",
    "        rseqc_dict[\"exint_ratio\"] = (\n",
    "            rseqc_dict[\"cds_rpk\"] / rseqc_dict[\"intron_rpk\"]\n",
    "        )\n",
    "    else:\n",
    "        rseqc_dict[\"exint_ratio\"] = None\n",
    "\n",
    "    return rseqc_dict\n",
    "\n",
    "\n",
    "def scrape_preseq(paper_id, sample_name, data_path):\n",
    "    \"\"\"Scrape read length and depth from preseq complexity report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "    Returns:\n",
    "        preseq_dict (dict) : scraped preseq metadata in dict format\n",
    "    \"\"\"\n",
    "    preseq_dict = {}\n",
    "\n",
    "    dirpath = data_path + paper_id + \"/qc/preseq/\"\n",
    "    filepath = dirpath + sample_name + \".lc_extrap.txt\"\n",
    "\n",
    "    # If preseq complexity data doesn't exist, return null value\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        preseq_dict[\"distinct_tenmillion_prop\"] = None\n",
    "        return preseq_dict\n",
    "\n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if line.startswith(\"10000000.0\"):\n",
    "            distinct = float(line.split()[1])\n",
    "\n",
    "    preseq_dict[\"distinct_tenmillion_prop\"] = distinct / 10000000\n",
    "\n",
    "    return preseq_dict\n",
    "\n",
    "\n",
    "def scrape_pileup(paper_id, sample_name, data_path):\n",
    "    \"\"\"Scrape read length and depth from pileup report.\n",
    "\n",
    "    Parameters:\n",
    "        paper_id (str) : paper identifier\n",
    "\n",
    "        sample_name (str) : sample name derived from db query\n",
    "\n",
    "        data_path (str) : path to database storage directory\n",
    "\n",
    "    Returns:\n",
    "        pileup_dict (dict) : scraped pileup metadata in dict format\n",
    "    \"\"\"\n",
    "    pileup_dict = {}\n",
    "\n",
    "    dirpath = data_path + paper_id + \"/qc/pileup/\"\n",
    "    filepath = dirpath + sample_name + \".coverage.stats.txt\"\n",
    "\n",
    "    # If pileup complexity data doesn't exist, return null value\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        pileup_dict[\"genome_prop_cov\"] = None\n",
    "        pileup_dict[\"avg_fold_cov\"] = None\n",
    "        return pileup_dict\n",
    "\n",
    "    # Add up reads in different categories to calculate coverage\n",
    "    fdata = open(filepath)\n",
    "    x = 0\n",
    "    total = cov = fold = 0\n",
    "    for line in fdata:\n",
    "        if x == 0:\n",
    "            x = x + 1\n",
    "            continue\n",
    "        else:\n",
    "            x = x + 1\n",
    "            total = total + int(line.split(\"\\t\")[2])\n",
    "            cov = cov + int(line.split(\"\\t\")[5])\n",
    "            fold = fold + (float(line.split(\"\\t\")[1]) \n",
    "                           * int(line.split(\"\\t\")[2]))\n",
    "\n",
    "    pileup_dict[\"genome_prop_cov\"] = cov / total\n",
    "    pileup_dict[\"avg_fold_cov\"] = fold / total\n",
    "\n",
    "    return pileup_dict\n",
    "\n",
    "\n",
    "def sample_qc_calc(db_sample):\n",
    "    \"\"\"Calculate sample qc and data scores.\n",
    "\n",
    "    Parameters:\n",
    "        db_sample (dict) : sample_accum entry dict from db query\n",
    "\n",
    "    Returns:\n",
    "        samp_score (int) : calculated sample scores in dict format\n",
    "    \"\"\"\n",
    "    trimrd = db_sample[\"trim_read_depth\"]\n",
    "    dup = db_sample[\"duplication_picard\"]\n",
    "    mapped = db_sample[\"map_prop\"]\n",
    "    complexity = db_sample[\"distinct_tenmillion_prop\"]\n",
    "    genome = db_sample[\"genome_prop_cov\"]\n",
    "    exint = db_sample[\"exint_ratio\"]\n",
    "\n",
    "    # Determine sample QC score\n",
    "    if (trimrd == None \n",
    "        or dup == None\n",
    "        or mapped == None \n",
    "        or complexity == None):\n",
    "\n",
    "        samp_score[\"samp_qc_score\"] = 0\n",
    "\n",
    "    elif (trimrd <= 5000000\n",
    "          or dup >= 0.95\n",
    "          or (mapped * trimrd) <= 4000000\n",
    "          or complexity < 0.05):\n",
    "\n",
    "        samp_score[\"samp_qc_score\"] = 5\n",
    "\n",
    "    elif (trimrd <= 10000000\n",
    "          or dup >= 0.80\n",
    "          or (mapped * trimrd) <= 8000000\n",
    "          or complexity < 0.2):\n",
    "\n",
    "        samp_score[\"samp_qc_score\"] = 4\n",
    "\n",
    "    elif (trimrd <= 15000000\n",
    "          or dup >= 0.65\n",
    "          or (mapped * trimrd) <= 12000000\n",
    "          or complexity < 0.35):\n",
    "\n",
    "        samp_score[\"samp_qc_score\"] = 3\n",
    "\n",
    "    elif (trimrd <= 20000000\n",
    "          or dup >= 0.5\n",
    "          or (mapped * trimrd) <= 16000000\n",
    "          or complexity < 0.5):\n",
    "\n",
    "        samp_score[\"samp_qc_score\"] = 2\n",
    "\n",
    "    else:\n",
    "        samp_score[\"samp_qc_score\"] = 1\n",
    "\n",
    "    # Determine sample data score\n",
    "    if (genome == None\n",
    "        or exint == None):\n",
    "\n",
    "        samp_score[\"samp_data_score\"] = 0\n",
    "\n",
    "    elif (genome <= 0.04\n",
    "          or exint >= 9):\n",
    "\n",
    "        samp_score[\"samp_data_score\"] = 5\n",
    "\n",
    "    elif (genome <= 0.08\n",
    "          or exint >= 7):\n",
    "\n",
    "        samp_score[\"samp_data_score\"] = 4\n",
    "\n",
    "    elif (genome <= 0.12\n",
    "          or exint >= 5):\n",
    "\n",
    "        samp_score[\"samp_data_score\"] = 3\n",
    "\n",
    "    elif (genome <= 0.16\n",
    "          or exint >= 3):\n",
    "        samp_score[\"samp_data_score\"] = 2\n",
    "\n",
    "    else:\n",
    "        samp_score[\"samp_data_score\"] = 1\n",
    "\n",
    "    return samp_score\n",
    "\n",
    "\n",
    "def paper_qc_calc(db_samples):\n",
    "    \"\"\"Calculate sample qc and data scores.\n",
    "\n",
    "    Parameters:\n",
    "        db_samples (list of dicts) : sample_accum entries from db query\n",
    "\n",
    "    Returns:\n",
    "        paper_scores (float) : calculated median scores in dict format\n",
    "    \"\"\"\n",
    "    qc_scores = []\n",
    "    data_scores = []\n",
    "    paper_scores = {}\n",
    "    \n",
    "    for entry in db_samples:\n",
    "        qc_scores.append(entry[\"samp_qc_score\"])\n",
    "        data_scores.append(entry[\"samp_data_score\"])\n",
    "\n",
    "    paper_scores[\"paper_qc_score\"] = median(qc_scores)\n",
    "    paper_scores[\"paper_data_score\"] = median(data_scores)\n",
    "\n",
    "    return paper_scores\n",
    "\n",
    "\n",
    "def dbnascent_backup(db, basedir, tables):\n",
    "    \"\"\"Create new database backup.\n",
    "\n",
    "    Parameters:\n",
    "        db (dbnascentConnection object) : current database connection\n",
    "\n",
    "        basedir (str) : path to base backup directory\n",
    "                        default /home/lsanford/Documents/data/dbnascent_backups\n",
    "\n",
    "        tables (list) : list of specific tables if whole db backup\n",
    "                        is not desired\n",
    "\n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    if not basedir:\n",
    "        basedir = \"/home/lsanford/Documents/data/dbnascent_backups\"\n",
    "    now = datetime.datetime.now()\n",
    "    nowdir = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    os.makedirs(basedir + \"/\" + nowdir)\n",
    "\n",
    "    if tables:\n",
    "        db.backup((basedir + \"/\" + nowdir), tables)\n",
    "    else:\n",
    "        db.backup((basedir + \"/\" + nowdir))\n",
    "\n",
    "\n",
    "def paper_add_update(db, config, identifier, basedir):\n",
    "    \"\"\"Add or update paper and associated sample metadata.\n",
    "\n",
    "    Parameters:\n",
    "        db (dbnascentConnection object) : current database connection\n",
    "\n",
    "        config (configParser object) : parsed config file\n",
    "\n",
    "        identifier (str) : paper identifier, used to locate all (meta)data\n",
    "\n",
    "        basedir (str) : path to base database data directory\n",
    "                        default /Shares/dbnascent\n",
    "\n",
    "    Returns:\n",
    "        none\n",
    "    \"\"\"\n",
    "    # Add experimental metadata\n",
    "    expt_keys = list(dict(config[\"expt keys\"]).values())\n",
    "    if not basedir:\n",
    "        basedir = \"/Shares/dbnascent\"\n",
    "    exptmeta_path = basedir + \"/\" + identifier + \"/\"\n",
    "\n",
    "    # Read in expt metadata and make sure entries are unique\n",
    "    exptmeta = utils.Metatable(exptmeta_path + \"metadata/expt_metadata.txt\")\n",
    "    expt_unique = exptmeta.unique(expt_keys)\n",
    "    \n",
    "    # Add expt metadata to database\n",
    "    db.engine.execute(exptMetadata.__table__.insert(), expt_unique.data())\n",
    "\n",
    "    # Add sample ids\n",
    "    \n",
    "\n",
    "#engine.execute(tablename.__table__.insert(),listofdicts)\n",
    "#\n",
    "# utils.py ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sqlalchemy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-58861c0641bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Code:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeclarative\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeclarative_base\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sqlalchemy'"
     ]
    }
   ],
   "source": [
    "# %load dborm.py\n",
    "# orm.py --- ORM for DBNascent\n",
    "#\n",
    "# Filename: orm.py\n",
    "# Description: ORM for DBNascent\n",
    "# Authors: Lynn Sanford <lynn.sanford@colorado.edu> and Zach Maas\n",
    "# Created: Mon Jun 10 13:11:55 2019 (-0600)\n",
    "# URL:\n",
    "#\n",
    "\n",
    "# Commentary:\n",
    "#\n",
    "# This file contains code for an ORM to interface with the Dowell\n",
    "# Lab's Nascent Database.\n",
    "#\n",
    "\n",
    "# Code:\n",
    "\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "# Base class for our ORM\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "# MAIN TABLES\n",
    "class organismInfo(Base):\n",
    "    __tablename__ = \"organismInfo\"\n",
    "#    metadata = MetaData()\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    genome_build = sql.Column(sql.String(length=50))\n",
    "    genome_bases = sql.Column(sql.Integer)\n",
    "\n",
    "\n",
    "class searchEq(Base):\n",
    "    __tablename__ = \"searchEq\"\n",
    "#    metadata = MetaData()\n",
    "    search_term = sql.Column(\n",
    "        sql.String(length=250), primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    db_term = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "class exptMetadata(Base):\n",
    "    __tablename__ = \"exptMetadata\"\n",
    "#    metadata = MetaData()\n",
    "    expt_id = sql.Column(sql.Integer,\n",
    "                         primary_key=True,\n",
    "                         index=True,\n",
    "                         unique=True)\n",
    "    srp = sql.Column(sql.String(length=50))\n",
    "    protocol = sql.Column(sql.String(length=50))\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), sql.ForeignKey(\"organismInfo.organism\")\n",
    "    )\n",
    "    library = sql.Column(sql.String(length=50))\n",
    "    spikein = sql.Column(sql.String(length=127))\n",
    "    paper_id = sql.Column(sql.String(length=127))\n",
    "    published = sql.Column(sql.Boolean)\n",
    "    year = sql.Column(sql.Integer)\n",
    "    first_author = sql.Column(sql.String(length=127))\n",
    "    last_author = sql.Column(sql.String(length=127))\n",
    "    doi = sql.Column(sql.String(length=300))\n",
    "    curator1 = sql.Column(sql.String(length=50))\n",
    "    curator2 = sql.Column(sql.String(length=50))\n",
    "    other_sr_data = sql.Column(sql.Boolean)\n",
    "    atac_seq = sql.Column(sql.Boolean)\n",
    "    rna_seq = sql.Column(sql.Boolean)\n",
    "    chip_seq = sql.Column(sql.Boolean)\n",
    "    three_dim_seq = sql.Column(sql.Boolean)\n",
    "    other_seq = sql.Column(sql.Boolean)\n",
    "    paper_qc_score = sql.Column(sql.Float)\n",
    "    paper_data_score = sql.Column(sql.Float)\n",
    "\n",
    "\n",
    "class sampleID(Base):\n",
    "    __tablename__ = \"sampleID\"\n",
    "#    metadata = MetaData()\n",
    "    srr = sql.Column(sql.String(length=50),\n",
    "                     primary_key=True,\n",
    "                     index=True,\n",
    "                     unique=True)\n",
    "    sample_name = sql.Column(sql.String(length=50))\n",
    "    sample_id = sql.Column(sql.Integer)\n",
    "\n",
    "\n",
    "class geneticInfo(Base):\n",
    "    __tablename__ = \"geneticInfo\"\n",
    "#    metadata = MetaData()\n",
    "    genetic_id = sql.Column(sql.Integer,\n",
    "                            primary_key=True,\n",
    "                            index=True,\n",
    "                            unique=True)\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), sql.ForeignKey(\"organismInfo.organism\")\n",
    "    )\n",
    "    sample_type = sql.Column(sql.String(length=127))\n",
    "    cell_type = sql.Column(sql.String(length=127))\n",
    "    clone_individual = sql.Column(sql.String(length=127))\n",
    "    strain = sql.Column(sql.String(length=127))\n",
    "    genotype = sql.Column(sql.String(length=127))\n",
    "    construct = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "class conditionInfo(Base):\n",
    "    __tablename__ = \"conditionInfo\"\n",
    "#    metadata = MetaData()\n",
    "    condition_id = sql.Column(sql.Integer,\n",
    "                              primary_key=True,\n",
    "                              index=True,\n",
    "                              unique=True)\n",
    "    condition_type = sql.Column(sql.String(length=127))\n",
    "    treatment = sql.Column(sql.String(length=127))\n",
    "    conc_intens = sql.Column(sql.String(length=50))\n",
    "    start_time = sql.Column(sql.Integer)\n",
    "    end_time = sql.Column(sql.Integer)\n",
    "    duration = sql.Column(sql.Integer)\n",
    "    time_unit = sql.Column(sql.String(length=50))\n",
    "    duration_unit = sql.Column(sql.String(length=50))\n",
    "\n",
    "\n",
    "exptCondition = sql.Table(\n",
    "    \"exptCondition\",\n",
    "    Base.metadata,\n",
    "    sql.Column(\"sample_id\",\n",
    "               sql.Integer,\n",
    "               sql.ForeignKey(\"sampleID.sample_id\")),\n",
    "    sql.Column(\"condition_id\",\n",
    "               sql.Integer,\n",
    "               sql.ForeignKey(\"conditionInfo.condition_id\")),\n",
    ")\n",
    "\n",
    "\n",
    "class linkIDs(Base):\n",
    "    __tablename__ = \"linkIDs\"\n",
    "#    metadata = MetaData()\n",
    "    sample_id = sql.Column(\n",
    "        sql.Integer,\n",
    "        sql.ForeignKey(\"sampleID.sample_id\"),\n",
    "        primary_key=True,\n",
    "        index=True,\n",
    "        unique=True,\n",
    "    )\n",
    "    genetic_id = sql.Column(sql.Integer,\n",
    "                            sql.ForeignKey(\"geneticInfo.genetic_id\"))\n",
    "    expt_id = sql.Column(sql.Integer,\n",
    "                         sql.ForeignKey(\"exptMetadata.expt_id\"))\n",
    "    sample_name = sql.Column(sql.String(length=127),\n",
    "                         sql.ForeignKey(\"sampleID.sample_name\"))\n",
    "    paper_id = sql.Column(sql.String(length=127),\n",
    "                         sql.ForeignKey(\"exptMetadata.paper_id\"))\n",
    "\n",
    "\n",
    "class sampleAccum(Base):\n",
    "    __tablename__ = \"sampleAccum\"\n",
    "#    metadata = MetaData()\n",
    "    sample_id = sql.Column(\n",
    "        sql.Integer,\n",
    "        sql.ForeignKey(\"sampleID.sample_id\"),\n",
    "        primary_key=True,\n",
    "        index=True,\n",
    "        unique=True,\n",
    "    )\n",
    "    replicate = sql.Column(sql.Integer)\n",
    "    single_paired = sql.Column(sql.String(length=50))\n",
    "    rcomp = sql.Column(sql.Boolean)\n",
    "    expt_unusable = sql.Column(sql.Boolean)\n",
    "    timecourse = sql.Column(sql.Boolean)\n",
    "    baseline_control_expt = sql.Column(sql.String(length=50))\n",
    "    notes = sql.Column(sql.String(length=300))\n",
    "    raw_read_depth = sql.Column(sql.Integer)\n",
    "    trim_read_depth = sql.Column(sql.Integer)\n",
    "    raw_read_length = sql.Column(sql.Integer)\n",
    "    duplication_picard = sql.Column(sql.Float)\n",
    "    single_map = sql.Column(sql.Integer)\n",
    "    multi_map = sql.Column(sql.Integer)\n",
    "    map_prop = sql.Column(sql.Float)\n",
    "    rseqc_tags = sql.Column(sql.Integer)\n",
    "    rseqc_cds = sql.Column(sql.Integer)\n",
    "    rseqc_five_utr = sql.Column(sql.Integer)\n",
    "    rseqc_three_utr = sql.Column(sql.Integer)\n",
    "    rseqc_intron = sql.Column(sql.Integer)\n",
    "    cds_rpk = sql.Column(sql.Float)\n",
    "    intron_rpk = sql.Column(sql.Float)\n",
    "    exint_ratio = sql.Column(sql.Float)\n",
    "    distinct_tenmillion_prop = sql.Column(sql.Float)\n",
    "    genome_prop_cov = sql.Column(sql.Float)\n",
    "    avg_fold_cov = sql.Column(sql.Float)\n",
    "    samp_qc_score = sql.Column(sql.Integer)\n",
    "    samp_data_score = sql.Column(sql.Integer)\n",
    "\n",
    "\n",
    "class nascentflowMetadata(Base):\n",
    "    __tablename__ = \"nascentflowMetadata\"\n",
    "#    metadata = MetaData()\n",
    "    nascentflow_version_id = sql.Column(\n",
    "        sql.Integer, primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    sample_id = sql.Column(sql.Integer, sql.ForeignKey(\"sampleID.sample_id\"))\n",
    "    nascentflow_version = sql.Column(sql.String(length=127))\n",
    "    pipeline_revision_hash = sql.Column(sql.String(length=127))\n",
    "    pipeline_hash = sql.Column(sql.String(length=127))\n",
    "    nascentflow_date = sql.Column(sql.Date)\n",
    "    nascentflow_redo_date = sql.Column(sql.Date)\n",
    "    nextflow_version = sql.Column(sql.String(length=127))\n",
    "    fastqc_version = sql.Column(sql.String(length=127))\n",
    "    bbmap_version = sql.Column(sql.String(length=127))\n",
    "    hisat2_version = sql.Column(sql.String(length=127))\n",
    "    samtools_version = sql.Column(sql.String(length=127))\n",
    "    sratools_version = sql.Column(sql.String(length=127))\n",
    "    preseq_version = sql.Column(sql.String(length=127))\n",
    "    preseq_date = sql.Column(sql.Date)\n",
    "    rseqc_version = sql.Column(sql.String(length=127))\n",
    "    rseqc_date = sql.Column(sql.Date)\n",
    "    java_version = sql.Column(sql.String(length=127))\n",
    "    picard_gc_version = sql.Column(sql.String(length=127))\n",
    "    picard_dups_version = sql.Column(sql.String(length=127))\n",
    "    picard_date = sql.Column(sql.Date)\n",
    "    bedtools_version = sql.Column(sql.String(length=127))\n",
    "    igvtools_version = sql.Column(sql.String(length=127))\n",
    "    seqkit_version = sql.Column(sql.String(length=127))\n",
    "    mpich_version = sql.Column(sql.String(length=127))\n",
    "    gcc_version = sql.Column(sql.String(length=127))\n",
    "    python_version = sql.Column(sql.String(length=127))\n",
    "    numpy_version = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "class bidirflowMetadata(Base):\n",
    "    __tablename__ = \"bidirflowMetadata\"\n",
    "#    metadata = MetaData()\n",
    "    bidirflow_version_id = sql.Column(\n",
    "        sql.Integer, primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    sample_id = sql.Column(sql.Integer, sql.ForeignKey(\"sampleID.sample_id\"))\n",
    "    bidirflow_version = sql.Column(sql.String(length=127))\n",
    "    pipeline_revision_hash = sql.Column(sql.String(length=127))\n",
    "    pipeline_hash = sql.Column(sql.String(length=127))\n",
    "    bidirflow_date = sql.Column(sql.Date)\n",
    "    nextflow_version = sql.Column(sql.String(length=127))\n",
    "    samtools_version = sql.Column(sql.String(length=127))\n",
    "    bedtools_version = sql.Column(sql.String(length=127))\n",
    "    mpich_version = sql.Column(sql.String(length=127))\n",
    "    openmpi_version = sql.Column(sql.String(length=127))\n",
    "    gcc_version = sql.Column(sql.String(length=127))\n",
    "    r_version = sql.Column(sql.String(length=127))\n",
    "    rsubread_version = sql.Column(sql.String(length=127))\n",
    "    boost_version = sql.Column(sql.String(length=127))\n",
    "    fstitch_version = sql.Column(sql.String(length=127))\n",
    "    tfit_version = sql.Column(sql.String(length=127))\n",
    "    dreg_version = sql.Column(sql.String(length=127))\n",
    "    dreg_date = sql.Column(sql.Date)\n",
    "    tfit_date = sql.Column(sql.Date)\n",
    "    fcgene_date = sql.Column(sql.Date)\n",
    "\n",
    "\n",
    "# The following were created by Zach and we may or may not use...\n",
    "\n",
    "# class tf(Base):\n",
    "#    __tablename__ = \"tf\"\n",
    "#    tf_id = sql.Column(sql.String(length=127), primary_key=True)\n",
    "#    tf_alias = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "# class pipeline_status(Base):\n",
    "#    __tablename__ = \"pipeline_status\"\n",
    "#    srr_id = sql.Column(\n",
    "#        sql.String(length=127),\n",
    "#        sql.ForeignKey(\"srr_metadata.srr_id\"),\n",
    "#        primary_key=True,\n",
    "#    )\n",
    "#    fastqc_complete = sql.Column(sql.Boolean)\n",
    "#    bbduk_complete = sql.Column(sql.Boolean)\n",
    "#    hisat2_complete = sql.Column(sql.Boolean)\n",
    "#    samtools_complete = sql.Column(sql.Boolean)\n",
    "#    fastq_dump_complete = sql.Column(sql.Boolean)\n",
    "#    pileup_complete = sql.Column(sql.String(length=127))\n",
    "#    preseq_complete = sql.Column(sql.Boolean)\n",
    "#    rseqc_complete = sql.Column(sql.String(length=127))\n",
    "#    bedtools_complete = sql.Column(sql.Boolean)\n",
    "#    igv_tools_complete = sql.Column(sql.Boolean)\n",
    "#    fstitch_complete = sql.Column(sql.Boolean)\n",
    "#    tfit_complete = sql.Column(sql.Boolean)\n",
    "\n",
    "\n",
    "# class md_score(Base):\n",
    "#    __tablename__ = \"md_score\"\n",
    "#    srr_id = sql.Column(\n",
    "#        sql.String(length=127),\n",
    "#        sql.ForeignKey(\"srr_metadata.srr_id\"),\n",
    "#        primary_key=True,\n",
    "#    )\n",
    "#    tf_id = sql.Column(sql.String, sql.ForeignKey(\"tf.tf_id\"))\n",
    "#    erna_type = sql.Column(sql.String(length=127))\n",
    "#    md_score_expected = sql.Column(sql.Integer)\n",
    "#    md_score_std = sql.Column(sql.Integer)\n",
    "\n",
    "\n",
    "# orm.py ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = dbutils.load_config(\"/home/lsanford/Documents/data/repositories/dbnascent_build/config.txt\")\n",
    "config = load_config(\n",
    "    \"/home/lsanford/Documents/data/repositories/DBNascent-build/config.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define database location and back up database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = config[\"file_locations\"][\"database\"]\n",
    "creds = config[\"file_locations\"][\"credentials\"]\n",
    "\n",
    "dbconnect = utils.dbnascentConnection(db_url, creds)\n",
    "utils.dbnascent_backup(dbconnect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create database connection object and database schema\n",
    "#### This creates tables that do not already exist\n",
    "#### Does not update tables that do exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbconnect.add_tables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add/update organism table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "organism_keys = list(dict(config[\"organism keys\"]).values())\n",
    "orgtable_path = config[\"file_locations\"][\"organism_table\"]\n",
    "\n",
    "# Read in organism table and make sure entries are unique\n",
    "orgs = utils.Metatable(orgtable_path)\n",
    "orgs_unique = orgs.unique(organism_keys)\n",
    "\n",
    "# Add data to database\n",
    "dbconnect.engine.execute(organismInfo.__table__.insert(), orgs_unique.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add/update search equivalencies table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keys = list(dict(config[\"searcheq keys\"]).values())\n",
    "searchtable_path = config[\"file_locations\"][\"searcheq_table\"]\n",
    "\n",
    "# Read in search equivalencies table and make sure entries are unique\n",
    "eqs = utils.Metatable(searchtable_path)\n",
    "eqs_unique = eqs.unique(search_keys)\n",
    "\n",
    "# Add data to database\n",
    "dbconnect.engine.execute(searchEq.__table__.insert(), eqs_unique.data())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate the paper add/update over all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
