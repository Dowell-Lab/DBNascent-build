{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dbnascent_initial_build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a script for initially building dbnascent from metadata\n",
    "### See dbnascent_update for updating existing database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile as zp\n",
    "from statistics import median\n",
    "\n",
    "import numpy as np\n",
    "import sqlalchemy as sql\n",
    "import yaml\n",
    "\n",
    "# from . import utils #(in script)\n",
    "# from . import orm #(in script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import utilities (in notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "# utils.py --- Utilities for simplifying database code\n",
    "#\n",
    "# Filename: utils.py\n",
    "# Description: Miscellaneous utilities for simplifying database code\n",
    "# Author: Zachary Maas <zama8258@colorado.edu> and Lynn Sanford\n",
    "# Maintainer: Lynn Sanford <lynn.sanford@colorado.edu>\n",
    "# Created: Mon Jul  1 16:04:05 2019 (-0600)\n",
    "#\n",
    "\n",
    "# Commentary:\n",
    "#\n",
    "# This module contains a few helpful utility functions and classes for\n",
    "# reducing the total amount of code needed for the database, since\n",
    "# there are many areas where the same patterns keep popping up.\n",
    "#\n",
    "\n",
    "# Code:\n",
    "\n",
    "import configparser\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "# Database Connection Handler\n",
    "class NascentDBConnection:\n",
    "    engine = None\n",
    "    _Session = None\n",
    "    session = None\n",
    "\n",
    "    def __init__(self, db_url):\n",
    "        self.engine = sql.create_engine(db_url, echo=False)\n",
    "        self.Session = sessionmaker(bind=self.engine)\n",
    "        self.session = self.Session()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.session\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.session.commit()\n",
    "        self.engine.dispose()\n",
    "\n",
    "\n",
    "# Configuration File Reader\n",
    "def load_config(filename: str):\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(\n",
    "            \"Configuration file does not exist at the provided path\"\n",
    "        )\n",
    "    config = configparser.ConfigParser()\n",
    "    with open(filename) as confFile:\n",
    "        config.read_string(confFile.read())\n",
    "    return config\n",
    "\n",
    "\n",
    "# Add/update (?) tables in database\n",
    "# (I'm not actually sure this updates if already existing)\n",
    "\n",
    "\n",
    "def update_tables(db_url: str) -> None:\n",
    "    engine = sql.create_engine(\"sqlite:///\" + db_url, echo=False)\n",
    "    Base.metadata.create_all(engine, checkfirst=True)\n",
    "\n",
    "\n",
    "# Function for parsing table into list of dicts\n",
    "def table_parse(table_filepath: str) -> list:\n",
    "    \"\"\"Parse table into list of dicts.\n",
    "\n",
    "    Takes the manually curated metadata table as input and\n",
    "    turns it into a list of dicts, one entry for each srr with\n",
    "    key: value pairs for each column in the metadata table\n",
    "    Output: List of dicts\n",
    "    \"\"\"\n",
    "    # Check that the table file exists\n",
    "    if not (os.path.exists(table_filepath) and os.path.isfile(table_filepath)):\n",
    "        raise FileNotFoundError(f\"{table_filepath} does not exist.\")\n",
    "\n",
    "    # Load in file as a list of dicts\n",
    "    table_list = []\n",
    "    with open(table_filepath, newline=\"\") as tab:\n",
    "        full_table = csv.DictReader(tab, delimiter=\"\\t\")\n",
    "        for entry in full_table:\n",
    "            table_list.append(dict(entry))\n",
    "\n",
    "    return table_list\n",
    "\n",
    "\n",
    "# Function for grabbing specific keys\n",
    "def key_grab(table_list, key_list) -> list:\n",
    "    \"\"\"Grab specific key values.\n",
    "\n",
    "    Takes list of dicts and a list of keys and\n",
    "    extracts specific values to a list for inputting into database\n",
    "    Output: List of values corresponding to input keys for each\n",
    "    table entry\n",
    "    \"\"\"\n",
    "    # Load in file as a list of dicts\n",
    "    value_list = []\n",
    "    for entry in table_list:\n",
    "        value_subset = []\n",
    "        for i in range(len(key_list)):\n",
    "            value_subset.append(entry[key_list[i]])\n",
    "        value_list.append(value_subset)\n",
    "\n",
    "    return value_list\n",
    "\n",
    "\n",
    "def get_unique_table(location_key, column_keys) -> dict:\n",
    "    filepath = config[\"file_locations\"][location_key]\n",
    "    full_table_dict = table_parse(filepath)\n",
    "\n",
    "    full_table_list = np.array(key_grab(full_table_dict, column_keys))\n",
    "    unique_list = np.unique(full_table_list, axis=0)\n",
    "\n",
    "    unique_table = []\n",
    "    for i in range(len(unique_list)):\n",
    "        entry = dict(zip(column_keys, unique_list[i]))\n",
    "        unique_table.append(entry)\n",
    "\n",
    "    return unique_table\n",
    "\n",
    "\n",
    "def value_compare(db_row, metatable_row, key_dict) -> bool:\n",
    "    for key in key_dict:\n",
    "        if db_row[key] == metatable_row[key_dict[key]]:\n",
    "            continue\n",
    "        else:\n",
    "            return 0\n",
    "    return 1\n",
    "\n",
    "\n",
    "def object_as_dict(obj):\n",
    "    return {c.key: getattr(obj, c.key) for c in sql.inspect(obj).mapper.column_attrs}\n",
    "\n",
    "\n",
    "#\n",
    "# utils.py ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ORM (in notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load orm.py\n",
    "# orm.py --- ORM for DBNascent\n",
    "#\n",
    "# Filename: orm.py\n",
    "# Description: ORM for DBNascent\n",
    "# Authors: Zach Maas and Lynn Sanford\n",
    "# Maintainer: Lynn Sanford <lynn.sanford@colorado.edu>\n",
    "# Created: Mon Jun 10 13:11:55 2019 (-0600)\n",
    "# URL:\n",
    "#\n",
    "\n",
    "# Commentary:\n",
    "#\n",
    "# This file contains code for an ORM to interface with the Dowell\n",
    "# Lab's Nascent Database.\n",
    "#\n",
    "\n",
    "# Code:\n",
    "\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "# Base class for our ORM\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "# MAIN TABLES\n",
    "class organismInfo(Base):\n",
    "    __tablename__ = \"organismInfo\"\n",
    "#    metadata = MetaData()\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    genome_build = sql.Column(sql.String(length=50))\n",
    "    genome_bases = sql.Column(sql.Integer)\n",
    "\n",
    "class searchEq(Base):\n",
    "    __tablename__ = \"searchEq\"\n",
    "#    metadata = MetaData()\n",
    "    search_term = sql.Column(\n",
    "        sql.String(length=250), primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    db_term = sql.Column(sql.String(length=127))\n",
    "\n",
    "class exptMetadata(Base):\n",
    "    __tablename__ = \"exptMetadata\"\n",
    "#    metadata = MetaData()\n",
    "    expt_id = sql.Column(sql.Integer,\n",
    "                         primary_key=True,\n",
    "                         index=True,\n",
    "                         unique=True)\n",
    "    srp = sql.Column(sql.String(length=50))\n",
    "    protocol = sql.Column(sql.String(length=50))\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), sql.ForeignKey(\"organismInfo.organism\")\n",
    "    )\n",
    "    library = sql.Column(sql.String(length=50))\n",
    "    spikein = sql.Column(sql.String(length=127))\n",
    "    paper_id = sql.Column(sql.String(length=127))\n",
    "    published = sql.Column(sql.Boolean)\n",
    "    year = sql.Column(sql.Integer)\n",
    "    first_author = sql.Column(sql.String(length=127))\n",
    "    last_author = sql.Column(sql.String(length=127))\n",
    "    doi = sql.Column(sql.String(length=300))\n",
    "    curator1 = sql.Column(sql.String(length=50))\n",
    "    curator2 = sql.Column(sql.String(length=50))\n",
    "    other_sr_data = sql.Column(sql.Boolean)\n",
    "    atac_seq = sql.Column(sql.Boolean)\n",
    "    rna_seq = sql.Column(sql.Boolean)\n",
    "    chip_seq = sql.Column(sql.Boolean)\n",
    "    three_dim_seq = sql.Column(sql.Boolean)\n",
    "    other_seq = sql.Column(sql.Boolean)\n",
    "    paper_qc_score = sql.Column(sql.Float)\n",
    "    paper_data_score = sql.Column(sql.Float)\n",
    "\n",
    "\n",
    "class sampleID(Base):\n",
    "    __tablename__ = \"sampleID\"\n",
    "#    metadata = MetaData()\n",
    "    srr = sql.Column(sql.String(length=50),\n",
    "                     primary_key=True,\n",
    "                     index=True,\n",
    "                     unique=True)\n",
    "    sample_name = sql.Column(sql.String(length=50))\n",
    "    sample_id = sql.Column(sql.Integer)\n",
    "\n",
    "\n",
    "class geneticInfo(Base):\n",
    "    __tablename__ = \"geneticInfo\"\n",
    "#    metadata = MetaData()\n",
    "    genetic_id = sql.Column(sql.Integer,\n",
    "                            primary_key=True,\n",
    "                            index=True,\n",
    "                            unique=True)\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), sql.ForeignKey(\"organismInfo.organism\")\n",
    "    )\n",
    "    sample_type = sql.Column(sql.String(length=127))\n",
    "    cell_type = sql.Column(sql.String(length=127))\n",
    "    clone_individual = sql.Column(sql.String(length=127))\n",
    "    strain = sql.Column(sql.String(length=127))\n",
    "    genotype = sql.Column(sql.String(length=127))\n",
    "    construct = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "class conditionInfo(Base):\n",
    "    __tablename__ = \"conditionInfo\"\n",
    "#    metadata = MetaData()\n",
    "    condition_id = sql.Column(sql.Integer,\n",
    "                              primary_key=True,\n",
    "                              index=True,\n",
    "                              unique=True)\n",
    "    condition_type = sql.Column(sql.String(length=127))\n",
    "    treatment = sql.Column(sql.String(length=127))\n",
    "    conc_intens = sql.Column(sql.String(length=50))\n",
    "    start_time = sql.Column(sql.Integer)\n",
    "    end_time = sql.Column(sql.Integer)\n",
    "    duration = sql.Column(sql.Integer)\n",
    "    time_unit = sql.Column(sql.String(length=50))\n",
    "    duration_unit = sql.Column(sql.String(length=50))\n",
    "\n",
    "\n",
    "exptCondition = sql.Table(\n",
    "    \"exptCondition\",\n",
    "    Base.metadata,\n",
    "    sql.Column(\"sample_id\",\n",
    "               sql.Integer,\n",
    "               sql.ForeignKey(\"sampleID.sample_id\")),\n",
    "    sql.Column(\"condition_id\",\n",
    "               sql.Integer,\n",
    "               sql.ForeignKey(\"conditionInfo.condition_id\")),\n",
    ")\n",
    "\n",
    "\n",
    "class linkIDs(Base):\n",
    "    __tablename__ = \"linkIDs\"\n",
    "#    metadata = MetaData()\n",
    "    sample_id = sql.Column(\n",
    "        sql.Integer,\n",
    "        sql.ForeignKey(\"sampleID.sample_id\"),\n",
    "        primary_key=True,\n",
    "        index=True,\n",
    "        unique=True,\n",
    "    )\n",
    "    genetic_id = sql.Column(sql.Integer,\n",
    "                            sql.ForeignKey(\"geneticInfo.genetic_id\"))\n",
    "    expt_id = sql.Column(sql.Integer,\n",
    "                         sql.ForeignKey(\"exptMetadata.expt_id\"))\n",
    "\n",
    "\n",
    "class sampleAccum(Base):\n",
    "    __tablename__ = \"sampleAccum\"\n",
    "#    metadata = MetaData()\n",
    "    sample_id = sql.Column(\n",
    "        sql.Integer,\n",
    "        sql.ForeignKey(\"sampleID.sample_id\"),\n",
    "        primary_key=True,\n",
    "        index=True,\n",
    "        unique=True,\n",
    "    )\n",
    "    replicate = sql.Column(sql.Integer)\n",
    "    single_paired = sql.Column(sql.String(length=50))\n",
    "    rcomp = sql.Column(sql.Boolean)\n",
    "    expt_unusable = sql.Column(sql.Boolean)\n",
    "    timecourse = sql.Column(sql.Boolean)\n",
    "    baseline_control_expt = sql.Column(sql.String(length=50))\n",
    "    notes = sql.Column(sql.String(length=300))\n",
    "    raw_read_depth = sql.Column(sql.Integer)\n",
    "    trim_read_depth = sql.Column(sql.Integer)\n",
    "    raw_read_length = sql.Column(sql.Integer)\n",
    "    duplication_picard = sql.Column(sql.Float)\n",
    "    single_map = sql.Column(sql.Integer)\n",
    "    multi_map = sql.Column(sql.Integer)\n",
    "    map_prop = sql.Column(sql.Float)\n",
    "    rseqc_tags = sql.Column(sql.Integer)\n",
    "    rseqc_cds = sql.Column(sql.Integer)\n",
    "    rseqc_five_utr = sql.Column(sql.Integer)\n",
    "    rseqc_three_utr = sql.Column(sql.Integer)\n",
    "    rseqc_intron = sql.Column(sql.Integer)\n",
    "    cds_rpk = sql.Column(sql.Float)\n",
    "    intron_rpk = sql.Column(sql.Float)\n",
    "    exint_ratio = sql.Column(sql.Float)\n",
    "    distinct_tenmillion_prop = sql.Column(sql.Float)\n",
    "    genome_prop_cov = sql.Column(sql.Float)\n",
    "    avg_fold_cov = sql.Column(sql.Float)\n",
    "    samp_qc_score = sql.Column(sql.Integer)\n",
    "    samp_data_score = sql.Column(sql.Integer)\n",
    "\n",
    "\n",
    "class nascentflowMetadata(Base):\n",
    "    __tablename__ = \"nascentflowMetadata\"\n",
    "#    metadata = MetaData()\n",
    "    nascentflow_version_id = sql.Column(\n",
    "        sql.Integer, primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    sample_id = sql.Column(sql.Integer, sql.ForeignKey(\"sampleID.sample_id\"))\n",
    "    nascentflow_version = sql.Column(sql.String(length=127))\n",
    "    pipeline_revision_hash = sql.Column(sql.String(length=127))\n",
    "    pipeline_hash = sql.Column(sql.String(length=127))\n",
    "    nascentflow_date = sql.Column(sql.Date)\n",
    "    nascentflow_redo_date = sql.Column(sql.Date)\n",
    "    nextflow_version = sql.Column(sql.String(length=127))\n",
    "    fastqc_version = sql.Column(sql.String(length=127))\n",
    "    bbmap_version = sql.Column(sql.String(length=127))\n",
    "    hisat2_version = sql.Column(sql.String(length=127))\n",
    "    samtools_version = sql.Column(sql.String(length=127))\n",
    "    sratools_version = sql.Column(sql.String(length=127))\n",
    "    preseq_version = sql.Column(sql.String(length=127))\n",
    "    preseq_date = sql.Column(sql.Date)\n",
    "    rseqc_version = sql.Column(sql.String(length=127))\n",
    "    rseqc_date = sql.Column(sql.Date)\n",
    "    java_version = sql.Column(sql.String(length=127))\n",
    "    picard_gc_version = sql.Column(sql.String(length=127))\n",
    "    picard_dups_version = sql.Column(sql.String(length=127))\n",
    "    picard_date = sql.Column(sql.Date)\n",
    "    bedtools_version = sql.Column(sql.String(length=127))\n",
    "    igvtools_version = sql.Column(sql.String(length=127))\n",
    "    seqkit_version = sql.Column(sql.String(length=127))\n",
    "    mpich_version = sql.Column(sql.String(length=127))\n",
    "    gcc_version = sql.Column(sql.String(length=127))\n",
    "    python_version = sql.Column(sql.String(length=127))\n",
    "    numpy_version = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "class bidirflowMetadata(Base):\n",
    "    __tablename__ = \"bidirflowMetadata\"\n",
    "#    metadata = MetaData()\n",
    "    bidirflow_version_id = sql.Column(\n",
    "        sql.Integer, primary_key=True, index=True, unique=True\n",
    "    )\n",
    "    sample_id = sql.Column(sql.Integer, sql.ForeignKey(\"sampleID.sample_id\"))\n",
    "    bidirflow_version = sql.Column(sql.String(length=127))\n",
    "    pipeline_revision_hash = sql.Column(sql.String(length=127))\n",
    "    pipeline_hash = sql.Column(sql.String(length=127))\n",
    "    bidirflow_date = sql.Column(sql.Date)\n",
    "    nextflow_version = sql.Column(sql.String(length=127))\n",
    "    samtools_version = sql.Column(sql.String(length=127))\n",
    "    bedtools_version = sql.Column(sql.String(length=127))\n",
    "    mpich_version = sql.Column(sql.String(length=127))\n",
    "    openmpi_version = sql.Column(sql.String(length=127))\n",
    "    gcc_version = sql.Column(sql.String(length=127))\n",
    "    r_version = sql.Column(sql.String(length=127))\n",
    "    rsubread_version = sql.Column(sql.String(length=127))\n",
    "    boost_version = sql.Column(sql.String(length=127))\n",
    "    fstitch_version = sql.Column(sql.String(length=127))\n",
    "    tfit_version = sql.Column(sql.String(length=127))\n",
    "    dreg_version = sql.Column(sql.String(length=127))\n",
    "    dreg_date = sql.Column(sql.Date)\n",
    "    tfit_date = sql.Column(sql.Date)\n",
    "    fcgene_date = sql.Column(sql.Date)\n",
    "\n",
    "\n",
    "# The following were created by Zach and we may or may not use...\n",
    "\n",
    "# class tf(Base):\n",
    "#    __tablename__ = \"tf\"\n",
    "#    tf_id = sql.Column(sql.String(length=127), primary_key=True)\n",
    "#    tf_alias = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "# class pipeline_status(Base):\n",
    "#    __tablename__ = \"pipeline_status\"\n",
    "#    srr_id = sql.Column(\n",
    "#        sql.String(length=127),\n",
    "#        sql.ForeignKey(\"srr_metadata.srr_id\"),\n",
    "#        primary_key=True,\n",
    "#    )\n",
    "#    fastqc_complete = sql.Column(sql.Boolean)\n",
    "#    bbduk_complete = sql.Column(sql.Boolean)\n",
    "#    hisat2_complete = sql.Column(sql.Boolean)\n",
    "#    samtools_complete = sql.Column(sql.Boolean)\n",
    "#    fastq_dump_complete = sql.Column(sql.Boolean)\n",
    "#    pileup_complete = sql.Column(sql.String(length=127))\n",
    "#    preseq_complete = sql.Column(sql.Boolean)\n",
    "#    rseqc_complete = sql.Column(sql.String(length=127))\n",
    "#    bedtools_complete = sql.Column(sql.Boolean)\n",
    "#    igv_tools_complete = sql.Column(sql.Boolean)\n",
    "#    fstitch_complete = sql.Column(sql.Boolean)\n",
    "#    tfit_complete = sql.Column(sql.Boolean)\n",
    "\n",
    "\n",
    "# class md_score(Base):\n",
    "#    __tablename__ = \"md_score\"\n",
    "#    srr_id = sql.Column(\n",
    "#        sql.String(length=127),\n",
    "#        sql.ForeignKey(\"srr_metadata.srr_id\"),\n",
    "#        primary_key=True,\n",
    "#    )\n",
    "#    tf_id = sql.Column(sql.String, sql.ForeignKey(\"tf.tf_id\"))\n",
    "#    erna_type = sql.Column(sql.String(length=127))\n",
    "#    md_score_expected = sql.Column(sql.Integer)\n",
    "#    md_score_std = sql.Column(sql.Integer)\n",
    "\n",
    "\n",
    "# orm.py ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = utils.load_config(\"/home/lsanford/Documents/data/repositories/dbnascent_build/config.txt\")\n",
    "config = load_config(\n",
    "    \"/home/lsanford/Documents/data/repositories/DBNascent-build/config.txt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables in database if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = config[\"file_locations\"][\"database\"]\n",
    "\n",
    "# utils.update_tables(db_url)\n",
    "update_tables(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse pre-created genome table and load into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "organism_keys = list(dict(config[\"organism keys\"]).values())\n",
    "\n",
    "# organism_table = utils.get_unique_table(\"organism_table\",organism_keys)\n",
    "organism_table = get_unique_table(\"organism_table\", organism_keys)\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(organism_table)):\n",
    "        entry = organismInfo(\n",
    "            organism=organism_table[i][config[\"organism keys\"][\"organism\"]],\n",
    "            genome_build=organism_table[i][config[\"organism keys\"][\"genome_build\"]],\n",
    "            genome_bases=organism_table[i][config[\"organism keys\"][\"genome_bases\"]],\n",
    "        )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse pre-created search equivalencies table and load into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcheq_keys = list(dict(config[\"searcheq keys\"]).values())\n",
    "\n",
    "# searcheq_table = utils.get_unique_table(\"searcheq_table\",searcheq_keys)\n",
    "searcheq_table = get_unique_table(\"searcheq_table\", searcheq_keys)\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(searcheq_table)):\n",
    "        entry = searchEq(\n",
    "            search_term=searcheq_table[i][config[\"searcheq keys\"][\"search_term\"]],\n",
    "            db_term=searcheq_table[i][config[\"searcheq keys\"][\"db_term\"]],\n",
    "        )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse metadata table and load exptMetadata values into DB, creating expt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_keys = list(dict(config[\"expt keys\"]).values())\n",
    "\n",
    "# expt_table = utils.get_unique_table(\"metadata\",expt_keys)\n",
    "expt_table = get_unique_table(\"metadata\", expt_keys)\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(expt_table)):\n",
    "        entry = exptMetadata(\n",
    "            srp=expt_table[i][config[\"expt keys\"][\"srp\"]],\n",
    "            protocol=expt_table[i][config[\"expt keys\"][\"protocol\"]],\n",
    "            organism=expt_table[i][config[\"expt keys\"][\"organism\"]],\n",
    "            library=expt_table[i][config[\"expt keys\"][\"library\"]],\n",
    "            spikein=expt_table[i][config[\"expt keys\"][\"spikein\"]],\n",
    "            paper_id=expt_table[i][config[\"expt keys\"][\"paper_id\"]],\n",
    "            published=int(expt_table[i][config[\"expt keys\"][\"published\"]]),\n",
    "            year=expt_table[i][config[\"expt keys\"][\"year\"]],\n",
    "            first_author=expt_table[i][config[\"expt keys\"][\"first_author\"]],\n",
    "            last_author=expt_table[i][config[\"expt keys\"][\"last_author\"]],\n",
    "            doi=expt_table[i][config[\"expt keys\"][\"doi\"]],\n",
    "            curator1=expt_table[i][config[\"expt keys\"][\"curator1\"]],\n",
    "            curator2=expt_table[i][config[\"expt keys\"][\"curator2\"]],\n",
    "            other_sr_data=int(expt_table[i][config[\"expt keys\"][\"other_sr_data\"]]),\n",
    "            atac_seq=int(expt_table[i][config[\"expt keys\"][\"atac_seq\"]]),\n",
    "            rna_seq=int(expt_table[i][config[\"expt keys\"][\"rna_seq\"]]),\n",
    "            chip_seq=int(expt_table[i][config[\"expt keys\"][\"chip_seq\"]]),\n",
    "            three_dim_seq=int(expt_table[i][config[\"expt keys\"][\"three_dim_seq\"]]),\n",
    "            other_seq=int(expt_table[i][config[\"expt keys\"][\"other_seq\"]]),\n",
    "        )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse metadata table and load sampleID values into DB, creating sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_keys = list(dict(config[\"sample keys\"]).values())\n",
    "\n",
    "# sample_table = utils.get_unique_table(\"metadata\",sample_keys)\n",
    "sample_table = get_unique_table(\"metadata\", sample_keys)\n",
    "\n",
    "# metatable = utils.table_parse(config[\"file_locations\"][\"metadata\"])\n",
    "# srz_list = np.array(utils.key_grab(metatable, [config[\"sample keys\"][\"sample_name\"]]))\n",
    "metatable = table_parse(config[\"file_locations\"][\"metadata\"])\n",
    "srz_list = np.array(key_grab(metatable, [config[\"sample keys\"][\"sample_name\"]]))\n",
    "srz_list = np.unique(srz_list[srz_list != \"\"])\n",
    "srz_table = dict(zip(srz_list, list(range(1, len(srz_list) + 1))))\n",
    "\n",
    "z = len(srz_table) + 1\n",
    "for i in range(len(sample_table)):\n",
    "    if sample_table[i][config[\"sample keys\"][\"sample_name\"]] == \"\":\n",
    "        sample_table[i][\"sample_id\"] = z\n",
    "        sample_table[i][config[\"sample keys\"][\"sample_name\"]] = sample_table[i][\n",
    "            config[\"sample keys\"][\"srr\"]\n",
    "        ]\n",
    "        z = z + 1\n",
    "    else:\n",
    "        sample_table[i][\"sample_id\"] = srz_table[\n",
    "            sample_table[i][config[\"sample keys\"][\"sample_name\"]]\n",
    "        ]\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(sample_table)):\n",
    "        entry = sampleID(\n",
    "            sample_id=sample_table[i][\"sample_id\"],\n",
    "            srr=sample_table[i][config[\"sample keys\"][\"srr\"]],\n",
    "            sample_name=sample_table[i][config[\"sample keys\"][\"sample_name\"]],\n",
    "        )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse metadata table and load geneticInfo values into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_keys = list(dict(config[\"genetic keys\"]).values())\n",
    "\n",
    "# genetic_table = utils.get_unique_table(\"metadata\",genetic_keys)\n",
    "genetic_table = get_unique_table(\"metadata\", genetic_keys)\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(genetic_table)):\n",
    "        genetic = geneticInfo(\n",
    "            organism=genetic_table[i][config[\"genetic keys\"][\"organism\"]],\n",
    "            sample_type=genetic_table[i][config[\"genetic keys\"][\"sample_type\"]],\n",
    "            cell_type=genetic_table[i][config[\"genetic keys\"][\"cell_type\"]],\n",
    "            clone_individual=genetic_table[i][\n",
    "                config[\"genetic keys\"][\"clone_individual\"]\n",
    "            ],\n",
    "            strain=genetic_table[i][config[\"genetic keys\"][\"strain\"]],\n",
    "            genotype=genetic_table[i][config[\"genetic keys\"][\"genotype\"]],\n",
    "            construct=genetic_table[i][config[\"genetic keys\"][\"construct\"]],\n",
    "        )\n",
    "        session.merge(genetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse metadata table and make conditions table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_condition_keys = list(dict(config[\"metatable condition keys\"]).values())\n",
    "meta_condition_keys.append(\"srr\")\n",
    "\n",
    "metatable = table_parse(config[\"file_locations\"][\"metadata\"])\n",
    "meta_condition_table = np.array(key_grab(metatable, meta_condition_keys))\n",
    "\n",
    "condition_table = [\n",
    "    [\n",
    "        \"srr\",\n",
    "        \"condition_type\",\n",
    "        \"treatment\",\n",
    "        \"conc_intens\",\n",
    "        \"start_time\",\n",
    "        \"end_time\",\n",
    "        \"time_unit\",\n",
    "    ]\n",
    "]\n",
    "for i in range(len(meta_condition_table)):\n",
    "    srr = meta_condition_table[i][3]\n",
    "    if \";\" in meta_condition_table[i][0]:\n",
    "        treatment_types = meta_condition_table[i][0].split(\";\")\n",
    "        treatments = meta_condition_table[i][1].split(\";\")\n",
    "        unparsed_times = meta_condition_table[i][2].split(\";\")\n",
    "\n",
    "        for j in range(len(treatment_types)):\n",
    "            if \"(\" in treatments[j]:\n",
    "                conc_int = treatments[j].split(\"(\")[1]\n",
    "                conc_int = conc_int.split(\")\")[0]\n",
    "                new_treatment = treatments[j].split(\"(\")[0]\n",
    "            else:\n",
    "                conc_int = \"NA or not known\"\n",
    "                new_treatment = treatments[j]\n",
    "\n",
    "            if len(unparsed_times[j]) > 0:\n",
    "                start = unparsed_times[j].split(\",\")[0]\n",
    "                end = unparsed_times[j].split(\",\")[1]\n",
    "                time_unit = unparsed_times[j].split(\",\")[2]\n",
    "            else:\n",
    "                start = \"\"\n",
    "                end = \"\"\n",
    "                time_unit = \"\"\n",
    "\n",
    "            entry = [\n",
    "                srr,\n",
    "                treatment_types[j],\n",
    "                new_treatment,\n",
    "                conc_int,\n",
    "                start,\n",
    "                end,\n",
    "                time_unit,\n",
    "            ]\n",
    "            condition_table.append(entry)\n",
    "\n",
    "    else:\n",
    "        treatment_types = meta_condition_table[i][0]\n",
    "        treatments = meta_condition_table[i][1]\n",
    "        unparsed_times = meta_condition_table[i][2]\n",
    "\n",
    "        if len(treatments) > 0:\n",
    "            if \"(\" in treatments:\n",
    "                conc_int = treatments.split(\"(\")[1]\n",
    "                conc_int = conc_int.split(\")\")[0]\n",
    "                new_treatment = treatments.split(\"(\")[0]\n",
    "            else:\n",
    "                conc_int = \"NA or not known\"\n",
    "                new_treatment = treatments\n",
    "        else:\n",
    "            conc_int = \"\"\n",
    "            new_treatment = treatments\n",
    "\n",
    "        if len(unparsed_times) > 0:\n",
    "            start = unparsed_times.split(\",\")[0]\n",
    "            end = unparsed_times.split(\",\")[1]\n",
    "            time_unit = unparsed_times.split(\",\")[2]\n",
    "        else:\n",
    "            start = \"\"\n",
    "            end = \"\"\n",
    "            time_unit = \"\"\n",
    "\n",
    "        entry = entry = [\n",
    "            srr,\n",
    "            treatment_types,\n",
    "            new_treatment,\n",
    "            conc_int,\n",
    "            start,\n",
    "            end,\n",
    "            time_unit,\n",
    "        ]\n",
    "        condition_table.append(entry)\n",
    "\n",
    "# Write out condition table\n",
    "outfile = config[\"file_locations\"][\"conditions\"]\n",
    "with open(outfile, \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    writer.writerows(condition_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load conditionInfo values into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_keys = list(dict(config[\"condition keys\"]).values())\n",
    "\n",
    "# condition_table = utils.get_unique_table(\"conditions\",condition_keys)\n",
    "condition_table = get_unique_table(\"conditions\", condition_keys)\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(condition_table)):\n",
    "        if not condition_table[i][\"end_time\"]:\n",
    "            duration = \"\"\n",
    "        else:\n",
    "            duration = int(condition_table[i][\"end_time\"]) - int(\n",
    "                condition_table[i][\"start_time\"]\n",
    "            )\n",
    "\n",
    "        if time_unit == \"s\":\n",
    "            if duration % 60 == 0:\n",
    "                if duration % 3600 == 0:\n",
    "                    if duration % 86400 == 0:\n",
    "                        duration = duration / 86400\n",
    "                        duration_unit = \"day\"\n",
    "                    else:\n",
    "                        duration = duration / 3600\n",
    "                        duration_unit = \"hr\"\n",
    "                else:\n",
    "                    duration = duration / 60\n",
    "                    duration_unit = \"min\"\n",
    "            else:\n",
    "                duration_unit = \"s\"\n",
    "        elif time_unit == \"min\":\n",
    "            if duration % 60 == 0:\n",
    "                if duration % 1440 == 0:\n",
    "                    duration = duration / 1440\n",
    "                    duration_unit = \"day\"\n",
    "                else:\n",
    "                    duration = duration / 60\n",
    "                    duration_unit = \"hr\"\n",
    "            else:\n",
    "                duration_unit = \"min\"\n",
    "        elif time_unit == \"hr\":\n",
    "            if duration % 24 == 0:\n",
    "                duration = duration / 24\n",
    "                duration_unit = \"day\"\n",
    "            else:\n",
    "                duration_unit = \"hr\"\n",
    "        else:\n",
    "            duration_unit = \"day\"\n",
    "\n",
    "        condition = conditionInfo(\n",
    "            condition_type=condition_table[i][\n",
    "                config[\"condition keys\"][\"condition_type\"]\n",
    "            ],\n",
    "            treatment=condition_table[i][config[\"condition keys\"][\"treatment\"]],\n",
    "            conc_intens=condition_table[i][config[\"condition keys\"][\"conc_intens\"]],\n",
    "            start_time=condition_table[i][config[\"condition keys\"][\"start_time\"]],\n",
    "            end_time=condition_table[i][config[\"condition keys\"][\"end_time\"]],\n",
    "            time_unit=condition_table[i][config[\"condition keys\"][\"time_unit\"]],\n",
    "            duration=duration,\n",
    "            duration_unit=duration_unit,\n",
    "        )\n",
    "        session.merge(condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create exptCondition equivalencies in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull condition info including ID from database and make a unique hash for conditions\n",
    "\n",
    "condition_id = []\n",
    "condition_details = []\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for row in session.query(conditionInfo).all():\n",
    "        condition_id.append(row.condition_id)\n",
    "        condition_details.append(\n",
    "            \"\".join(\n",
    "                [\n",
    "                    str(row.condition_type),\n",
    "                    str(row.treatment),\n",
    "                    str(row.conc_intens),\n",
    "                    str(row.start_time),\n",
    "                    str(row.end_time),\n",
    "                    str(row.time_unit),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "\n",
    "condition_dict = dict(zip(condition_details, condition_id))\n",
    "\n",
    "# Pull sample ID from database for each SRR\n",
    "\n",
    "srr = []\n",
    "sample_id = []\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for row in session.query(sampleID).all():\n",
    "        srr.append(row.srr)\n",
    "        sample_id.append(row.sample_id)\n",
    "\n",
    "sample_dict = dict(zip(srr, sample_id))\n",
    "\n",
    "# Grab condition table including SRRs and make SRR/condition hash\n",
    "\n",
    "condition_keys = list(dict(config[\"condition keys\"]).values())\n",
    "condition_keys.append(\"srr\")\n",
    "# condition_table = utils.table_parse(config[\"file_locations\"][\"conditions\"])\n",
    "# cond_str = utils.key_grab(condition_table,condition_keys)\n",
    "condition_table = table_parse(config[\"file_locations\"][\"conditions\"])\n",
    "cond_str = key_grab(condition_table, condition_keys)\n",
    "\n",
    "srr_cond = []\n",
    "for i in range(len(cond_str)):\n",
    "    srr_cond.append([cond_str[i][-1], \"\".join(cond_str[i][0:-1])])\n",
    "\n",
    "srr_cond = np.unique(np.array(srr_cond), axis=0)\n",
    "\n",
    "# Make sample ID/condition ID table\n",
    "sample_condition = []\n",
    "for i in range(len(srr_cond)):\n",
    "    sample_id = sample_dict[srr_cond[i][0]]\n",
    "    condition = condition_dict[srr_cond[i][1]]\n",
    "    sample_condition.append([sample_id, condition])\n",
    "\n",
    "sample_condition = np.unique(np.array(sample_condition), axis=0)\n",
    "\n",
    "# Add to database\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(sample_condition)):\n",
    "        statement = exptCondition.insert().values(\n",
    "            sample_id=int(sample_condition[i][0]),\n",
    "            condition_id=int(sample_condition[i][1]),\n",
    "        )\n",
    "        session.execute(statement)\n",
    "        session.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build sampleAccum table and linker table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add relevant DB keys to table in correct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metatable = utils.table_parse(config[\"file_locations\"][\"metadata\"])\n",
    "metatable = table_parse(config[\"file_locations\"][\"metadata\"])\n",
    "\n",
    "# Do some data massaging\n",
    "for i in range(len(metatable)):\n",
    "    metatable[i][\"year\"] = int(metatable[i][\"year\"])\n",
    "    metatable[i][\"replicate\"] = metatable[i][\"replicate\"][0:4]\n",
    "    if not metatable[i][config[\"sample keys\"][\"sample_name\"]]:\n",
    "        metatable[i][config[\"sample keys\"][\"sample_name\"]] = metatable[i][\n",
    "            config[\"sample keys\"][\"srr\"]\n",
    "        ]\n",
    "    for key in metatable[i]:\n",
    "        if metatable[i][key] == \"0\":\n",
    "            metatable[i][key] = False\n",
    "        elif metatable[i][key] == \"1\":\n",
    "            metatable[i][key] = True\n",
    "\n",
    "# Add sample id\n",
    "sample_keys = dict(config[\"sample keys\"])\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for row in session.query(sampleID).all():\n",
    "        db_row = object_as_dict(row)\n",
    "        for i in range(len(metatable)):\n",
    "            if value_compare(db_row, metatable[i], sample_keys):\n",
    "                metatable[i][\"sample_id\"] = row.sample_id\n",
    "                metatable[i][\"sample_name\"] = row.sample_name\n",
    "\n",
    "# Add genetic id\n",
    "genetic_keys = dict(config[\"genetic keys\"])\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for row in session.query(geneticInfo).all():\n",
    "        db_row = object_as_dict(row)\n",
    "        for i in range(len(metatable)):\n",
    "            if value_compare(db_row, metatable[i], genetic_keys):\n",
    "                metatable[i][\"genetic_id\"] = row.genetic_id\n",
    "\n",
    "# Add experimental id\n",
    "expt_keys = dict(config[\"expt keys\"])\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for row in session.query(exptMetadata).all():\n",
    "        db_row = object_as_dict(row)\n",
    "        for i in range(len(metatable)):\n",
    "            if value_compare(db_row, metatable[i], expt_keys):\n",
    "                metatable[i][\"expt_id\"] = row.expt_id\n",
    "\n",
    "# Collapse table to unique values based on sample_id (combines SRZs, essentially)\n",
    "metatable = list({v[\"sample_id\"]: v for v in metatable}.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make linkIDs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(metatable)):\n",
    "        entry = linkIDs(\n",
    "            sample_id=metatable[i][\"sample_id\"],\n",
    "            genetic_id=metatable[i][\"genetic_id\"],\n",
    "            expt_id=metatable[i][\"expt_id\"],\n",
    "        )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load database data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = config[\"file_locations\"][\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape FastQC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    #    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    paper_id = \"\"\n",
    "    sample = metatable[i][\"sample_name\"]\n",
    "    dirpath = data_path + paper_id + \"/qc/fastqc/zips/\"\n",
    "\n",
    "    if metatable[i][config[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "        samp_zip = dirpath + sample + \"_1_fastqc\"\n",
    "    else:\n",
    "        samp_zip = dirpath + sample + \"_fastqc\"\n",
    "\n",
    "    if not (os.path.exists(samp_zip + \".zip\")):\n",
    "        metatable[i][\"raw_read_depth\"] = None\n",
    "        metatable[i][\"raw_read_length\"] = None\n",
    "        metatable[i][\"trim_read_depth\"] = None\n",
    "        continue\n",
    "\n",
    "    with zp.ZipFile(samp_zip + \".zip\", \"r\") as zp_ref:\n",
    "        zp_ref.extractall(dirpath)\n",
    "\n",
    "    fdata = open(samp_zip + \"/fastqc_data.txt\")\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Sequences\").search(line):\n",
    "            metatable[i][\"raw_read_depth\"] = int(line.split()[2])\n",
    "        if re.compile(\"Sequence length\").search(line):\n",
    "            metatable[i][\"raw_read_length\"] = int(line.split()[2].split(\"-\")[0])\n",
    "\n",
    "    shutil.rmtree(samp_zip)\n",
    "\n",
    "    if metatable[i][config[\"accum keys\"][\"rcomp\"]] == 1:\n",
    "        if metatable[i][config[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "            samp_zip = dirpath + sample + \"_1.flip.trim_fastqc\"\n",
    "        else:\n",
    "            samp_zip = dirpath + sample + \".flip.trim_fastqc\"\n",
    "    else:\n",
    "        if metatable[i][config[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "            samp_zip = dirpath + sample + \"_1.trim_fastqc\"\n",
    "        else:\n",
    "            samp_zip = dirpath + sample + \".trim_fastqc\"\n",
    "\n",
    "    with zp.ZipFile(samp_zip + \".zip\", \"r\") as zp_ref:\n",
    "        zp_ref.extractall(dirpath)\n",
    "\n",
    "    fdata = open(samp_zip + \"/fastqc_data.txt\")\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Sequences\").search(line):\n",
    "            metatable[i][\"trim_read_depth\"] = int(line.split()[2])\n",
    "\n",
    "    shutil.rmtree(samp_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape picardtools data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    #    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    paper_id = \"\"\n",
    "    sample = metatable[i][\"sample_name\"]\n",
    "    dirpath = data_path + paper_id + \"/qc/picard/dups/\"\n",
    "    filepath = dirpath + sample + \".marked_dup_metrics.txt\"\n",
    "\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        metatable[i][\"duplication_picard\"] = None\n",
    "        continue\n",
    "\n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Unknown Library\").search(line):\n",
    "            metatable[i][\"duplication_picard\"] = float(line.split(\"\\t\")[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape mapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    #    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    paper_id = \"\"\n",
    "    sample = metatable[i][\"sample_name\"]\n",
    "    dirpath = data_path + paper_id + \"/qc/hisat2_mapstats/\"\n",
    "    filepath = dirpath + sample + \".hisat2_mapstats.txt\"\n",
    "\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        metatable[i][\"single_map\"] = None\n",
    "        metatable[i][\"multi_map\"] = None\n",
    "        metatable[i][\"map_prop\"] = None\n",
    "        continue\n",
    "\n",
    "    fdata = open(filepath)\n",
    "    if metatable[i][config[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "        for line in fdata:\n",
    "            if re.compile(\"concordantly 1 time\").search(line):\n",
    "                reads = int(line.split(\": \")[1].split(\" (\")[0]) * 2\n",
    "            if re.compile(\"Aligned 1 time\").search(line):\n",
    "                metatable[i][\"single_map\"] = reads + int(\n",
    "                    line.split(\": \")[1].split(\" (\")[0]\n",
    "                )\n",
    "            if re.compile(\"concordantly >1 times\").search(line):\n",
    "                reads = int(line.split(\": \")[1].split(\" (\")[0]) * 2\n",
    "            if re.compile(\"Aligned >1 times\").search(line):\n",
    "                metatable[i][\"multi_map\"] = reads + int(\n",
    "                    line.split(\": \")[1].split(\" (\")[0]\n",
    "                )\n",
    "            if re.compile(\"Overall alignment rate\").search(line):\n",
    "                metatable[i][\"map_prop\"] = (\n",
    "                    float(line.split(\": \")[1].split(\"%\")[0]) / 100\n",
    "                )\n",
    "    else:\n",
    "        for line in fdata:\n",
    "            if re.compile(\"Aligned 1 time\").search(line):\n",
    "                metatable[i][\"single_map\"] = int(line.split(\": \")[1].split(\" (\")[0])\n",
    "            if re.compile(\"Aligned >1 times\").search(line):\n",
    "                metatable[i][\"multi_map\"] = int(line.split(\": \")[1].split(\" (\")[0])\n",
    "            if re.compile(\"Overall alignment rate\").search(line):\n",
    "                metatable[i][\"map_prop\"] = (\n",
    "                    float(line.split(\": \")[1].split(\"%\")[0]) / 100\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape rseqc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    #    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    paper_id = \"\"\n",
    "    sample = metatable[i][\"sample_name\"]\n",
    "    dirpath = data_path + paper_id + \"/qc/rseqc/read_distribution/\"\n",
    "    filepath = dirpath + sample + \".read_distribution.txt\"\n",
    "\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        metatable[i][\"rseqc_tags\"] = None\n",
    "        metatable[i][\"rseqc_cds\"] = None\n",
    "        metatable[i][\"cds_rpk\"] = None\n",
    "        metatable[i][\"rseqc_five_utr\"] = None\n",
    "        metatable[i][\"rseqc_three_utr\"] = None\n",
    "        metatable[i][\"rseqc_intron\"] = None\n",
    "        metatable[i][\"intron_rpk\"] = None\n",
    "        metatable[i][\"exint_ratio\"] = None\n",
    "        continue\n",
    "\n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Assigned Tags\").search(line):\n",
    "            metatable[i][\"rseqc_tags\"] = int(line.split()[-1])\n",
    "        if re.compile(\"CDS_Exons\").search(line):\n",
    "            metatable[i][\"rseqc_cds\"] = int(line.split()[2])\n",
    "            metatable[i][\"cds_rpk\"] = float(line.split()[-1])\n",
    "        if re.compile(\"5'UTR_Exons\").search(line):\n",
    "            metatable[i][\"rseqc_five_utr\"] = int(line.split()[2])\n",
    "        if re.compile(\"3'UTR_Exons\").search(line):\n",
    "            metatable[i][\"rseqc_three_utr\"] = int(line.split()[2])\n",
    "        if re.compile(\"Introns\").search(line):\n",
    "            metatable[i][\"rseqc_intron\"] = int(line.split()[2])\n",
    "            metatable[i][\"intron_rpk\"] = float(line.split()[-1])\n",
    "\n",
    "    if metatable[i][\"intron_rpk\"] > 0:\n",
    "        metatable[i][\"exint_ratio\"] = (\n",
    "            metatable[i][\"cds_rpk\"] / metatable[i][\"intron_rpk\"]\n",
    "        )\n",
    "    else:\n",
    "        metatable[i][\"exint_ratio\"] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull preseq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    #    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    paper_id = \"\"\n",
    "    sample = metatable[i][\"sample_name\"]\n",
    "    dirpath = data_path + paper_id + \"/qc/preseq/\"\n",
    "    filepath = dirpath + sample + \".lc_extrap.txt\"\n",
    "\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        metatable[i][\"distinct_tenmillion_prop\"] = None\n",
    "        continue\n",
    "\n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if line.startswith(\"10000000.0\"):\n",
    "            distinct = float(line.split()[1])\n",
    "\n",
    "    metatable[i][\"distinct_tenmillion_prop\"] = distinct / 10000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull pileup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    #    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    paper_id = \"\"\n",
    "    sample = metatable[i][\"sample_name\"]\n",
    "    dirpath = data_path + paper_id + \"/qc/pileup/\"\n",
    "    filepath = dirpath + sample + \".coverage.stats.txt\"\n",
    "\n",
    "    if not (os.path.exists(filepath) and os.path.isfile(filepath)):\n",
    "        metatable[i][\"genome_prop_cov\"] = None\n",
    "        metatable[i][\"avg_fold_cov\"] = None\n",
    "        continue\n",
    "\n",
    "    fdata = open(filepath)\n",
    "    x = 0\n",
    "    total = cov = fold = 0\n",
    "    for line in fdata:\n",
    "        if x == 0:\n",
    "            x = x + 1\n",
    "            continue\n",
    "        else:\n",
    "            x = x + 1\n",
    "            total = total + int(line.split(\"\\t\")[2])\n",
    "            cov = cov + int(line.split(\"\\t\")[5])\n",
    "            fold = fold + float(line.split(\"\\t\")[1]) * int(line.split(\"\\t\")[2])\n",
    "\n",
    "    metatable[i][\"genome_prop_cov\"] = cov / total\n",
    "    metatable[i][\"avg_fold_cov\"] = fold / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate qc/data scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    if (\n",
    "        metatable[i][\"trim_read_depth\"] == None\n",
    "        or metatable[i][\"duplication_picard\"] == None\n",
    "        or metatable[i][\"map_prop\"] == None\n",
    "        or metatable[i][\"distinct_tenmillion_prop\"] == None\n",
    "    ):\n",
    "        metatable[i][\"samp_qc_score\"] = 0\n",
    "    elif (\n",
    "        (metatable[i][\"trim_read_depth\"] <= 5000000)\n",
    "        or (metatable[i][\"duplication_picard\"] >= 0.95)\n",
    "        or (metatable[i][\"map_prop\"] * metatable[i][\"trim_read_depth\"] <= 4000000)\n",
    "        or (metatable[i][\"distinct_tenmillion_prop\"] < 0.05)\n",
    "    ):\n",
    "        metatable[i][\"samp_qc_score\"] = 5\n",
    "    elif (\n",
    "        (metatable[i][\"trim_read_depth\"] <= 10000000)\n",
    "        or (metatable[i][\"duplication_picard\"] >= 0.80)\n",
    "        or (metatable[i][\"map_prop\"] * metatable[i][\"trim_read_depth\"] <= 8000000)\n",
    "        or (metatable[i][\"distinct_tenmillion_prop\"] < 0.2)\n",
    "    ):\n",
    "        metatable[i][\"samp_qc_score\"] = 4\n",
    "    elif (\n",
    "        (metatable[i][\"trim_read_depth\"] <= 15000000)\n",
    "        or (metatable[i][\"duplication_picard\"] >= 0.65)\n",
    "        or (metatable[i][\"map_prop\"] * metatable[i][\"trim_read_depth\"] <= 12000000)\n",
    "        or (metatable[i][\"distinct_tenmillion_prop\"] < 0.35)\n",
    "    ):\n",
    "        metatable[i][\"samp_qc_score\"] = 3\n",
    "    elif (\n",
    "        (metatable[i][\"trim_read_depth\"] <= 20000000)\n",
    "        or (metatable[i][\"duplication_picard\"] >= 0.5)\n",
    "        or (metatable[i][\"map_prop\"] * metatable[i][\"trim_read_depth\"] <= 16000000)\n",
    "        or (metatable[i][\"distinct_tenmillion_prop\"] < 0.5)\n",
    "    ):\n",
    "        metatable[i][\"samp_qc_score\"] = 2\n",
    "    else:\n",
    "        metatable[i][\"samp_qc_score\"] = 1\n",
    "\n",
    "    if metatable[i][\"genome_prop_cov\"] == None or metatable[i][\"exint_ratio\"] == None:\n",
    "        metatable[i][\"samp_data_score\"] = 0\n",
    "    elif (metatable[i][\"genome_prop_cov\"] <= 0.04) or (\n",
    "        metatable[i][\"exint_ratio\"] >= 9\n",
    "    ):\n",
    "        metatable[i][\"samp_data_score\"] = 5\n",
    "    elif (metatable[i][\"genome_prop_cov\"] <= 0.08) or (\n",
    "        metatable[i][\"exint_ratio\"] >= 7\n",
    "    ):\n",
    "        metatable[i][\"samp_data_score\"] = 4\n",
    "    elif (metatable[i][\"genome_prop_cov\"] <= 0.12) or (\n",
    "        metatable[i][\"exint_ratio\"] >= 5\n",
    "    ):\n",
    "        metatable[i][\"samp_data_score\"] = 3\n",
    "    elif (metatable[i][\"genome_prop_cov\"] <= 0.16) or (\n",
    "        metatable[i][\"exint_ratio\"] >= 3\n",
    "    ):\n",
    "        metatable[i][\"samp_data_score\"] = 2\n",
    "    else:\n",
    "        metatable[i][\"samp_data_score\"] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output database values for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open(\"/home/lsanford/Desktop/db_output.csv\", \"w\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=metatable[0].keys())\n",
    "    w.writeheader()\n",
    "    for data in metatable:\n",
    "        w.writerow(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_keys = list(dict(config[\"accum keys\"]).values())\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(metatable)):\n",
    "        entry = sampleAccum(\n",
    "            sample_id=metatable[i][\"sample_id\"],\n",
    "            replicate=metatable[i][config[\"accum keys\"][\"replicate\"]],\n",
    "            single_paired=metatable[i][config[\"accum keys\"][\"single_paired\"]],\n",
    "            rcomp=metatable[i][config[\"accum keys\"][\"rcomp\"]],\n",
    "            expt_unusable=metatable[i][config[\"accum keys\"][\"expt_unusable\"]],\n",
    "            timecourse=metatable[i][config[\"accum keys\"][\"timecourse\"]],\n",
    "            baseline_control_expt=metatable[i][\n",
    "                config[\"accum keys\"][\"baseline_control_expt\"]\n",
    "            ],\n",
    "            notes=metatable[i][config[\"accum keys\"][\"notes\"]],\n",
    "            raw_read_depth=metatable[i][\"raw_read_depth\"],\n",
    "            trim_read_depth=metatable[i][\"trim_read_depth\"],\n",
    "            raw_read_length=metatable[i][\"raw_read_length\"],\n",
    "            duplication_picard=metatable[i][\"duplication_picard\"],\n",
    "            single_map=metatable[i][\"single_map\"],\n",
    "            multi_map=metatable[i][\"multi_map\"],\n",
    "            map_prop=metatable[i][\"map_prop\"],\n",
    "            rseqc_tags=metatable[i][\"rseqc_tags\"],\n",
    "            rseqc_cds=metatable[i][\"rseqc_cds\"],\n",
    "            rseqc_five_utr=metatable[i][\"rseqc_five_utr\"],\n",
    "            rseqc_three_utr=metatable[i][\"rseqc_three_utr\"],\n",
    "            rseqc_intron=metatable[i][\"rseqc_intron\"],\n",
    "            cds_rpk=metatable[i][\"cds_rpk\"],\n",
    "            intron_rpk=metatable[i][\"intron_rpk\"],\n",
    "            exint_ratio=metatable[i][\"exint_ratio\"],\n",
    "            distinct_tenmillion_prop=metatable[i][\"distinct_tenmillion_prop\"],\n",
    "            genome_prop_cov=metatable[i][\"genome_prop_cov\"],\n",
    "            avg_fold_cov=metatable[i][\"avg_fold_cov\"],\n",
    "            samp_qc_score=metatable[i][\"samp_qc_score\"],\n",
    "            samp_data_score=metatable[i][\"samp_data_score\"],\n",
    "        )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add paper qc score to the exptMetadata table based on sample qc scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_update = []\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for paperid in session.query(exptMetadata.paper_id).distinct():\n",
    "        qc_scores = []\n",
    "        data_scores = []\n",
    "\n",
    "        for qc_score in (\n",
    "            session.query(sampleAccum.samp_qc_score)\n",
    "            .join(linkIDs, linkIDs.sample_id == sampleAccum.sample_id)\n",
    "            .join(exptMetadata, exptMetadata.expt_id == linkIDs.expt_id)\n",
    "            .filter(exptMetadata.paper_id == paperid[0])\n",
    "        ):\n",
    "            qc_scores.append(qc_score[0])\n",
    "        for data_score in (\n",
    "            session.query(sampleAccum.samp_data_score)\n",
    "            .join(linkIDs, linkIDs.sample_id == sampleAccum.sample_id)\n",
    "            .join(exptMetadata, exptMetadata.expt_id == linkIDs.expt_id)\n",
    "            .filter(exptMetadata.paper_id == paperid[0])\n",
    "        ):\n",
    "            data_scores.append(data_score[0])\n",
    "\n",
    "        score_update.append(\n",
    "            dict(\n",
    "                identifier=paperid[0],\n",
    "                paperqc=median(qc_scores),\n",
    "                paperdata=median(data_scores),\n",
    "            )\n",
    "        )\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(score_update)):\n",
    "        session.query(exptMetadata).filter(\n",
    "            exptMetadata.paper_id == score_update[i][\"identifier\"]\n",
    "        ).update(\n",
    "            {\n",
    "                exptMetadata.paper_qc_score: score_update[i][\"paperqc\"],\n",
    "                exptMetadata.paper_data_score: score_update[i][\"paperdata\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create nascentflow and bidirectionalflow versions table and scrape version data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "nascentflow_keys = list(dict(config[\"nascentflow keys\"]).values())\n",
    "bidirflow_keys = list(dict(config[\"bidirflow keys\"]).values())\n",
    "dirpath = config[\"file_locations\"][\"version_data\"]\n",
    "\n",
    "nf_table = []\n",
    "bidir_table = []\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for row in (\n",
    "        session.query(linkIDs.sample_id, exptMetadata.paper_id, sampleID.sample_name)\n",
    "        .join(exptMetadata, exptMetadata.expt_id == linkIDs.expt_id)\n",
    "        .join(sampleID, sampleID.sample_id == linkIDs.sample_id)\n",
    "        .distinct()\n",
    "    ):\n",
    "        nf_table.append(\n",
    "            {\"sample_id\": row[0], \"paper_id\": row[1], \"sample_name\": row[2]}\n",
    "        )\n",
    "        bidir_table.append(\n",
    "            {\"sample_id\": row[0], \"paper_id\": row[1], \"sample_name\": row[2]}\n",
    "        )\n",
    "\n",
    "for i in range(len(nf_table)):\n",
    "    paper = nf_table[i][\"paper_id\"]\n",
    "    sampname = nf_table[i][\"sample_name\"]\n",
    "    sampid = nf_table[i][\"sample_id\"]\n",
    "#    nascent_path = dirpath + paper + \"/\" + sampname + \"_nascent.yaml\"\n",
    "#    bidir_path = dirpath + paper + \"/\" + sampname + \"_bidir.yaml\"\n",
    "    nascent_path = dirpath + sampname + \"_nascent.yaml\"\n",
    "    bidir_path = dirpath + sampname + \"_bidir.yaml\"\n",
    "\n",
    "    if not (os.path.exists(nascent_path) and os.path.isfile(nascent_path)):\n",
    "        for key in nascentflow_keys:\n",
    "            nf_table[i][key] = None\n",
    "        for key in bidirflow_keys:\n",
    "            bidir_table[i][key] = None\n",
    "        continue\n",
    "\n",
    "    with open(nascent_path) as f:\n",
    "        j = 0\n",
    "        for nfrun in yaml.safe_load_all(f):\n",
    "            if j == 0:\n",
    "                j = 1\n",
    "                nf_table[i].update(nfrun)\n",
    "                for key in nascentflow_keys:\n",
    "                    if not key in nf_table[i]:\n",
    "                        nf_table[i][key] = None\n",
    "            else:\n",
    "                nf_table.append(\n",
    "                    {\"sample_id\": sampid, \"paper_id\": paper, \"sample_name\": sampname}\n",
    "                )\n",
    "                nf_table[-1].update(nfrun)\n",
    "                for key in nascentflow_keys:\n",
    "                    if not key in nf_table[-1]:\n",
    "                        nf_table[-1][key] = None\n",
    "\n",
    "    if not (os.path.exists(bidir_path) and os.path.isfile(bidir_path)):\n",
    "        for key in bidirflow_keys:\n",
    "            bidir_table[i][key] = None\n",
    "        continue\n",
    "\n",
    "    with open(bidir_path) as f:\n",
    "        j = 0\n",
    "        for bidirrun in yaml.safe_load_all(f):\n",
    "            if j == 0:\n",
    "                j = 1\n",
    "                bidir_table[i].update(bidirrun)\n",
    "                for key in bidirflow_keys:\n",
    "                    if not key in bidir_table[i]:\n",
    "                        bidir_table[i][key] = None\n",
    "            else:\n",
    "                bidir_table.append(\n",
    "                    {\"sample_id\": sampid, \"paper_id\": paper, \"sample_name\": sampname}\n",
    "                )\n",
    "                bidir_table[-1].update(bidirrun)\n",
    "                for key in bidirflow_keys:\n",
    "                    if not key in bidir_table[-1]:\n",
    "                        bidir_table[-1][key] = None\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(nf_table)):\n",
    "        entry = nascentflowMetadata(\n",
    "            sample_id=nf_table[i][\"sample_id\"],\n",
    "            nascentflow_version=nf_table[i][config[\"nascentflow keys\"][\"nascentflow_version\"]],\n",
    "            pipeline_revision_hash=nf_table[i][config[\"nascentflow keys\"][\"pipeline_revision_hash\"]],\n",
    "            pipeline_hash=nf_table[i][config[\"nascentflow keys\"][\"pipeline_hash\"]],\n",
    "            nascentflow_date=nf_table[i][config[\"nascentflow keys\"][\"nascentflow_date\"]],\n",
    "            nascentflow_redo_date=nf_table[i][config[\"nascentflow keys\"][\"nascentflow_redo_date\"]],\n",
    "            nextflow_version=nf_table[i][config[\"nascentflow keys\"][\"nextflow_version\"]],\n",
    "            fastqc_version=nf_table[i][config[\"nascentflow keys\"][\"fastqc_version\"]],\n",
    "            bbmap_version=nf_table[i][config[\"nascentflow keys\"][\"bbmap_version\"]],\n",
    "            hisat2_version=nf_table[i][config[\"nascentflow keys\"][\"hisat2_version\"]],\n",
    "            samtools_version=nf_table[i][config[\"nascentflow keys\"][\"samtools_version\"]],\n",
    "            sratools_version=nf_table[i][config[\"nascentflow keys\"][\"sratools_version\"]],\n",
    "            preseq_version=nf_table[i][config[\"nascentflow keys\"][\"preseq_version\"]],\n",
    "            preseq_date=nf_table[i][config[\"nascentflow keys\"][\"preseq_date\"]],\n",
    "            rseqc_version=nf_table[i][config[\"nascentflow keys\"][\"rseqc_version\"]],\n",
    "            rseqc_date=nf_table[i][config[\"nascentflow keys\"][\"rseqc_date\"]],\n",
    "            java_version=nf_table[i][config[\"nascentflow keys\"][\"java_version\"]],\n",
    "            picard_gc_version=nf_table[i][config[\"nascentflow keys\"][\"picard_gc_version\"]],\n",
    "            picard_dups_version=nf_table[i][config[\"nascentflow keys\"][\"picard_dups_version\"]],\n",
    "            picard_date=nf_table[i][config[\"nascentflow keys\"][\"picard_date\"]],\n",
    "            bedtools_version=nf_table[i][config[\"nascentflow keys\"][\"bedtools_version\"]],\n",
    "            igvtools_version=nf_table[i][config[\"nascentflow keys\"][\"igvtools_version\"]],\n",
    "            seqkit_version=nf_table[i][config[\"nascentflow keys\"][\"seqkit_version\"]],\n",
    "            mpich_version=nf_table[i][config[\"nascentflow keys\"][\"mpich_version\"]],\n",
    "            gcc_version=nf_table[i][config[\"nascentflow keys\"][\"gcc_version\"]],\n",
    "            python_version=nf_table[i][config[\"nascentflow keys\"][\"python_version\"]],\n",
    "            numpy_version=nf_table[i][config[\"nascentflow keys\"][\"numpy_version\"]],\n",
    "        )\n",
    "        session.merge(entry)\n",
    "\n",
    "# with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url=\"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(bidir_table)):\n",
    "        entry = bidirflowMetadata(\n",
    "            sample_id=bidir_table[i][\"sample_id\"],\n",
    "            bidirflow_version=bidir_table[i][config[\"bidirflow keys\"][\"bidirflow_version\"]],\n",
    "            pipeline_revision_hash=bidir_table[i][config[\"bidirflow keys\"][\"pipeline_revision_hash\"]],\n",
    "            pipeline_hash=bidir_table[i][config[\"bidirflow keys\"][\"pipeline_hash\"]],\n",
    "            bidirflow_date=bidir_table[i][config[\"bidirflow keys\"][\"bidirflow_date\"]],\n",
    "            nextflow_version=bidir_table[i][config[\"bidirflow keys\"][\"nextflow_version\"]],\n",
    "            samtools_version=bidir_table[i][config[\"bidirflow keys\"][\"samtools_version\"]],\n",
    "            bedtools_version=bidir_table[i][config[\"bidirflow keys\"][\"bedtools_version\"]],\n",
    "            mpich_version=bidir_table[i][config[\"bidirflow keys\"][\"mpich_version\"]],\n",
    "            openmpi_version=bidir_table[i][config[\"bidirflow keys\"][\"openmpi_version\"]],\n",
    "            gcc_version=bidir_table[i][config[\"bidirflow keys\"][\"gcc_version\"]],\n",
    "            r_version=bidir_table[i][config[\"bidirflow keys\"][\"r_version\"]],\n",
    "            rsubread_version=bidir_table[i][config[\"bidirflow keys\"][\"rsubread_version\"]],\n",
    "            boost_version=bidir_table[i][config[\"bidirflow keys\"][\"boost_version\"]],\n",
    "            fstitch_version=bidir_table[i][config[\"bidirflow keys\"][\"fstitch_version\"]],\n",
    "            tfit_version=bidir_table[i][config[\"bidirflow keys\"][\"tfit_version\"]],\n",
    "            dreg_version=bidir_table[i][config[\"bidirflow keys\"][\"dreg_version\"]],\n",
    "            dreg_date=bidir_table[i][config[\"bidirflow keys\"][\"dreg_date\"]],\n",
    "            tfit_date=bidir_table[i][config[\"bidirflow keys\"][\"tfit_date\"]],\n",
    "            fcgene_date=bidir_table[i][config[\"bidirflow keys\"][\"fcgene_date\"]],\n",
    "        )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
