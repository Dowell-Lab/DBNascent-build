{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dbnascent_initial_build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is a script for initially building dbnascent from metadata\n",
    "### See dbnascent_update for updating existing database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import sqlalchemy as sql\n",
    "import zipfile as zp\n",
    "import re\n",
    "#from . import utils #(in script)\n",
    "#from . import orm #(in script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import utilities (in notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "# utils.py --- Utilities for simplifying database code\n",
    "#\n",
    "# Filename: utils.py\n",
    "# Description: Miscellaneous utilities for simplifying database code\n",
    "# Author: Zachary Maas <zama8258@colorado.edu> and Lynn Sanford \n",
    "# Maintainer: Lynn Sanford <lynn.sanford@colorado.edu>\n",
    "# Created: Mon Jul  1 16:04:05 2019 (-0600)\n",
    "#\n",
    "\n",
    "# Commentary:\n",
    "#\n",
    "# This module contains a few helpful utility functions and classes for\n",
    "# reducing the total amount of code needed for the database, since\n",
    "# there are many areas where the same patterns keep popping up.\n",
    "#\n",
    "\n",
    "# Code:\n",
    "\n",
    "import os\n",
    "import configparser\n",
    "import sqlalchemy as sql\n",
    "import csv\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "\n",
    "# Database Connection Handler\n",
    "class NascentDBConnection:\n",
    "    engine = None\n",
    "    _Session = None\n",
    "    session = None\n",
    "\n",
    "    def __init__(self, db_url):\n",
    "        self.engine = sql.create_engine(db_url, echo=False)\n",
    "        self.Session = sessionmaker(bind=self.engine)\n",
    "        self.session = self.Session()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self.session\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.session.commit()\n",
    "        self.engine.dispose()\n",
    "\n",
    "\n",
    "# Configuration File Reader\n",
    "def load_config(filename: str):\n",
    "    if not os.path.exists(filename):\n",
    "        raise FileNotFoundError(\n",
    "            \"Configuration file does not exist at the provided path\"\n",
    "        )\n",
    "    config = configparser.ConfigParser()\n",
    "    with open(filename) as confFile:\n",
    "        config.read_string(confFile.read())\n",
    "    return config\n",
    "\n",
    "# Add/update (?) tables in database (I'm not actually sure this updates if already existing)\n",
    "\n",
    "def update_tables(db_url: str) -> None:\n",
    "    engine = sql.create_engine(\"sqlite:///\" + db_url, echo=False)\n",
    "    Base.metadata.create_all(engine, checkfirst = True)\n",
    "\n",
    "# Function for parsing table into list of dicts\n",
    "def table_parse(table_filepath: str) -> list:\n",
    "    \"\"\"Takes the manually curated metadata table as input and \n",
    "    turns it into a list of dicts, one entry for each srr with\n",
    "    key: value pairs for each column in the metadata table\n",
    "    Output: List of dicts\n",
    "    \"\"\"\n",
    "\n",
    "    # Check that the table file exists\n",
    "    if not (\n",
    "        os.path.exists(table_filepath) and os.path.isfile(table_filepath)\n",
    "    ):\n",
    "        raise FileNotFoundError(f\"{table_filepath} does not exist.\")\n",
    "\n",
    "    # Load in file as a list of dicts\n",
    "    table_list = []\n",
    "    with open(table_filepath, newline = '') as tab:                                                                                          \n",
    "        full_table = csv.DictReader(tab, delimiter=\"\\t\")\n",
    "        for entry in full_table:\n",
    "            table_list.append(dict(entry))\n",
    "    \n",
    "    return table_list\n",
    "\n",
    "# Function for grabbing specific keys\n",
    "def key_grab(table_list, key_list) -> list:\n",
    "    \"\"\"Takes list of dicts and a list of keys and \n",
    "    extracts specific values to a list for inputting into database\n",
    "    Output: List of values corresponding to input keys for each \n",
    "    table entry\n",
    "    \"\"\"\n",
    "    # Load in file as a list of dicts\n",
    "    value_list = []\n",
    "    for entry in table_list:\n",
    "        value_subset = []\n",
    "        for i in range(len(key_list)):\n",
    "            value_subset.append(entry[key_list[i]])\n",
    "        value_list.append(value_subset)\n",
    "    \n",
    "    return value_list\n",
    "\n",
    "def get_unique_table(location_key, column_keys) -> dict:\n",
    "    filepath = config[\"file_locations\"][location_key]\n",
    "    full_table_dict = table_parse(filepath)\n",
    "    \n",
    "    full_table_list = np.array(key_grab(full_table_dict, column_keys))\n",
    "    unique_list = np.unique(full_table_list, axis=0)\n",
    "\n",
    "    unique_table = []\n",
    "    for i in range(len(unique_list)):\n",
    "        entry = dict(zip(column_keys, unique_list[i]))\n",
    "        unique_table.append(entry)\n",
    "    \n",
    "    return unique_table\n",
    "\n",
    "def value_compare(db_row,metatable_row,key_dict) -> bool:\n",
    "    for key in key_dict:\n",
    "        if db_row[key] == metatable_row[key_dict[key]]:\n",
    "            continue\n",
    "        else:\n",
    "            return 0\n",
    "    return 1   \n",
    "    \n",
    "def object_as_dict(obj):\n",
    "    return {c.key: getattr(obj, c.key)\n",
    "            for c in sql.inspect(obj).mapper.column_attrs}\n",
    "    \n",
    "#\n",
    "# utils.py ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import ORM (in notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load orm.py\n",
    "# orm.py --- ORM for DBNascent\n",
    "#\n",
    "# Filename: orm.py\n",
    "# Description: ORM for DBNascent\n",
    "# Author: Zach Maas and Lynn Sanford\n",
    "# Maintainer: Lynn Sanford <lynn.sanford@colorado.edu>\n",
    "# Created: Mon Jun 10 13:11:55 2019 (-0600)\n",
    "# URL:\n",
    "#\n",
    "\n",
    "# Commentary:\n",
    "#\n",
    "# This file contains code for an ORM to interface with the Dowell\n",
    "# Lab's Nascent Database.\n",
    "#\n",
    "\n",
    "# Code:\n",
    "\n",
    "import sqlalchemy as sql\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "\n",
    "# Base class for our ORM\n",
    "Base = declarative_base()\n",
    "\n",
    "\n",
    "# MAIN TABLES\n",
    "class organismInfo(Base):\n",
    "    __tablename__ = \"organismInfo\"\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), \n",
    "        primary_key = True, \n",
    "        index = True, \n",
    "        unique = True\n",
    "    )\n",
    "    genome_build = sql.Column(sql.String(length=50))\n",
    "    genome_bases = sql.Column(sql.Integer)\n",
    "\n",
    "class exptMetadata(Base):\n",
    "    __tablename__ = \"exptMetadata\"\n",
    "    expt_id = sql.Column(\n",
    "        sql.Integer, \n",
    "        primary_key = True, \n",
    "        index = True, \n",
    "        unique = True\n",
    "    )\n",
    "    srp = sql.Column(sql.String(length=50))\n",
    "    protocol = sql.Column(sql.String(length=50))\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), \n",
    "        sql.ForeignKey(\"organismInfo.organism\")\n",
    "    )\n",
    "    library = sql.Column(sql.String(length=50))\n",
    "    spikein = sql.Column(sql.String(length=127))\n",
    "    paper_id = sql.Column(sql.String(length=127))\n",
    "    published = sql.Column(sql.Boolean)\n",
    "    year = sql.Column(sql.Integer)\n",
    "    first_author = sql.Column(sql.String(length=127))\n",
    "    last_author = sql.Column(sql.String(length=127))\n",
    "    doi = sql.Column(sql.String(length=300))\n",
    "    curator1 = sql.Column(sql.String(length=50))\n",
    "    curator2 = sql.Column(sql.String(length=50))\n",
    "    other_sr_data = sql.Column(sql.Boolean)\n",
    "    atac_seq = sql.Column(sql.Boolean)\n",
    "    rna_seq = sql.Column(sql.Boolean)\n",
    "    chip_seq = sql.Column(sql.Boolean)\n",
    "    three_dim_seq = sql.Column(sql.Boolean)\n",
    "    \n",
    "class sampleID(Base):\n",
    "    __tablename__ = \"sampleID\"\n",
    "    srr = sql.Column(\n",
    "        sql.String(length=50), \n",
    "        primary_key = True, \n",
    "        index = True, \n",
    "        unique = True\n",
    "    )\n",
    "    sample_name = sql.Column(sql.String(length=50))\n",
    "    sample_id = sql.Column(sql.Integer)\n",
    "    \n",
    "class geneticInfo(Base):\n",
    "    __tablename__ = \"geneticInfo\"\n",
    "    genetic_id = sql.Column(\n",
    "        sql.Integer, \n",
    "        primary_key = True, \n",
    "        index = True, \n",
    "        unique = True\n",
    "    )\n",
    "    organism = sql.Column(\n",
    "        sql.String(length=127), \n",
    "        sql.ForeignKey(\"organismInfo.organism\")\n",
    "    )\n",
    "    cell_type = sql.Column(sql.String(length=127))\n",
    "    strain = sql.Column(sql.String(length=127))\n",
    "    genotype = sql.Column(sql.String(length=127))\n",
    "    construct = sql.Column(sql.String(length=127))\n",
    "\n",
    "class conditionInfo(Base):\n",
    "    __tablename__ = \"conditionInfo\"\n",
    "    condition_id = sql.Column(\n",
    "        sql.Integer, \n",
    "        primary_key = True, \n",
    "        index = True, \n",
    "        unique = True\n",
    "    )\n",
    "    condition_type = sql.Column(sql.String(length=127))\n",
    "    treatment = sql.Column(sql.String(length=127))\n",
    "    conc_intens = sql.Column(sql.String(length=50))\n",
    "    start_time = sql.Column(sql.Integer)\n",
    "    end_time = sql.Column(sql.Integer)\n",
    "    duration = sql.Column(sql.Integer)\n",
    "    time_unit = sql.Column(sql.String(length=50))\n",
    "    \n",
    "exptCondition = sql.Table('exptCondition', Base.metadata,\n",
    "    sql.Column('sample_id', \n",
    "               sql.Integer, \n",
    "               sql.ForeignKey('sampleID.sample_id')),\n",
    "    sql.Column('condition_id', \n",
    "               sql.Integer, \n",
    "               sql.ForeignKey('conditionInfo.condition_id'))\n",
    ")\n",
    "    \n",
    "class linkIDs(Base):\n",
    "    __tablename__ = \"linkIDs\"\n",
    "    sample_id = sql.Column(\n",
    "        sql.Integer,\n",
    "        sql.ForeignKey(\"sampleID.sample_id\"),\n",
    "        primary_key=True,\n",
    "        index = True,\n",
    "        unique = True\n",
    "    )\n",
    "    genetic_id = sql.Column(\n",
    "        sql.Integer,\n",
    "        sql.ForeignKey(\"geneticInfo.genetic_id\")\n",
    "    )\n",
    "    expt_id = sql.Column(\n",
    "        sql.Integer, \n",
    "        sql.ForeignKey(\"exptMetadata.expt_id\")\n",
    "    )\n",
    "    \n",
    "class sampleAccum(Base):\n",
    "    __tablename__ = \"sampleAccum\"\n",
    "    sample_id = sql.Column(\n",
    "        sql.Integer,\n",
    "        sql.ForeignKey(\"sampleID.sample_id\"),\n",
    "        primary_key=True,\n",
    "        index = True,\n",
    "        unique = True\n",
    "    )\n",
    "    replicate = sql.Column(sql.Integer)\n",
    "    single_paired = sql.Column(sql.String(length=50))\n",
    "    rcomp = sql.Column(sql.Boolean)\n",
    "    raw_read_depth = sql.Column(sql.Integer)\n",
    "    trim_read_depth = sql.Column(sql.Integer)\n",
    "    raw_read_length = sql.Column(sql.Integer)\n",
    "    duplication_picard = sql.Column(sql.Float)\n",
    "    single_map = sql.Column(sql.Integer)\n",
    "    multi_map = sql.Column(sql.Integer)\n",
    "    map_prop = sql.Column(sql.Float)\n",
    "    rseqc_tags = sql.Column(sql.Integer)\n",
    "    rseqc_cds = sql.Column(sql.Integer)\n",
    "    rseqc_five_utr = sql.Column(sql.Integer)\n",
    "    rseqc_three_utr = sql.Column(sql.Integer)\n",
    "    rseqc_intron = sql.Column(sql.Integer)\n",
    "    cds_rpk = sql.Column(sql.Float)\n",
    "    intron_rpk = sql.Column(sql.Float)\n",
    "    exint_ratio = sql.Column(sql.Float)\n",
    "    distinct_tenmillion_prop = sql.Column(sql.Float)\n",
    "    genome_prop_cov = sql.Column(sql.Float)\n",
    "    avg_fold_cov = sql.Column(sql.Float)\n",
    "    \n",
    "class pipelineMetadata(Base):\n",
    "    __tablename__ = \"pipelineMetadata\"\n",
    "    sample_id = sql.Column(\n",
    "        sql.Integer,\n",
    "        sql.ForeignKey(\"sampleID.sample_id\"),\n",
    "        primary_key=True\n",
    "    )\n",
    "    fastqc_version = sql.Column(sql.String(length=127))\n",
    "    bbduk_version = sql.Column(sql.String(length=127))\n",
    "    hisat2_version = sql.Column(sql.String(length=127))\n",
    "    samtools_version = sql.Column(sql.String(length=127))\n",
    "    fastq_dump_version = sql.Column(sql.String(length=127))\n",
    "    pileup_version = sql.Column(sql.String(length=127))\n",
    "    preseq_version = sql.Column(sql.String(length=127))\n",
    "    rseqc_version = sql.Column(sql.String(length=127))\n",
    "    bedtools_version = sql.Column(sql.String(length=127))\n",
    "    igv_tools_version = sql.Column(sql.String(length=127))\n",
    "    fstitch_version = sql.Column(sql.String(length=127))\n",
    "    tfit_version = sql.Column(sql.String(length=127))\n",
    "    dreg_version = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "# The following were created by Zach and we may or may not use...    \n",
    "    \n",
    "#class tf(Base):\n",
    "#    __tablename__ = \"tf\"\n",
    "#    tf_id = sql.Column(sql.String(length=127), primary_key=True)\n",
    "#    tf_alias = sql.Column(sql.String(length=127))\n",
    "\n",
    "\n",
    "#class pipeline_status(Base):\n",
    "#    __tablename__ = \"pipeline_status\"\n",
    "#    srr_id = sql.Column(\n",
    "#        sql.String(length=127),\n",
    "#        sql.ForeignKey(\"srr_metadata.srr_id\"),\n",
    "#        primary_key=True,\n",
    "#    )\n",
    "#    fastqc_complete = sql.Column(sql.Boolean)\n",
    "#    bbduk_complete = sql.Column(sql.Boolean)\n",
    "#    hisat2_complete = sql.Column(sql.Boolean)\n",
    "#    samtools_complete = sql.Column(sql.Boolean)\n",
    "#    fastq_dump_complete = sql.Column(sql.Boolean)\n",
    "#    pileup_complete = sql.Column(sql.String(length=127))\n",
    "#    preseq_complete = sql.Column(sql.Boolean)\n",
    "#    rseqc_complete = sql.Column(sql.String(length=127))\n",
    "#    bedtools_complete = sql.Column(sql.Boolean)\n",
    "#    igv_tools_complete = sql.Column(sql.Boolean)\n",
    "#    fstitch_complete = sql.Column(sql.Boolean)\n",
    "#    tfit_complete = sql.Column(sql.Boolean)\n",
    "\n",
    "\n",
    "#class md_score(Base):\n",
    "#    __tablename__ = \"md_score\"\n",
    "#    srr_id = sql.Column(\n",
    "#        sql.String(length=127),\n",
    "#        sql.ForeignKey(\"srr_metadata.srr_id\"),\n",
    "#        primary_key=True,\n",
    "#    )\n",
    "#    tf_id = sql.Column(sql.String, sql.ForeignKey(\"tf.tf_id\"))\n",
    "#    erna_type = sql.Column(sql.String(length=127))\n",
    "#    md_score_expected = sql.Column(sql.Integer)\n",
    "#    md_score_std = sql.Column(sql.Integer)\n",
    "\n",
    "\n",
    "# orm.py ends here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = utils.load_config(\"/home/lsanford/Documents/data/repositories/dbnascent_build/config.txt\")\n",
    "config = load_config(\"/home/lsanford/Documents/data/repositories/dbnascent_build/config.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tables in database if they don't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_url = config[\"file_locations\"][\"database\"]\n",
    "\n",
    "#utils.update_tables(db_url)\n",
    "update_tables(db_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse pre-created genome table and load into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "organism_keys = list(dict(config[\"organism keys\"]).values())\n",
    "\n",
    "#organism_table = utils.get_unique_table(\"organism_table\",organism_keys)\n",
    "organism_table = get_unique_table(\"organism_table\",organism_keys)\n",
    "\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(organism_table)):\n",
    "        entry = organismInfo(\n",
    "            organism = organism_table[i][config[\"organism keys\"][\"organism\"]],\n",
    "            genome_build = organism_table[i][config[\"organism keys\"][\"genome_build\"]],\n",
    "            genome_bases = organism_table[i][config[\"organism keys\"][\"genome_bases\"]],\n",
    "        )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse metadata table and load exptMetadata values into DB, creating expt_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_keys = list(dict(config[\"expt keys\"]).values())\n",
    "\n",
    "#expt_table = utils.get_unique_table(\"metadata\",expt_keys)\n",
    "expt_table = get_unique_table(\"metadata\",expt_keys)\n",
    "\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(expt_table)):\n",
    "        entry = exptMetadata(\n",
    "            srp = expt_table[i][config[\"expt keys\"][\"srp\"]],\n",
    "            protocol = expt_table[i][config[\"expt keys\"][\"protocol\"]],\n",
    "            organism = expt_table[i][config[\"expt keys\"][\"organism\"]],\n",
    "            library = expt_table[i][config[\"expt keys\"][\"library\"]],\n",
    "            spikein = expt_table[i][config[\"expt keys\"][\"spikein\"]],\n",
    "            paper_id = expt_table[i][config[\"expt keys\"][\"paper_id\"]],\n",
    "            published = int(expt_table[i][config[\"expt keys\"][\"published\"]]),\n",
    "            year = expt_table[i][config[\"expt keys\"][\"year\"]],\n",
    "            first_author = expt_table[i][config[\"expt keys\"][\"first_author\"]],\n",
    "            last_author = expt_table[i][config[\"expt keys\"][\"last_author\"]],\n",
    "            doi = expt_table[i][config[\"expt keys\"][\"doi\"]],\n",
    "            curator1 = expt_table[i][config[\"expt keys\"][\"curator1\"]],\n",
    "            curator2 = expt_table[i][config[\"expt keys\"][\"curator2\"]],\n",
    "            other_sr_data = int(expt_table[i][config[\"expt keys\"][\"other_sr_data\"]]),\n",
    "            atac_seq = int(expt_table[i][config[\"expt keys\"][\"atac_seq\"]]),\n",
    "            rna_seq = int(expt_table[i][config[\"expt keys\"][\"rna_seq\"]]),\n",
    "            chip_seq = int(expt_table[i][config[\"expt keys\"][\"chip_seq\"]]),\n",
    "            three_dim_seq = int(expt_table[i][config[\"expt keys\"][\"three_dim_seq\"]]),\n",
    "            )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse metadata table and load sampleID values into DB, creating sample_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_keys = list(dict(config[\"sample keys\"]).values())\n",
    "\n",
    "#sample_table = utils.get_unique_table(\"metadata\",sample_keys)\n",
    "sample_table = get_unique_table(\"metadata\",sample_keys)\n",
    "\n",
    "#metatable = utils.table_parse(config[\"file_locations\"][\"metadata\"])\n",
    "#srz_list = np.array(utils.key_grab(metatable, [config[\"sample keys\"][\"sample_name\"]]))\n",
    "metatable = table_parse(config[\"file_locations\"][\"metadata\"])\n",
    "srz_list = np.array(key_grab(metatable, [config[\"sample keys\"][\"sample_name\"]]))\n",
    "srz_list = np.unique(srz_list[srz_list != \"\"])\n",
    "srz_table = dict(zip(srz_list,list(range(1,len(srz_list)+1))))\n",
    "\n",
    "z = len(srz_table) + 1\n",
    "for i in range(len(sample_table)):\n",
    "    if sample_table[i][config[\"sample keys\"][\"sample_name\"]] == \"\":\n",
    "        sample_table[i][\"sample_id\"] = z\n",
    "        sample_table[i][config[\"sample keys\"][\"sample_name\"]] = sample_table[i][config[\"sample keys\"][\"srr\"]]\n",
    "        z = z + 1\n",
    "    else:\n",
    "        sample_table[i][\"sample_id\"] = srz_table[sample_table[i][config[\"sample keys\"][\"sample_name\"]]]\n",
    "        \n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(sample_table)):\n",
    "        entry = sampleID(\n",
    "            sample_id = sample_table[i][\"sample_id\"],\n",
    "            srr = sample_table[i][config[\"sample keys\"][\"srr\"]],\n",
    "            sample_name = sample_table[i][config[\"sample keys\"][\"sample_name\"]],\n",
    "            )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse metadata table and load geneticInfo values into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_keys = list(dict(config[\"genetic keys\"]).values())\n",
    "\n",
    "#genetic_table = utils.get_unique_table(\"metadata\",genetic_keys)\n",
    "genetic_table = get_unique_table(\"metadata\",genetic_keys)\n",
    "\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(genetic_table)):\n",
    "        genetic = geneticInfo(\n",
    "            organism = genetic_table[i][config[\"genetic keys\"][\"organism\"]],\n",
    "            cell_type = genetic_table[i][config[\"genetic keys\"][\"cell_type\"]],\n",
    "            strain = genetic_table[i][config[\"genetic keys\"][\"strain\"]],\n",
    "            genotype = genetic_table[i][config[\"genetic keys\"][\"genotype\"]],\n",
    "            construct = genetic_table[i][config[\"genetic keys\"][\"construct\"]],\n",
    "            )\n",
    "        session.merge(genetic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse metadata table and load conditionInfo values into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_keys = list(dict(config[\"condition keys\"]).values())\n",
    "\n",
    "#condition_table = utils.get_unique_table(\"conditions\",condition_keys)\n",
    "condition_table = get_unique_table(\"conditions\",condition_keys)\n",
    "\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(condition_table)):\n",
    "        if not condition_table[i][\"end time\"]:\n",
    "            duration = \"\"\n",
    "        else:\n",
    "            duration = int(condition_table[i][\"end time\"]) - int(condition_table[i][\"start time\"])\n",
    "            \n",
    "        condition = conditionInfo(\n",
    "            condition_type = condition_table[i][config[\"condition keys\"][\"condition_type\"]],\n",
    "            treatment = condition_table[i][config[\"condition keys\"][\"treatment\"]],\n",
    "            conc_intens = condition_table[i][config[\"condition keys\"][\"conc_intens\"]],\n",
    "            start_time = condition_table[i][config[\"condition keys\"][\"start_time\"]],\n",
    "            end_time = condition_table[i][config[\"condition keys\"][\"end_time\"]],\n",
    "            duration = duration,\n",
    "            time_unit = condition_table[i][config[\"condition keys\"][\"time_unit\"]],\n",
    "            )\n",
    "        session.merge(condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create exptCondition equivalencies in DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull condition info including ID from database and make a unique hash for conditions\n",
    "\n",
    "condition_id = []\n",
    "condition_details = []\n",
    "\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for row in session.query(conditionInfo).all():\n",
    "        condition_id.append(row.condition_id)\n",
    "        condition_details.append(\"\".join([str(row.condition_type),\n",
    "                                         str(row.treatment),\n",
    "                                         str(row.conc_intens),\n",
    "                                         str(row.start_time),\n",
    "                                         str(row.end_time),\n",
    "                                         str(row.time_unit)]))\n",
    "\n",
    "condition_dict = dict(zip(condition_details,condition_id))\n",
    "\n",
    "# Pull sample ID from database for each SRR\n",
    "\n",
    "srr = []\n",
    "sample_id = []\n",
    "\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for row in session.query(sampleID).all():\n",
    "        srr.append(row.srr)\n",
    "        sample_id.append(row.sample_id)\n",
    "        \n",
    "sample_dict = dict(zip(srr,sample_id))\n",
    "\n",
    "# Grab condition table including SRRs and make SRR/condition hash\n",
    "\n",
    "condition_keys = list(dict(config[\"condition keys\"]).values())\n",
    "condition_keys.append(\"srr\")\n",
    "#condition_table = utils.table_parse(config[\"file_locations\"][\"conditions\"])\n",
    "#cond_str = utils.key_grab(condition_table,condition_keys)\n",
    "condition_table = table_parse(config[\"file_locations\"][\"conditions\"])\n",
    "cond_str = key_grab(condition_table,condition_keys)\n",
    "\n",
    "srr_cond = []\n",
    "for i in range(len(cond_str)):\n",
    "    srr_cond.append([cond_str[i][-1],\"\".join(cond_str[i][0:-1])])\n",
    "\n",
    "srr_cond = np.unique(np.array(srr_cond),axis=0)\n",
    "\n",
    "# Make sample ID/condition ID table\n",
    "sample_condition = []\n",
    "for i in range(len(srr_cond)):\n",
    "    sample_id = sample_dict[srr_cond[i][0]]\n",
    "    condition = condition_dict[srr_cond[i][1]]\n",
    "    sample_condition.append([sample_id,condition])\n",
    "\n",
    "sample_condition = np.unique(np.array(sample_condition),axis=0)\n",
    "\n",
    "# Add to database\n",
    "\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(sample_condition)):\n",
    "        statement = exptCondition.insert().values(sample_id=int(sample_condition[i][0]), \n",
    "                                                  condition_id=int(sample_condition[i][1]))\n",
    "        session.execute(statement)\n",
    "        session.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build sampleAccum table and linker table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add relevant DB keys to table in correct rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#metatable = utils.table_parse(config[\"file_locations\"][\"metadata\"])\n",
    "metatable = table_parse(config[\"file_locations\"][\"metadata\"])\n",
    "\n",
    "# Do some data massaging\n",
    "for i in range(len(metatable)):\n",
    "    metatable[i][\"year\"] = int(metatable[i][\"year\"])\n",
    "    metatable[i][\"replicate\"] = metatable[i][\"replicate\"][0:4]\n",
    "    if not metatable[i][config[\"sample keys\"][\"sample_name\"]]:\n",
    "        metatable[i][config[\"sample keys\"][\"sample_name\"]] = metatable[i][config[\"sample keys\"][\"srr\"]]\n",
    "    for key in metatable[i]:\n",
    "        if metatable[i][key] == '0':\n",
    "            metatable[i][key] = False\n",
    "        elif metatable[i][key] == '1':\n",
    "            metatable[i][key] = True\n",
    "\n",
    "# Add sample id\n",
    "sample_keys = dict(config[\"sample keys\"])\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for row in session.query(sampleID).all():\n",
    "        db_row = object_as_dict(row)\n",
    "        for i in range(len(metatable)):\n",
    "            if value_compare(db_row,metatable[i],sample_keys):\n",
    "                metatable[i][\"sample_id\"] = row.sample_id\n",
    "                metatable[i][\"sample_name\"] = row.sample_name\n",
    "\n",
    "# Add genetic id\n",
    "genetic_keys = dict(config[\"genetic keys\"])\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for row in session.query(geneticInfo).all():\n",
    "        db_row = object_as_dict(row)\n",
    "        for i in range(len(metatable)):\n",
    "            if value_compare(db_row,metatable[i],genetic_keys):\n",
    "                metatable[i][\"genetic_id\"] = row.genetic_id         \n",
    "\n",
    "# Add experimental id                \n",
    "expt_keys = dict(config[\"expt keys\"])\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for row in session.query(exptMetadata).all():\n",
    "        db_row = object_as_dict(row)\n",
    "        for i in range(len(metatable)):\n",
    "            if value_compare(db_row,metatable[i],expt_keys):\n",
    "                metatable[i][\"expt_id\"] = row.expt_id\n",
    "                \n",
    "# Collapse table to unique values based on sample_id (combines SRZs, essentially)\n",
    "metatable = list({v['sample_id']:v for v in metatable}.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make linkIDs table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(metatable)):\n",
    "        entry = linkIDs(\n",
    "            sample_id = metatable[i][\"sample_id\"],\n",
    "            genetic_id = metatable[i][\"genetic_id\"],\n",
    "            expt_id = metatable[i][\"expt_id\"],\n",
    "        )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load database data location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = config[\"file_locations\"][\"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape FastQC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    sample = metatable[i][\"sample_name\"]\n",
    "    dirpath = data_path + paper_id + \"/qc/fastqc/zips/\"\n",
    "    \n",
    "    if metatable[i][config[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "        samp_zip = dirpath + sample + \"_1_fastqc\"\n",
    "    else:\n",
    "        samp_zip = dirpath + sample + \"_fastqc\"\n",
    "\n",
    "    if not (\n",
    "        os.path.exists(samp_zip + \".zip\")\n",
    "    ):\n",
    "        continue\n",
    "        \n",
    "    with zp.ZipFile(samp_zip + \".zip\", 'r') as zp_ref:\n",
    "        zp_ref.extractall(dirpath)\n",
    "\n",
    "    fdata = open(samp_zip + \"/fastqc_data.txt\")\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Sequences\").search(line):\n",
    "            metatable[i][\"raw_read_depth\"] = int(line.split()[2])\n",
    "        if re.compile(\"Sequence length\").search(line):\n",
    "            metatable[i][\"raw_read_length\"] = int(line.split()[2].split(\"-\")[0])\n",
    "\n",
    "    shutil.rmtree(samp_zip)\n",
    "\n",
    "    if metatable[i][config[\"accum keys\"][\"rcomp\"]] == 1:\n",
    "        if metatable[i][config[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "            samp_zip = dirpath + sample + \"_1.flip.trim_fastqc\"\n",
    "        else:\n",
    "            samp_zip = dirpath + sample + \".flip.trim_fastqc\"\n",
    "    else:\n",
    "        if metatable[i][config[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "            samp_zip = dirpath + sample + \"_1.trim_fastqc\"\n",
    "        else:\n",
    "            samp_zip = dirpath + sample + \".trim_fastqc\"\n",
    "    \n",
    "    with zp.ZipFile(samp_zip + \".zip\", 'r') as zp_ref:\n",
    "        zp_ref.extractall(dirpath)\n",
    "\n",
    "    fdata = open(samp_zip + \"/fastqc_data.txt\")\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Sequences\").search(line):\n",
    "            metatable[i][\"trim_read_depth\"] = int(line.split()[2])\n",
    "\n",
    "    shutil.rmtree(samp_zip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape picardtools data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    sample = metatable[i][\"sample_name\"]    \n",
    "    dirpath = data_path + paper_id + \"/qc/picard/dups/\"\n",
    "    filepath = dirpath + sample + \".marked_dup_metrics.txt\"\n",
    "    \n",
    "    if not (\n",
    "        os.path.exists(filepath) and os.path.isfile(filepath)\n",
    "    ):\n",
    "        continue\n",
    "        \n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Unknown Library\").search(line):\n",
    "            metatable[i][\"duplication_picard\"] = float(line.split(\"\\t\")[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape mapping data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    sample = metatable[i][\"sample_name\"]    \n",
    "    dirpath = data_path + paper_id + \"/qc/hisat2_mapstats/\"\n",
    "    filepath = dirpath + sample + \".hisat2_mapstats.txt\"\n",
    "    \n",
    "    if not (\n",
    "        os.path.exists(filepath) and os.path.isfile(filepath)\n",
    "    ):\n",
    "        continue\n",
    "        \n",
    "    fdata = open(filepath)\n",
    "    if metatable[i][config[\"accum keys\"][\"single_paired\"]] == \"paired\":\n",
    "        for line in fdata:\n",
    "            if re.compile(\"concordantly 1 time\").search(line):\n",
    "                reads = int(line.split(\": \")[1].split(\" (\")[0]) * 2\n",
    "            if re.compile(\"Aligned 1 time\").search(line):    \n",
    "                metatable[i][\"single_map\"] = reads + int(line.split(\": \")[1].split(\" (\")[0])\n",
    "            if re.compile(\"concordantly >1 times\").search(line):\n",
    "                reads = int(line.split(\": \")[1].split(\" (\")[0]) * 2\n",
    "            if re.compile(\"Aligned >1 times\").search(line):\n",
    "                metatable[i][\"multi_map\"] = reads + int(line.split(\": \")[1].split(\" (\")[0])\n",
    "            if re.compile(\"Overall alignment rate\").search(line):\n",
    "                metatable[i][\"map_prop\"] = float(line.split(\": \")[1].split(\"%\")[0])/100\n",
    "    else:\n",
    "        for line in fdata:\n",
    "            if re.compile(\"Aligned 1 time\").search(line):\n",
    "                metatable[i][\"single_map\"] = int(line.split(\": \")[1].split(\" (\")[0])\n",
    "            if re.compile(\"Aligned >1 times\").search(line):\n",
    "                metatable[i][\"multi_map\"] = int(line.split(\": \")[1].split(\" (\")[0])\n",
    "            if re.compile(\"Overall alignment rate\").search(line):\n",
    "                metatable[i][\"map_prop\"] = float(line.split(\": \")[1].split(\"%\")[0])/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scrape rseqc data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    sample = metatable[i][\"sample_name\"] \n",
    "    dirpath = data_path + paper_id + \"/qc/rseqc/read_distribution/\"\n",
    "    filepath = dirpath + sample + \".read_distribution.txt\"\n",
    "    \n",
    "    if not (\n",
    "        os.path.exists(filepath) and os.path.isfile(filepath)\n",
    "    ):\n",
    "        continue\n",
    "        \n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if re.compile(\"Total Assigned Tags\").search(line):\n",
    "            metatable[i][\"rseqc_tags\"] = int(line.split()[-1])\n",
    "        if re.compile(\"CDS_Exons\").search(line):\n",
    "            metatable[i][\"rseqc_cds\"] = int(line.split()[2])\n",
    "            metatable[i][\"cds_rpk\"] = float(line.split()[-1])\n",
    "        if re.compile(\"5'UTR_Exons\").search(line):\n",
    "            metatable[i][\"rseqc_five_utr\"] = int(line.split()[2])\n",
    "        if re.compile(\"3'UTR_Exons\").search(line):\n",
    "            metatable[i][\"rseqc_three_utr\"] = int(line.split()[2])\n",
    "        if re.compile(\"Introns\").search(line):\n",
    "            metatable[i][\"rseqc_intron\"] = int(line.split()[2])\n",
    "            metatable[i][\"intron_rpk\"] = float(line.split()[-1])\n",
    "\n",
    "    metatable[i][\"exint_ratio\"] =  metatable[i][\"cds_rpk\"]/metatable[i][\"intron_rpk\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull preseq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    sample = metatable[i][\"sample_name\"]  \n",
    "    dirpath = data_path + paper_id + \"/qc/preseq/\"\n",
    "    filepath = dirpath + sample + \".lc_extrap.txt\"\n",
    "    \n",
    "    if not (\n",
    "        os.path.exists(filepath) and os.path.isfile(filepath)\n",
    "    ):\n",
    "        continue\n",
    "           \n",
    "    fdata = open(filepath)\n",
    "    for line in fdata:\n",
    "        if line.startswith(\"10000000.0\"):\n",
    "            distinct = float(line.split()[1])\n",
    "            \n",
    "    metatable[i][\"distinct_tenmillion_prop\"] = distinct/10000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pull pileup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(metatable)):\n",
    "    paper_id = metatable[i][config[\"expt keys\"][\"paper_id\"]]\n",
    "    sample = metatable[i][\"sample_name\"]\n",
    "    dirpath = data_path + paper_id + \"/qc/pileup/\"\n",
    "    filepath = dirpath + sample + \".coverage.stats.txt\"\n",
    "    \n",
    "    if not (\n",
    "        os.path.exists(filepath) and os.path.isfile(filepath)\n",
    "    ):\n",
    "        continue\n",
    "         \n",
    "    fdata = open(filepath)\n",
    "    x = 0\n",
    "    total = cov = fold = 0\n",
    "    for line in fdata:\n",
    "        if x == 0:\n",
    "            x = x + 1\n",
    "            continue\n",
    "        else:\n",
    "            x = x + 1\n",
    "            total = total + int(line.split(\"\\t\")[2])\n",
    "            cov = cov + int(line.split(\"\\t\")[5])\n",
    "            fold = fold + float(line.split(\"\\t\")[1])*int(line.split(\"\\t\")[2])\n",
    "        \n",
    "    metatable[i][\"genome_prop_cov\"] = cov/total\n",
    "    metatable[i][\"avg_fold_cov\"] = fold/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_keys = list(dict(config[\"accum keys\"]).values())\n",
    "\n",
    "#with utils.NascentDBConnection(db_url=db_url) as session:\n",
    "with NascentDBConnection(db_url = \"sqlite:///\" + db_url) as session:\n",
    "    for i in range(len(metatable)):\n",
    "        entry = sampleAccum(\n",
    "            sample_id = metatable[i][\"sample_id\"],\n",
    "            replicate = metatable[i][config[\"accum keys\"][\"replicate\"]],\n",
    "            single_paired = metatable[i][config[\"accum keys\"][\"single_paired\"]],\n",
    "            rcomp = metatable[i][config[\"accum keys\"][\"rcomp\"]],\n",
    "            raw_read_depth = metatable[i][\"raw_read_depth\"],\n",
    "            trim_read_depth = metatable[i][\"trim_read_depth\"],\n",
    "            raw_read_length = metatable[i][\"raw_read_length\"],\n",
    "            duplication_picard = metatable[i][\"duplication_picard\"],\n",
    "            single_map = metatable[i][\"single_map\"],\n",
    "            multi_map = metatable[i][\"multi_map\"],\n",
    "            map_prop = metatable[i][\"map_prop\"],\n",
    "            rseqc_tags = metatable[i][\"rseqc_tags\"],\n",
    "            rseqc_cds = metatable[i][\"rseqc_cds\"],\n",
    "            rseqc_five_utr = metatable[i][\"rseqc_five_utr\"],\n",
    "            rseqc_three_utr = metatable[i][\"rseqc_three_utr\"],\n",
    "            rseqc_intron = metatable[i][\"rseqc_intron\"],\n",
    "            cds_rpk = metatable[i][\"cds_rpk\"],\n",
    "            intron_rpk = metatable[i][\"intron_rpk\"],\n",
    "            exint_ratio = metatable[i][\"exint_ratio\"],\n",
    "            distinct_tenmillion_prop = metatable[i][\"distinct_tenmillion_prop\"],\n",
    "            genome_prop_cov = metatable[i][\"genome_prop_cov\"],\n",
    "            avg_fold_cov = metatable[i][\"avg_fold_cov\"],\n",
    "        )\n",
    "        session.merge(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
